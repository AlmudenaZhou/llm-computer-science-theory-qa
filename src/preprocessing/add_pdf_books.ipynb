{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pdf to Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The automatic markdown difficults the extraction of the questions with the titles, as it does not get it right. That's why I disabled the title and keep only the basic md through hdr_info = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../data/books/500 Data Science Interview Questions.pdf...\n",
      "[                                        ] (0/14[                                        ] (  1/141)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                        ] (  2/14[                                        ] (  3/141[=                                       ] (  4/141[=                                       ] (  5/141[=                                       ] (  6/1[=                                       ] (  7/14[==                                      ] (  8/14[==                                      ] (  9/14[==                                      ] ( 10/141[===                                     ] ( 11/1[===                                     ] ( 12/1[===                                     ] ( 13/1[===                                     ] ( 14/14[====                                    ] ( 15/14[====                                    ] ( 16/14[====                                    ] ( 17/141[=====                                   ] ( 18/1[=====                                   ] ( 19/1[=====                                   ] ( 20/1[=====                                   ] ( 21/14[======                                  ] ( 22/14[======                                  ] ( 23/14[======                                  ] ( 24/141[=======                                 ] ( 25/1[=======                                 ] ( 26/1[=======                                 ] ( 27/141[=======                                 ] ( 28/14[========                                ] ( 29/14[========                                ] ( 30/14[========                                ] ( 31/141[=========                               ] ( 32/1[=========                               ] ( 33/141[=========                               ] ( 34/141[=========                               ] ( 35/14[==========                              ] ( 36/14[==========                              ] ( 37/14[==========                              ] ( 38/141[===========                             ] ( 39/1[===========                             ] ( 40/141[===========                             ] ( 41/1[===========                             ] ( 42/14[============                            ] ( 43/14[============                            ] ( 44/14[============                            ] ( 45/141[=============                           ] ( 46/1[=============                           ] ( 47/1[=============                           ] ( 48/141[=============                           ] ( 49/14[==============                          ] ( 50/14[==============                          ] ( 51/14[==============                          ] ( 52/141[===============                         ] ( 53/141[===============                         ] ( 54/1[===============                         ] ( 55/1[===============                         ] ( 56/14[================                        ] ( 57/14[================                        ] ( 58/14[================                        ] ( 59/141[=================                       ] ( 60/1[=================                       ] ( 61/1[=================                       ] ( 62/141[=================                       ] ( 63/14[==================                      ] ( 64/14[==================                      ] ( 65/14[==================                      ] ( 66/141[===================                     ] ( 67/141[===================                     ] ( 68/1[===================                     ] ( 69/1[===================                     ] ( 70/14[====================                    ] ( 71/14[====================                    ] ( 72/14[====================                    ] ( 73/14[====================                    ] ( 74/141[=====================                   ] ( 75/1[=====================                   ] ( 76/141[=====================                   ] ( 77/14[======================                  ] ( 78/14[======================                  ] ( 79/14[======================                  ] ( 80/14[======================                  ] ( 81/141[=======================                 ] ( 82/1[=======================                 ] ( 83/1[=======================                 ] ( 84/14[========================                ] ( 85/14[========================                ] ( 86/14[========================                ] ( 87/14[========================                ] ( 88/141[=========================               ] ( 89/1[=========================               ] ( 90/1[=========================               ] ( 91/14[==========================              ] ( 92/14[==========================              ] ( 93/14[==========================              ] ( 94/14[==========================              ] ( 95/141[===========================             ] ( 96/141[===========================             ] ( 97/141[===========================             ] ( 98/14[============================            ] ( 99/14[============================            ] (100/14[============================            ] (101/14[============================            ] (102/141[=============================           ] (103/141[=============================           ] (104/141[=============================           ] (105/14[==============================          ] (106/14[==============================          ] (107/14[==============================          ] (108/14[==============================          ] (109/141[===============================         ] (110/1[===============================         ] (111/1[===============================         ] (112/14[================================        ] (113/14[================================        ] (114/14[================================        ] (115/14[================================        ] (116/141[=================================       ] (117/141[=================================       ] (118/141[=================================       ] (119/14[==================================      ] (120/14[==================================      ] (121/14[==================================      ] (122/14[==================================      ] (123/141[===================================     ] (124/1[===================================     ] (125/1[===================================     ] (126/14[====================================    ] (127/14[====================================    ] (128/14[====================================    ] (129/14[====================================    ] (130/141[=====================================   ] (131/1[=====================================   ] (132/1[=====================================   ] (133/14[======================================  ] (134/14[======================================  ] (135/14[======================================  ] (136/14[======================================  ] (137/141[======================================= ] (138/1[======================================= ] (139/1[======================================= ] (140/14[========================================] (141/141]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-0-0.png)\\n\\n-----\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-1-0.png)\\n\\n-----\\n\\n500 Data Science Interview Questions and Answers\\n\\nVamsee Puligadda\\n\\n**Disclaimer**\\n\\nAlthough the author and publisher have made every effort to ensure that the information in this book was correct at press time, the\\nauthor and publisher do not assume and hereby disclaim any liability to any party for any loss, damage, or disruption caused by\\nerrors or omissions, whether such errors or omissions result from negligence, accident, or any other cause.\\n\\n**For any suggestions/concerns, please write to us:** indianwolfpublications@gmail.com\\n\\n**Copyright 2018 Vamsee Puligadda & Indian Wolf Publications**\\n\\n\\n-----\\n\\n**Foreword**\\n\\nI am neither the owner of a famous publishing company nor am a top IT company with hundreds of inhouse developers to create anything I wanted to with a great ease.\\n\\nI am an independent Software developer with passion towards what I do and trust me, a lot of time, efforts\\nwere put into creating this extensive collection of questions and answers at a single place.\\n\\nIf it helps at least a few in their careers to achieve their dream jobs, I will be more than happy.\\n\\nThank You.\\n\\n-  Vamsee Puligadda.\\n\\n\\n-----\\n\\n**Question 1. What Is A Recommender System?**\\n\\n**Answer:**\\nA recommender system is today widely deployed in multiple fields like movie\\nrecommendations, music preferences, social tags, research articles, search\\nqueries and so on. The recommender systems work as per collaborative and\\ncontent-based filte\\nring or by deploying a personality-based approach. This type of system works\\nbased on a person’s past behavior in order to build a model for the future. This\\nwill predict the future product buying, movie viewing or book reading by\\npeople. It also creates a filtering approach using the discrete characteristics of\\nitems while recommending additional items.\\n\\n**Question 2. Compare Sas, R And Python Programming?**\\n\\n**Answer:**\\n**SAS: it is one of the most widely used analytics tools used by some of the**\\nbiggest companies on earth. It has some of the best statistical functions,\\ngraphical user interface, but can come with a price tag and hence it cannot be\\nreadily adopted by smaller enterprises\\n**R: The best part about R is that it is an Open Source tool and hence used**\\ngenerously by academia and the research community. It is a robust tool for\\nstatistical computation, graphical representation and reporting. Due to its open\\nsource nature it is always being updated with the latest features and then\\nreadily available to everybody.\\n**Python: Python is a powerful open source programming language that is easy**\\nto learn, works well with most other tools and technologies. The best part\\nabout Python is that it has innumerable libraries and community created\\nmodules making it very robust. It has functions for statistical operation, model\\nbuilding and more.\\n\\n**Question 3. Explain The Various Benefits Of R Language?**\\n\\n**Answer:**\\nThe R programming language includes a set of software suite that is used for\\ngraphical representation, statistical computing, data manipulation and\\ncalculation.\\n**Some of the highlights of R programming environment include the**\\n**following:**\\n\\nAn extensive collection of tools for data analysis\\nOperators for performing calculations on matrix and array\\nData analysis technique for graphical representation\\n\\n\\n-----\\n\\nA highly developed yet simple and effective programming language\\nIt extensively supports machine learning applications\\nIt acts as a connecting link between various software, tools and datasets\\nCreate high quality reproducible analysis that is flexible and powerful\\nProvides a robust package ecosystem for diverse needs\\nIt is useful when you have to solve a data-oriented problem\\n\\n**Question 4. How Do Data Scientists Use Statistics?**\\n\\n**Answer:**\\nStatistics helps Data Scientists to look into the data for patterns, hidden\\ninsights and convert Big Data into Big insights. It helps to get a better idea of\\nwhat the customers are expecting. Data Scientists can learn about the\\nconsumer behavior, interest, engagement, retention and finally conversion all\\nthrough the power of insightful statistics. It helps them to build powerful data\\nmodels in order to validate certain inferences and predictions. All this can be\\nconverted into a powerful business proposition by giving users what they\\nwant at precisely when they want it.\\n\\n**Question 5. What Is Logistic Regression?**\\n\\n**Answer:**\\nIt is a statistical technique or a model in order to analyze a dataset and predict\\nthe binary outcome. The outcome has to be a binary outcome that is either\\nzero or one or a yes or no.\\n\\n**Question 6. Why Data Cleansing Is Important In Data Analysis?**\\n\\n**Answer:**\\nWith data coming in from multiple sources it is important to ensure that data\\nis good enough for analysis. This is where data cleansing becomes extremely\\nvital. Data cleansing extensively deals with the process of detecting and\\ncorrecting of data records, ensuring that data is complete and accurate and the\\ncomponents of data that are irrelevant are deleted or modified as per the\\nneeds. This process can be deployed in concurrence with data wrangling or\\nbatch processing.\\nOnce the data is cleaned it confirms with the rules of the data sets in the\\nsystem. Data cleansing is an essential part of the data science because the data\\ncan be prone to error due to human negligence, corruption during\\ntransmission or storage among other things. Data cleansing takes a huge\\nchunk of time and effort of a Data Scientist because of the multiple sources\\nfrom which data emanates and the speed at which it comes.\\n\\n**Question 7. Describe Univariate, Bivariate And Multivariate Analysis.?**\\n\\n**Answer:**\\n\\n\\n-----\\n\\nAs the name suggests these are analysis methodologies having a single,\\ndouble or multiple variables.\\nSo a univariate analysis will have one variable and due to this there are no\\nrelationships, causes. The major aspect of the univariate analysis is to\\nsummarize the data and find the patterns within it to make actionable\\ndecisions.\\nA Bivariate analysis deals with the relationship between two sets of data.\\nThese sets of paired data come from related sources, or samples. There are\\nvarious tools to analyze such data including the chi-squared tests and t-tests\\nwhen the data are having a correlation.\\nIf the data can be quantified then it can analyzed using a graph plot or a\\nscatterplot. The strength of the correlation between the two data sets will be\\ntested in a Bivariate analysis.\\n\\n**Question 8. How Machine Learning Is Deployed In Real World Scenarios?**\\n\\n**Answer:**\\n**Here are some of the scenarios in which machine learning finds**\\n**applications in real world:**\\n**Ecommerce: Understanding the customer churn, deploying targeted**\\nadvertising, remarketing.\\n**Search engine: Ranking pages depending on the personal preferences of the**\\nsearcher\\n**Finance: Evaluating investment opportunities & risks, detecting fraudulent**\\ntransactions\\n**Medicare: Designing drugs depending on the patient’s history and needs**\\n**Robotics: Machine learning for handling situations that are out of the**\\nordinary\\n**Social media: Understanding relationships and recommending connections**\\n**Extraction of information: framing questions for getting answers from**\\ndatabases over the web.\\n\\n**Question 9. What Are The Various Aspects Of A Machine Learning**\\n**Process?**\\n\\n**Answer:**\\nIn this post I will discuss the components involved in solving a problem using\\nmachine learning.\\n**Domain knowledge:**\\nThis is the first step wherein we need to understand how to extract the various\\nfeatures from the data and learn more about the data that we are dealing with.\\nIt has got more to do with the type of domain that we are dealing with and\\n\\n\\n-----\\n\\nfamiliarizing the system to learn more about it.\\n**Feature Selection:**\\nThis step has got more to do with the feature that we are selecting from the set\\nof features that we have. Sometimes it happens that there are a lot of features\\nand we have to make an intelligent decision regarding the type of feature that\\nwe want to select to go ahead with our machine learning endeavor.\\n**Algorithm:**\\nThis is a vital step since the algorithms that we choose will have a very major\\nimpact on the entire process of machine learning. You can choose between the\\nlinear and nonlinear algorithm. Some of the algorithms used are Support\\nVector Machines, Decision Trees, Naïve Bayes, K-Means Clustering, etc.\\n**Training:**\\nThis is the most important part of the machine learning technique and this is\\nwhere it differs from the traditional programming. The training is done based\\non the data that we have and providing more real world experiences. With\\neach consequent training step the machine gets better and smarter and able to\\ntake improved decisions.\\n**Evaluation:**\\nIn this step we actually evaluate the decisions taken by the machine in order\\nto decide whether it is up to the mark or not. There are various metrics that are\\ninvolved in this process and we have to closed deploy each of these to decide\\non the efficacy of the whole machine learning endeavor.\\n**Optimization:**\\nThis process involves improving the performance of the machine learning\\nprocess using various optimization techniques. Optimization of machine\\nlearning is one of the most vital components wherein the performance of the\\nalgorithm is vastly improved. The best part of optimization techniques is that\\nmachine learning is not just a consumer of optimization techniques but it also\\nprovides new ideas for optimization too.\\n**Testing:**\\nHere various tests are carried out and some these are unseen set of test cases.\\nThe data is partitioned into test and training set. There are various testing\\ntechniques like cross-validation in order to deal with multiple situations.\\n\\n**Question 10. What Do You Understand By The Term Normal**\\n**Distribution?**\\n\\n**Answer:**\\nIt is a set of continuous variable spread across a normal curve or in the shape\\nof a bell curve. It can be considered as a continuous probability distribution\\n\\n\\n-----\\n\\nand is useful in statistics. It is the most common distribution curve and it\\nbecomes very useful to analyze the variables and their relationships when we\\nhave the normal distribution curve.\\nThe normal distribution curve is symmetrical. The non-normal distribution\\napproaches the normal distribution as the size of the samples increases. It is\\nalso very easy to deploy the Central Limit Theorem. This method helps to\\nmake sense of data that is random by creating an order and interpreting the\\nresults using a bell-shaped graph.\\n\\n**Question 11. What Is Linear Regression?**\\n\\n**Answer:**\\nIt is the most commonly used method for predictive analytics. The Linear\\nRegression method is used to describe relationship between a dependent\\nvariable and one or independent variable. The main task in the Linear\\nRegression is the method of fitting a single line within a scatter plot.\\n**The Linear Regression consists of the following three methods:**\\nDetermining and analyzing the correlation and direction of the data\\nDeploying the estimation of the model\\nEnsuring the usefulness and validity of the model\\nIt is extensively used in scenarios where the cause effect model comes into\\nplay. For example you want to know the effect of a certain action in order to\\ndetermine the various outcomes and extent of effect the cause has in\\ndetermining the final outcome.\\n\\n**Question 12. What Is Interpolation And Extrapolation?**\\n\\n**Answer:**\\nThe terms of interpolation and extrapolation are extremely important in any\\nstatistical analysis. Extrapolation is the determination or estimation using a\\nknown set of values or facts by extending it and taking it to an area or region\\nthat is unknown. It is the technique of inferring something using data that is\\navailable.\\nInterpolation on the other hand is the method of determining a certain value\\nwhich falls between a certain set of values or the sequence of values.\\nThis is especially useful when you have data at the two extremities of a\\ncertain region but you don’t have enough data points at the specific point.\\nThis is when you deploy interpolation to determine the value that you need.\\n\\n**Question 13. What Is Power Analysis?**\\n\\n**Answer:**\\nThe power analysis is a vital part of the experimental design. It is involved\\n\\n\\n-----\\n\\nwith the process of determining the sample size needed for detecting an effect\\nof a given size from a cause with a certain degree of assurance. It lets you\\ndeploy specific probability in a sample size constraint.\\nThe various techniques of statistical power analysis and sample size\\nestimation are widely deployed for making statistical judgment that are\\naccurate and evaluate the size needed for experimental effects in practice.\\nPower analysis lets you understand the sample size estimate so that they are\\nneither high nor low. A low sample size there will be no authentication to\\nprovide reliable answers and if it is large there will be wastage of resources.\\n\\n**Question 14. What Is K-means? How Can You Select K For K-means?**\\n\\n**Answer:**\\nK-means clustering can be termed as the basic unsupervised learning\\nalgorithm. It is the method of classifying data using a certain set of clusters\\ncalled as K clusters. It is deployed for grouping data in order to find similarity\\nin the data.\\nIt includes defining the K centers, one each in a cluster. The clusters are\\ndefined into K groups with K being predefined. The K points are selected at\\nrandom as cluster centers. The objects are assigned to their nearest cluster\\ncenter. The objects within a cluster are as closely related to one another as\\npossible and differ as much as possible to the objects in other clusters. Kmeans clustering works very well for large sets of data.\\n\\n**Question 15. How Is Data Modeling Different From Database Design?**\\n\\n**Answer:**\\n**Data Modeling: It can be considered as the first step towards the design of a**\\ndatabase. Data modeling creates a conceptual model based on the relationship\\nbetween various data models. The process involves moving from the\\nconceptual stage to the logical model to the physical schema. It involves the\\nsystematic method of applying the data modeling techniques.\\n**Database Design: This is the process of designing the database. The database**\\ndesign creates an output which is a detailed data model of the database.\\nStrictly speaking database design includes the detailed logical model of a\\ndatabase but it can also include physical design choices and storage\\nparameters.\\n\\n**Question 16. What Are Feature Vectors?**\\n\\n**Answer:**\\n\\nn-dimensional vector of numerical features that represent some object\\nTerm occurrences frequencies, pixels of an image etc.\\nFeature space: vector space associated with these vectors\\n\\n\\n-----\\n\\n**Question 17. Explain The Steps In Making A Decision Tree.?**\\n\\n**Answer:**\\n\\nTake the entire data set as input\\nLook for a split that maximizes the separation of the classes. A split is\\nany test that divides the data in two sets\\nApply the split to the input data (divide step)\\nRe-apply steps 1 to 2 to the divided data\\nStop when you meet some stopping criteria\\nThis step is called pruning. Clean up the tree when you went too far\\ndoing splits.\\n\\n**Question 18. What Is Root Cause Analysis?**\\n\\n**Answer:**\\nRoot cause analysis was initially developed to analyze industrial accidents,\\nbut is now widely used in other areas. It is basically a technique of problem\\nsolving used for isolating the root causes of faults or problems. A factor is\\ncalled a root cause if its deduction from the problem-fault-sequence averts the\\nfinal undesirable event from reoccurring.\\n\\n**Question 19. Explain Cross-validation.?**\\n\\n**Answer:**\\nIt is a model validation technique for evaluating how the outcomes of a\\nstatistical analysis will generalize to an independent data set. Mainly used in\\nbackgrounds where the objective is forecast and one wants to estimate how\\naccurately a model will accomplish in practice.\\nThe goal of cross-validation is to term a data set to test the model in the\\ntraining phase (i.e. validation data set) in order to limit problems like over\\nfitting, and get an insight on how the model will generalize to an independent\\ndata set.\\n\\n**Question 20. What Is Collaborative Filtering?**\\n\\n**Answer:**\\nThe process of filtering used by most of the recommender systems to find\\npatterns or information by collaborating perspectives, numerous data sources\\nand several agents.\\n\\n**Question 21. Do Gradient Descent Methods At All Times Converge To**\\n**Similar Point?**\\n\\n**Answer:**\\nNo, they do not because in some cases it reaches a local minima or a local\\noptima point. You will not reach the global optima point. This is governed by\\nthe data and the starting conditions.\\n\\n\\n-----\\n\\n**Question 22. What Is The Goal Of A/b Testing?**\\n\\n**Answer:**\\nIt is a statistical hypothesis testing for randomized experiment with two\\nvariables A and B. The objective of A/B Testing is to detect any changes to\\nthe web page to maximize or increase the outcome of an interest.\\n\\n**Question 23. What Are The Drawbacks Of Linear Model?**\\n\\n**Answer:**\\n**Some drawbacks of the linear model are:**\\n\\nThe assumption of linearity of the errors\\nIt can’t be used for count outcomes, binary outcomes\\nThere are overfitting problems that it can’t solve\\n\\n**Question 24. What Is The Law Of Large Numbers?**\\n\\n**Answer:**\\nIt is a theorem that describes the result of performing the same experiment a\\nlarge number of times. This theorem forms the basis of frequency-style\\nthinking. It says that the sample mean, the sample variance and the sample\\nstandard deviation converge to what they are trying to estimate.\\n\\n**Question 25. What Are Confounding Variables?**\\n\\n**Answer:**\\nThese are extraneous variables in a statistical model that correlate directly or\\ninversely with both the dependent and the independent variable. The estimate\\nfails to account for the confounding factor.\\n\\n**Question 26. Explain Star Schema.?**\\n\\n**Answer:**\\nIt is a traditional database schema with a central table. Satellite tables map\\nID’s to physical name or description and can be connected to the central fact\\ntable using the ID fields; these tables are known as lookup tables, and are\\nprincipally useful in real-time applications, as they save a lot of memory.\\nSometimes star schemas involve several layers of summarization to recover\\ninformation faster.\\n\\n**Question 27. How Regularly An Algorithm Must Be Update?**\\n\\n**Answer:**\\n**You want to update an algorithm when:**\\n\\nYou want the model to evolve as data streams through infrastructure\\nThe underlying data source is changing\\nThere is a case of non-stationarity\\n\\n**Question 28. What Are Eigenvalue And Eigenvector?**\\n\\n**Answer:**\\n\\n\\n-----\\n\\nEigenvectors are for understanding linear transformations. In data analysis,\\nwe usually calculate the eigenvectors for a correlation or covariance matrix.\\nEigenvectors are the directions along which a particular linear transformation\\nacts by flipping, compressing or stretching.\\n\\n**Question 29. Why Is Resampling Done?**\\n\\n**Answer:**\\n**Resampling is done in one of these cases:**\\n\\nEstimating the accuracy of sample statistics by using subsets of\\naccessible data or drawing randomly with replacement from a set of data\\npoints\\nSubstituting labels on data points when performing significance tests\\nValidating models by using random subsets (bootstrapping, cross\\nvalidation.\\n\\n**Question 30. Explain Selective Bias.?**\\n\\n**Answer:**\\nSelection bias, in general, is a problematic situation in which error is\\nintroduced due to a non-random population sample.\\n\\n**Question 31. What Are The Types Of Biases That Can Occur During**\\n**Sampling?**\\n\\n**Answer:**\\n\\nSelection bias\\nUnder coverage bias\\nSurvivorship bias\\n\\n**Question 32. How To Work Towards A Random Forest?**\\n\\n**Answer:**\\nUnderlying principle of this technique is that several weak learners combined\\nprovide a strong learner. The steps involved are\\n\\nBuild several decision trees on bootstrapped training samples of data\\nOn each tree, each time a split is considered, a random sample of mm\\npredictors is chosen as split candidates, out of all pp predictors\\n**Rule of thumb: at each split m=p√m=p**\\n**Predictions: at the majority rule.**\\n\\n**32)     Python or R – Which one would you prefer for text analytics?**\\nThe best possible answer for this would be Python because it has Pandas library\\nthat provides easy to use data structures and high performance data analysis\\ntools.\\n\\n\\n-----\\n\\n**34)     What is logistic regression? Or State an example when you have**\\n**used logistic regression recently.**\\nLogistic Regression often referred as logit model is a technique to predict the\\nbinary outcome from a linear combination of predictor variables. For example, if\\nyou want to predict whether a particular political leader will win the election or\\nnot. In this case, the outcome of prediction is binary i.e. 0 or 1 (Win/Lose). The\\npredictor variables here would be the amount of money spent for election\\ncampaigning of a particular candidate, the amount of time spent in campaigning,\\netc.\\n\\n**35)     What are Recommender Systems?**\\nA subclass of information filtering systems that are meant to predict the\\npreferences or ratings that a user would give to a product. Recommender\\nsystems are widely used in movies, news, research articles, products, social tags,\\nmusic, etc.\\n\\n**36)     Why data cleaning plays a vital role in analysis?**\\nCleaning data from multiple sources to transform it into a format that data\\nanalysts or data scientists can work with is a cumbersome process because - as\\nthe number of data sources increases, the time take to clean the data increases\\nexponentially due to the number of sources and the volume of data generated in\\nthese sources. It might take up to 80% of the time for just cleaning data making\\nit a critical part of analysis task.\\n\\n**37)     Differentiate between univariate, bivariate and multivariate**\\n**analysis.**\\nThese are descriptive statistical analysis techniques which can be differentiated\\nbased on the number of variables involved at a given point of time. For example,\\nthe pie charts of sales based on territory involve only one variable and can be\\nreferred to as univariate analysis.\\n\\nIf the analysis attempts to understand the difference between 2 variables at time\\nas in a scatterplot, then it is referred to as bivariate analysis. For example,\\nanalysing the volume of sale and a spending can be considered as an example of\\nbivariate analysis.\\n\\nAnalysis that deals with the study of more than two variables to understand the\\neffect of variables on the responses is referred to as multivariate analysis.\\n\\n**38)     What do you understand by the term Normal Distribution?**\\nData is usually distributed in different ways with a bias to the left or to the right\\nor it can all be jumbled up. However, there are chances that data is distributed\\n\\n\\n-----\\n\\naround a central value without any bias to the left or right and reaches normal\\ndistribution in the form of a bell shaped curve. The random variables are\\ndistributed in the form of an symmetrical bell shaped curve.\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-14-0.png)\\n\\nImage Credit : mathisfun.com\\n\\n**39)     What is Linear Regression?**\\nLinear regression is a statistical technique where the score of a variable Y is\\npredicted from the score of a second variable X. X is referred to as the predictor\\nvariable and Y as the criterion variable.\\n\\n**40)    What is Interpolation and Extrapolation?**\\nEstimating a value from 2 known values from a list of values is Interpolation.\\nExtrapolation is approximating a value by extending a known set of values or\\nfacts.\\n\\n**41)    What is power analysis?**\\nAn experimental design technique for determining the effect of a given sample\\nsize.\\n\\n**42)   What is K-means? How can you select K for K-means?**\\n**43)    What is Collaborative filtering?**\\nThe process of filtering used by most of the recommender systems to find\\npatterns or information by collaborating viewpoints, various data sources and\\nmultiple agents.\\n\\n**44)    What is the difference between Cluster and Systematic Sampling?**\\nCluster sampling is a technique used when it becomes difficult to study the target\\npopulation spread across a wide area and simple random sampling cannot be\\napplied. Cluster Sample is a probability sample where each sampling unit is a\\ncollection, or cluster of elements. Systematic sampling is a statistical technique\\nwhere elements are selected from an ordered sampling frame. In systematic\\nsampling, the list is progressed in a circular manner so once you reach the end of\\n\\n\\n-----\\n\\nthe list,it is progressed from the top again. The best example for systematic\\nsampling is equal probability method.\\n\\n**45)    Are expected value and mean value different?**\\nThey are not different but the terms are used in different contexts. Mean is\\ngenerally referred when talking about a probability distribution or sample\\npopulation whereas expected value is generally referred in a random variable\\ncontext.\\n\\n**For Sampling Data**\\nMean value is the only value that comes from the sampling data.\\n\\nExpected Value is the mean of all the means i.e. the value that is built from\\nmultiple samples. Expected value is the population mean.\\n\\n**For Distributions**\\nMean value and Expected value are same irrespective of the distribution, under\\nthe condition that the distribution is in the same population.\\n\\n**46)    What does P-value signify about the statistical data?**\\nP-value is used to determine the significance of results after a hypothesis test in\\nstatistics. P-value helps the readers to draw conclusions and is always between 0\\nand 1.\\n\\n-       P- Value > 0.05 denotes weak evidence against the null hypothesis which\\nmeans the null hypothesis cannot be rejected.\\n\\n-       P-value <= 0.05 denotes strong evidence against the null hypothesis\\nwhich means the null hypothesis can be rejected.\\n\\n-       P-value=0.05is the marginal value indicating it is possible to go either\\nway.\\n\\n**47) Do gradient descent methods always converge to same point?**\\nNo, they do not because in some cases it reaches a local minima or a local\\noptima point. You don’t reach the global optima point. It depends on the data and\\nstarting conditions\\n\\n**48) What are categorical variables?**\\n**49)    A test has a true positive rate of 100% and false positive rate of 5%.**\\n**There is a population with a 1/1000 rate of having the condition the test**\\n**identifies. Considering a positive test, what is the probability of having that**\\n**condition?**\\nLet’s suppose you are being tested for a disease, if you have the illness the test\\n\\n\\n-----\\n\\nwill end up saying you have the illness. However, if you don’t have the illness5% of the times the test will end up saying you have the illness and 95% of the\\ntimes the test will give accurate result that you don’t have the illness. Thus there\\nis a 5% error in case you do not have the illness.\\n\\nOut of 1000 people, 1 person who has the disease will get true positive result.\\n\\nOut of the remaining 999 people, 5% will also get true positive result.\\n\\nClose to 50 people will get a true positive result for the disease.\\n\\nThis means that out of 1000 people, 51 people will be tested positive for the\\ndisease even though only one person has the illness. There is only a 2%\\nprobability of you having the disease even if your reports say that you have the\\ndisease.\\n\\n**50)    How you can make data normal using Box-Cox transformation?**\\n**51)    What is the difference between Supervised Learning an**\\n**Unsupervised Learning?**\\nIf an algorithm learns something from the training data so that the knowledge\\ncan be applied to the test data, then it is referred to as Supervised Learning.\\nClassification is an example for Supervised Learning. If the algorithm does not\\nlearn anything beforehand because there is no response variable or any training\\ndata, then it is referred to as unsupervised learning. Clustering is an example for\\nunsupervised learning.\\n\\n**52) Explain the use of Combinatorics in data science.**\\n**53) Why is vectorization considered a powerful method for optimizing**\\n**numerical code?**\\n**54) What is the goal of A/B Testing?**\\nIt is a statistical hypothesis testing for randomized experiment with two variables\\nA and B. The goal of A/B Testing is to identify any changes to the web page to\\nmaximize or increase the outcome of an interest. An example for this could be\\nidentifying the click through rate for a banner ad.\\n\\n**55)    What is an Eigenvalue and Eigenvector?**\\nEigenvectors are used for understanding linear transformations. In data analysis,\\nwe usually calculate the eigenvectors for a correlation or covariance matrix.\\nEigenvectors are the directions along which a particular linear transformation\\nacts by flipping, compressing or stretching. Eigenvalue can be referred to as the\\nstrength of the transformation in the direction of eigenvector or the factor by\\nwhich the compression occurs.\\n\\n\\n-----\\n\\n**56)    What is Gradient Descent?**\\n**57)    How can outlier values be treated?**\\nOutlier values can be identified by using univariate or any other graphical\\nanalysis method. If the number of outlier values is few then they can be assessed\\nindividually but for large number of outliers the values can be substituted with\\neither the 99th or the 1st percentile values. All extreme values are not outlier\\nvalues.The most common ways to treat outlier values –\\n\\n1) To change the value and bring in within a range\\n\\n2) To just remove the value.\\n\\n**58)    How can you assess a good logistic model?**\\nThere are various methods to assess the results of a logistic regression analysis\\n-       Using Classification Matrix to look at the true negatives and false\\npositives.\\n\\n-       Concordance that helps identify the ability of the logistic model to\\ndifferentiate between the event happening and not happening.\\n\\n-       Lift helps assess the logistic model by comparing it with random\\nselection.\\n\\n**59)    What are various steps involved in an analytics project?**\\n\\n-       Understand the business problem\\n\\n-       Explore the data and become familiar with it.\\n\\n-       Prepare the data for modelling by detecting outliers, treating missing\\nvalues, transforming variables, etc.\\n\\n-       After data preparation, start running the model, analyse the result and\\ntweak the approach. This is an iterative step till the best possible outcome is\\nachieved.\\n\\n-       Validate the model using a new data set.\\n\\n-       Start implementing the model and track the result to analyse the\\nperformance of the model over the period of time.\\n\\n**60)** **How can you iterate over a list and also retrieve element indices at the**\\n**same time?**\\nThis can be done using the enumerate function which takes every element in a\\nsequence just like in a list and adds its location just before it.\\n\\n\\n-----\\n\\n**61)    During analysis, how do you treat missing values?**\\nThe extent of the missing values is identified after identifying the variables with\\nmissing values. If any patterns are identified the analyst has to concentrate on\\nthem as it could lead to interesting and meaningful business insights. If there are\\nno patterns identified, then the missing values can be substituted with mean or\\nmedian values (imputation) or they can simply be ignored.There are various\\nfactors to be considered when answering this question\\nUnderstand the problem statement, understand the data and then give the\\nanswer.Assigning a default value which can be mean, minimum or maximum\\nvalue. Getting into the data is important.\\nIf it is a categorical variable, the default value is assigned. The missing value\\nis assigned a default value.\\nIf you have a distribution of data coming, for normal distribution give the\\nmean value.\\nShould we even treat missing values is another important point to consider? If\\n80% of the values for a variable are missing then you can answer that you\\nwould be dropping the variable instead of treating the missing values.\\n\\n**62)    Explain about the box cox transformation in regression models.**\\nFor some reason or the other, the response variable for a regression analysis\\nmight not satisfy one or more assumptions of an ordinary least squares\\nregression. The residuals could either curve as the prediction increases or follow\\nskewed distribution. In such scenarios, it is necessary to transform the response\\nvariable so that the data meets the required assumptions. A Box cox\\ntransformation is a statistical technique to transform non-mornla dependent\\nvariables into a normal shape. If the given data is not normal then most of the\\nstatistical techniques assume normality. Applying a box cox transformation\\nmeans that you can run a broader number of tests.\\n\\n**63)    Can you use machine learning for time series analysis?**\\nYes, it can be used but it depends on the applications.\\n\\n**64)    Write a function that takes in two sorted lists and outputs a sorted**\\n**list that is their union.**\\nFirst solution which will come to your mind is to merge two lists and short them\\nafterwards\\n\\n**Python code-**\\ndef return_union(list_a, list_b):\\nreturn sorted(list_a + list_b)\\n\\n\\n-----\\n\\n**R code-**\\nreturn_union <- function(list_a, list_b)\\n{\\nlist_c<-list(c(unlist(list_a),unlist(list_b)))\\nreturn(list(list_c[[1]][order(list_c[[1]])]))\\n}\\nGenerally, the tricky part of the question is not to use any sorting or ordering\\nfunction. In that case you will have to write your own logic to answer the\\nquestion and impress your interviewer.\\n\\nPython codedef return_union(list_a, list_b):\\nlen1 = len(list_a)\\nlen2 = len(list_b)\\nfinal_sorted_list = []\\nj = 0\\nk = 0\\n\\nfor i in range(len1+len2):\\nif k == len1:\\nfinal_sorted_list.extend(list_b[j:])\\nbreak\\nelif j == len2:\\nfinal_sorted_list.extend(list_a[k:])\\nbreak\\nelif list_a[k] < list_b[j]:\\nfinal_sorted_list.append(list_a[k])\\nk += 1\\nelse:\\nfinal_sorted_list.append(list_b[j])\\nj += 1\\nreturn final_sorted_list\\n\\nSimilar function can be returned in R as well by following the similar steps.\\n\\nreturn_union <- function(list_a,list_b)\\n{\\n#Initializing length variables\\nlen_a <- length(list_a)\\nlen_b <- length(list_b)\\n\\n\\n-----\\n\\nlen <- len_a + len_b\\n\\n#initializing counter variables\\n\\nj=1\\nk=1\\n\\n#Creating an empty list which has length equal to sum of both the lists\\n\\nlist_c <- list(rep(NA,len))\\n\\n#Here goes our for loop\\n\\nfor(i in 1:len)\\n{\\nif(j>len_a)\\n{\\nlist_c[i:len] <- list_b[k:len_b]\\nbreak\\n}\\nelse if(k>len_b)\\n{\\nlist_c[i:len] <- list_a[j:len_a]\\nbreak\\n}\\nelse if(list_a[[j]] <= list_b[[k]])\\n{\\nlist_c[[i]] <- list_a[[j]]\\nj <- j+1\\n}\\nelse if(list_a[[j]] > list_b[[k]])\\n{\\nlist_c[[i]] <- list_b[[k]]\\nk <- k+1\\n}\\n}\\nreturn(list(unlist(list_c)))\\n\\n}\\n\\n**65)    What is the difference between Bayesian Estimate and Maximum**\\n**Likelihood Estimation (MLE)?**\\n\\n\\n-----\\n\\nIn bayesian estimate we have some knowledge about the data/problem (prior)\\n.There may be several values of the parameters which explain data and hence we\\ncan look for multiple parameters like 5 gammas and 5 lambdas that do this. As a\\nresult of Bayesian Estimate, we get multiple models for making multiple\\npredcitions i.e. one for each pair of parameters but with the same prior. So, if a\\nnew example need to be predicted than computing the weighted sum of these\\npredictions serves the purpose.\\n\\nMaximum likelihood does not take prior into consideration (ignores the prior) so\\nit is like being a Bayesian while using some kind of a flat prior.\\n\\n**66)    What is Regularization and what kind of problems does**\\n**regularization solve?**\\n**67)    What is multicollinearity and how you can overcome it?**\\n**68)    What is the curse of dimensionality?**\\n**69)    How do you decide whether your linear regression model fits the**\\n**data?**\\n**70)    What is the difference between squared error and absolute error?**\\n**71)    What is Machine Learning?**\\nThe simplest way to answer this question is – we give the data and equation to\\nthe machine. Ask the machine to look at the data and identify the coefficient\\nvalues in an equation.\\n\\nFor example for the linear regression y=mx+c, we give the data for the variable\\nx, y and the machine learns about the values of m and c from the data.\\n\\n**72) How are confidence intervals constructed and how will you interpret**\\n**them?**\\n**73) How will you explain logistic regression to an economist, physican**\\n**scientist and biologist?**\\n**74) How can you overcome Overfitting?**\\n**75) Differentiate between wide and tall data formats?**\\n**76) Is Naïve Bayes bad? If yes, under what aspects.**\\n**77) How would you develop a model to identify plagiarism?**\\n**78) How will you define the number of clusters in a clustering algorithm?**\\nThough the Clustering Algorithm is not specified, this question will mostly be\\nasked in reference to K-Means clustering where “K” defines the number of\\nclusters. The objective of clustering is to group similar entities in a way that the\\nentities within a group are similar to each other but the groups are different from\\neach other.\\n\\n\\n-----\\n\\nFor example, the following image shows three different groups.\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-22-1.png)\\n\\nWithin Sum of squares is generally used to explain the homogeneity within a\\ncluster. If you plot WSS for a range of number of clusters, you will get the plot\\nshown below. The Graph is generally known as Elbow Curve.\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-22-0.png)\\n\\nRed circled point in above graph i.e. Number of Cluster =6 is the point after\\nwhich you don’t see any decrement in WSS. This point is known as bending\\npoint and taken as K in K – Means.\\n\\nThis is the widely used approach but few data scientists also use Hierarchical\\nclustering first to create dendograms and identify the distinct groups from there.\\n\\n**79) Is it better to have too many false negatives or too many false positives?**\\n**80) Is it possible to perform logistic regression with Microsoft Excel?**\\nIt is possible to perform logistic regression with Microsoft Excel. There are two\\nways to do it using Excel.\\n\\n\\n-----\\n\\na)   One is to use Add-ins provided by many websites which we can use.\\n\\nb)   Second is to use fundamentals of logistic regression and use Excel’s\\ncomputational power to build a logistic regression\\n\\nBut when this question is being asked in an interview, interviewer is not looking\\nfor a name of Add-ins rather a method using the base excel functionalities.\\n\\nLet’s use a sample data to learn about logistic regression using Excel. (Example\\nassumes that you are familiar with basic concepts of logistic regression)\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-23-0.png)\\n\\nData shown above consists of three variables where X1 and X2 are independent\\nvariables and Y is a class variable. We have kept only 2 categories for our\\npurpose of binary logistic regression classifier.\\n\\nNext we have to create a logit function using independent variables, i.e.\\n\\nLogit = L = β0 + β1*X1 + β2*X2\\n\\n\\n-----\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-24-1.png)\\n\\nWe have kept the initial values of beta 1, beta 2 as 0.1 for now and we will\\nuse Excel Solve to optimize the beta values in order to maximize our log\\nlikelihood estimate.\\n\\nAssuming that you are aware of logistic regression basics, we calculate\\nprobability values from Logit using following formula:\\n\\nProbability= e^Logit/(1+ e^Logit )\\n\\ne is base of natural logarithm i.e. e = 2.71828163\\nLet’s put it into excel formula to calculate probability values for each of\\nthe observation.\\n\\n\\n-----\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-25-3.png)\\n\\nThe conditional probability is the probability of Predicted Y, given\\n\\nset of independent variables X.\\n\\nAnd this p can be calculated as\\nP �(X) �^Yactual*[1-P �(X) �^(1-Yactual)]\\n\\nThen we have to take natural log of the above function\\nln �[ � P �(X) �^Yactual*[1-P(X)^(1-Yactual) ]]\\n\\nWhich turns out to be –\\n\\nYactual*ln �[ � P(X)]*(Yactual- 1)*ln[1-P(X)]\\n\\nLog likelihood function LL is the sum of above equation for all the\\nobservations\\n\\n\\n-----\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-26-0.png)\\n\\nLog likelihood LL will be sum of column G, which we just calculated\\n\\n\\n-----\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-27-0.png)\\n\\nThe objective is to maximize the Log Likelihood i.e. cell H2 in this\\nexample. We have to maximize H2 by optimizing B0, B1, and B2.\\nWe’ll use Excel’s solver add-in to achieve the same.\\n\\nExcel comes with this Add-in pre-installed and you must see it under Data\\nTab in Excel as shown below\\n\\nIf you don’t see it there then make sure if you have loaded it. To load an\\nadd-in in Excel,\\n\\nGo to File >> Options >> Add-Ins and see if checkbox in front of required\\nadd-in is checked or not? Make sure to check it to load an add-in into\\nExcel.\\nIf you don’t see Solver Add-in there, go to the bottom of the screen\\n(Manage Add-Ins) and click on OK. Next you will see a popup window\\nwhich should have your Solver add-in present. Check the checkbox infront of the add-in name. If you don’t see it there as well click on browse\\nand direct it to the required folder which contains Solver Add-In.\\n\\nOnce you have your Solver loaded, click on Solver icon under Data tab\\nand You will see a new window popped up like –\\n\\n\\n-----\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-28-0.png)\\n\\nPut H2 in set objective, select max and fill cells E2 to E4 in next form\\nfield.\\nBy doing this we have told Solver to Maximize H2 by changing values\\nin cells E2 to E4.\\n\\nNow click on Solve button at the bottom –\\n\\nYou will see a popup like below \\n\\n-----\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-29-1.png)\\n\\nThis shows that Solver has found a local maxima solution but we are in need of\\nGlobal Maxima Output. Keep clicking on Continue until it shows the below\\npopup\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-29-0.png)\\n\\nIt shows that Solver was able to find and converge the solution. In case it is not\\nable to converge it will throw an error. Select “Keep Solver Solution” and Click\\non OK to accept the solution provided by Solver.\\n\\nNow, you can see that value of Beta coefficients from B0, B1 B2 have changed\\nand our Log Likelihood function has been maximized.\\n\\n\\n-----\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-30-0.png)\\n\\nUsing these values of Betas you can calculate the probability and hence response\\nvariable by deciding the probability cut-off.\\n\\n**81) What do you understand by Fuzzy merging ? Which language will you**\\n**use to handle it?**\\n**82) What is the difference between skewed and uniform distribution?**\\nWhen the observations in a dataset are spread equally across the range of\\ndistribution, then it is referred to as uniform distribution. There are no clear\\nperks in an uniform distribution. Distributions that have more observations on\\none side of the graph than the other are referred to as skewed\\ndistribution.Distributions with fewer observations on the left ( towards lower\\nvalues) are said to be skewed left and distributions with fewer observation on the\\nright ( towards higher values) are said to be skewed right.\\n\\n**83) You created a predictive model of a quantitative outcome variable using**\\n**multiple regressions. What are the steps you would follow to validate the**\\n**model?**\\nSince the question asked, is about post model building exercise, we will assume\\nthat you have already tested for null hypothesis, multi collinearity and Standard\\nerror of coefficients.\\n\\nOnce you have built the model, you should check for following –\\n\\n\\n-----\\n\\n-      Global F-test to see the significance of group of independent variables on\\ndependent variable\\n\\n-      R^2\\n\\n-      Adjusted R^2\\n\\n-      RMSE, MAPE\\n\\nIn addition to above mentioned quantitative metrics you should also check for\\n-      Residual plot\\n\\n-      Assumptions of linear regression\\n\\n**84) What do you understand by Hypothesis in the content of Machine**\\n**Learning?**\\n**85) What do you understand by Recall and Precision?**\\nRecall measures \"Of all the actual true samples how many did we classify as\\ntrue?\"\\n\\nPrecision measures \"Of all the samples we classified as true how many are\\nactually true?\"\\n\\nWe will explain this with a simple example for better understanding \\nImagine that your wife gave you surprises every year on your anniversary in last\\n12 years. One day all of a sudden your wife asks -\"Darling, do you remember all\\nanniversary surprises from me?\".\\n\\nThis simple question puts your life into danger.To save your life, you need to\\nRecall all 12 anniversary surprises from your memory. Thus, Recall(R) is the\\nratio of number of events you can correctly recall to the number of all correct\\nevents. If you can recall all the 12 surprises correctly then the recall ratio is 1\\n(100%) but if you can recall only 10 suprises correctly of the 12 then the recall\\nratio is 0.83 (83.3%).\\n\\nHowever, you might be wrong in some cases. For instance, you answer 15\\ntimes, 10 times the surprises you guess are correct and 5 wrong. This implies\\nthat your recall ratio is 100% but the precision is 66.67%.\\n\\nPrecision is the ratio of number of events you can correctly recall to a number of\\nall events you recall (combination of wrong and correct recalls).\\n\\n**86) How will you find the right K for K-means?**\\n**87) Why L1 regularizations causes parameter sparsity whereas L2**\\n\\n\\n-----\\n\\n**regularization does not?**\\nRegularizations in statistics or in the field of machine learning is used to include\\nsome extra information in order to solve a problem in a better way. L1 & L2\\nregularizations are generally used to add constraints to optimization problems.\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-32-0.png)\\n\\nIn the example shown above H0 is a hypothesis. If you observe, in L1 there is a\\nhigh likelihood to hit the corners as solutions while in L2, it doesn’t. So in\\nL1 variables are penalized more as compared to L2 which results into sparsity.\\nIn other words, errors are squared in L2, so model sees higher error and tries to\\nminimize that squared error.\\n**88) How can you deal with different types of seasonality in time series**\\n**modelling?**\\nSeasonality in time series occurs when time series shows a repeated pattern over\\ntime. E.g., stationary sales decreases during holiday season, air conditioner sales\\nincreases during the summers etc. are few examples of seasonality in a time\\nseries.\\n\\nSeasonality makes your time series non-stationary because average value of the\\nvariables at different time periods. Differentiating a time series is generally\\nknown as the best method of removing seasonality from a time series. Seasonal\\ndifferencing can be defined as a numerical difference between a particular value\\nand a value with a periodic lag (i.e. 12, if monthly seasonality is present)\\n\\n**89) In experimental design, is it necessary to do randomization? If yes, why?**\\n**90) What do you understand by conjugate-prior with respect to Naïve**\\n**Bayes?**\\n\\n\\n-----\\n\\n**91) Can you cite some examples where a false positive is important than a**\\n**false negative?**\\nBefore we start, let us understand what are false positives and what are false\\nnegatives.\\n\\nFalse Positives are the cases where you wrongly classified a non-event as an\\nevent a.k.a Type I error.\\n\\nAnd, False Negatives are the cases where you wrongly classify events as nonevents, a.k.a Type II error.\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-33-0.png)\\n\\nIn medical field, assume you have to give chemo therapy to patients. Your lab\\ntests patients for certain vital information and based on those results they decide\\nto give radiation therapy to a patient.\\n\\nAssume a patient comes to that hospital and he is tested positive for cancer (But\\nhe doesn’t have cancer) based on lab prediction. What will happen to him?\\n(Assuming Sensitivity is 1)\\n\\nOne more example might come from marketing. Let’s say an ecommerce\\ncompany decided to give $1000 Gift voucher to the customers whom they\\nassume to purchase at least $5000 worth of items. They send free voucher mail\\ndirectly to 100 customers without any minimum purchase condition because they\\nassume to make at least 20% profit on sold items above 5K.\\n\\n\\n-----\\n\\nNow what if they have sent it to false positive cases?\\n\\n92) Can you cite some examples where a false negative important than a\\n**false positive?**\\nAssume there is an airport ‘A’ which has received high security threats and based\\non certain characteristics they identify whether a particular passenger can be a\\nthreat or not. Due to shortage of staff they decided to scan passenger being\\npredicted as risk positives by their predictive model.\\n\\nWhat will happen if a true threat customer is being flagged as non-threat by\\nairport model?\\n\\nAnother example can be judicial system. What if Jury or judge decide to make a\\ncriminal go free?\\n\\nWhat if you rejected to marry a very good person based on your predictive\\nmodel and you happen to meet him/her after few years and realize that you had a\\nfalse negative?\\n\\n**93) Can you cite some examples where both false positive and false**\\n**negatives are equally important?**\\nIn the banking industry giving loans is the primary source of making money but\\nat the same time if your repayment rate is not good you will not make any profit,\\nrather you will risk huge losses.\\n\\nBanks don’t want to lose good customers and at the same point of time they\\ndon’t want to acquire bad customers. In this scenario both the false positives and\\nfalse negatives become very important to measure.\\nThese days we hear many cases of players using steroids during sport\\ncompetitions Every player has to go through a steroid test before the game starts.\\nA false positive can ruin the career of a Great sportsman and a false negative can\\nmake the game unfair.\\n\\n**94) Can you explain the difference between a Test Set and a Validation Set?**\\nValidation set can be considered as a part of the training set as it is used for\\nparameter selection and to avoid Overfitting of the model being built. On the\\nother hand, test set is used for testing or evaluating the performance of a trained\\nmachine leaning model.\\n\\nIn simple terms,the differences can be summarized as\\nTraining Set is to fit the parameters i.e. weights.\\nTest Set is to assess the performance of the model i.e. evaluating the\\n\\n\\n-----\\n\\npredictive power and generalization.\\nValidation set is to tune the parameters.\\n\\n**95) What makes a dataset gold standard?**\\n**96) What do you understand by statistical power of sensitivity and how do**\\n**you calculate it?**\\nSensitivity is commonly used to validate the accuracy of a classifier (Logistic,\\nSVM, RF etc.). Sensitivity is nothing but “Predicted TRUE events/ Total\\nevents”. True events here are the events which were true and model also\\npredicted them as true.\\n\\nCalculation of senstivity is pretty straight forward\\n**Senstivity = True Positives /Positives in Actual Dependent Variable**\\nWhere, True positives are Positive events which are correctly classified as\\nPositives.\\n\\n**97) What is the importance of having a selection bias?**\\nSelection Bias occurs when there is no appropriate randomization acheived\\nwhile selecting individuals, groups or data to be analysed.Selection bias implies\\nthat the obtained sample does not exactly represent the population that was\\nactually intended to be analyzed.Selection bias consists of Sampling Bias, Data,\\nAttribute and Time Interval.\\n\\n**98) Give some situations where you will use an SVM over a RandomForest**\\n**Machine Learning algorithm and vice-versa.**\\nSVM and Random Forest are both used in classification problems.\\n\\na)   If you are sure that your data is outlier free and clean then go for SVM. It\\nis the opposite -  if your data might contain outliers then Random forest would\\nbe the best choice\\n\\nb)   Generally, SVM consumes more computational power than Random\\nForest, so if you are constrained with memory go for Random Forest machine\\nlearning algorithm.\\n**c) Random Forest gives you a very good idea of variable importance in your**\\ndata, so if you want to have variable importance then choose Random Forest\\nmachine learning algorithm.\\nd)   Random Forest machine learning algorithms are preferred for multiclass\\nproblems.\\n\\ne)   SVM is preferred in multi-dimensional problem set - like text classification\\n\\nbut as a good data scientist, you should experiment with both of them and test\\n\\n\\n-----\\n\\nfor accuracy or rather you can use ensemble of many Machine\\nLearning techniques.\\n\\n**99) What do you understand by feature vectors?**\\n**100) How do data management procedures like missing data handling make**\\n**selection bias worse?**\\nMissing value treatment is one of the primary tasks which a data scientist is\\nsupposed to do before starting data analysis. There are multiple methods for\\nmissing value treatment. If not done properly, it could potentially result into\\nselection bias. Let see few missing value treatment examples and their impact on\\nselection\\n**Complete Case Treatment: Complete case treatment is when you remove entire**\\nrow in data even if one value is missing. You could achieve a selection bias if\\nyour values are not missing at random and they have some pattern. Assume you\\nare conducting a survey and few people didn’t specify their gender. Would you\\nremove all those people? Can’t it tell a different story?\\n**Available case analysis: Let say you are trying to calculate correlation matrix**\\nfor data so you might remove the missing values from variables which are\\nneeded for that particular correlation coefficient. In this case your values will not\\nbe fully correct as they are coming from population sets.\\n**Mean Substitution: In this method missing values are replaced with mean of**\\nother available values.This might make your distribution biased e.g., standard\\ndeviation, correlation and regression are mostly dependent on the mean value of\\nvariables.\\nHence, various data management procedures might include selection bias in your\\ndata if not chosen correctly.\\n\\n**Question 101. Explain About Data Import In R Language?**\\n\\n**Answer:**\\n**R Commander is used to import data in R language. To start the R**\\n**commander GUI, the user must type in the command Rcmdr into the**\\n**console. There are 3 different ways in which data can be imported in R**\\n**language-**\\n\\nUsers can select the data set in the dialog box or enter the name of the\\ndata set (if they know).\\nData can also be entered directly using the editor of R Commander via\\nData->New Data Set. However, this works well when the data set is not\\ntoo large.\\n\\n\\n-----\\n\\nData can also be imported from a URL or from a plain text file (ASCII),\\nfrom any other statistical package or from the clipboard.\\n\\n**Question 102. How Missing Values And Impossible Values Are**\\n**Represented In R Language?**\\n\\n**Answer:**\\nNaN (Not a Number) is used to represent impossible values whereas NA (Not\\nAvailable) is used to represent missing values. The best way to answer this\\nquestion would be to mention that deleting missing values is not a good idea\\nbecause the probable cause for missing value could be some problem with\\ndata collection or programming or the query. It is good to find the root cause\\nof the missing values and then take necessary steps handle them.\\n\\n**Question 103. R Language Has Several Packages For Solving A Particular**\\n**Problem. How Do You Make A Decision On Which One Is The Best To**\\n**Use?**\\n\\n**Answer:**\\nCRAN package ecosystem has more than 6000 packages. The best way for\\nbeginners to answer this question is to mention that they would look for a\\npackage that follows good software development principles. The next thing\\nwould be to look for user reviews and find out if other data scientists or\\nanalysts have been able to solve a similar problem.\\n\\n**Question 104. Which Function In R Language Is Used To Find Out**\\n**Whether The Means Of 2 Groups Are Equal To Each Other Or Not?**\\n\\n**Answer:**\\nt.tests ()\\n\\n**Question 105. What Is The Best Way To Communicate The Results Of**\\n**Data Analysis Using R Language?**\\n\\n**Answer:**\\nThe best possible way to do this is combine the data, code and analysis results\\nin a single document using knitr for reproducible research. This helps others\\nto verify the findings, add to them and engage in discussions. Reproducible\\nresearch makes it easy to redo the experiments by inserting new data and\\napplying it to a different problem.\\n\\n**Question 106. How Many Data Structures Does R Language Have?**\\n\\n**Answer:**\\nR language has Homogeneous and Heterogeneous data structures.\\n**Homogeneous data structures have same type of objects – Vector, Matrix**\\nad Array.\\n**Heterogeneous data structures have different type of objects – Data**\\n\\n\\n-----\\n\\nframes and lists.\\n\\n**Question 107. What Is The Process To Create A Table In R Language**\\n**Without Using External Files?**\\n\\n**Answer:**\\nMyTable= data.frame ()\\nedit (MyTable)\\nThe above code will open an Excel Spreadsheet for entering data into\\nMyTable.\\nLearn Data Science in R Programming to land a top gig as an Enterprise Data\\nScientist!\\n\\n**Question 108. Explain About The Significance Of Transpose In R**\\n**Language?**\\n\\n**Answer:**\\nTranspose t () is the easiest method for reshaping the data before analysis.\\n\\n**Question 109. What Are With () And By () Functions Used For?**\\n\\n**Answer:**\\nWith () function is used to apply an expression for a given dataset and BY ()\\nfunction is used for applying a function each level of factors.\\n\\n**Question 110. Dplyr Package Is Used To Speed Up Data Frame**\\n**Management Code. Which Package Can Be Integrated With Dplyr For**\\n**Large Fast Tables?**\\n\\n**Answer:**\\ndata.table\\n\\n**Question 111. In Base Graphics System, Which Function Is Used To Add**\\n**Elements To A Plot?**\\n\\n**Answer:**\\nboxplot () or text ()\\n\\n**Question 112. What Are The Different Type Of Sorting Algorithms**\\n**Available In R Language?**\\n\\n**Answer:**\\n\\nBucket Sort\\nSelection Sort\\nQuick Sort\\nBubble Sort\\nMerge Sort\\n\\n**Question 113. What Is The Command Used To Store R Objects In A File?**\\n\\n**Answer:**\\n\\n\\n-----\\n\\nsave (x, file=”x.Rdata”)\\n\\n**Question 114. What Is The Best Way To Use Hadoop And R Together For**\\n**Analysis?**\\n\\n**Answer:**\\nHDFS can be used for storing the data for long-term. MapReduce jobs\\nsubmitted from either Oozie, Pig or Hive can be used to encode, improve and\\nsample the data sets from HDFS into R. This helps to leverage complex\\nanalysis tasks on the subset of data prepared in R.\\n\\n**Question 115. What Will Be The Output Of Log (-5.8) When Executed On**\\n**R Console?**\\n\\n**Answer:**\\nExecuting the above on R console will display a warning sign that NaN (Not a\\nNumber) will be produced because it is not possible to take the log of\\nnegative number.\\n\\n**Question 116. How Is A Data Object Represented Internally In R**\\n**Language?**\\n\\n**Answer:**\\nunclass (as.Date (“2016-10-05″))\\n\\n**Question 117. Which Package In R Supports The Exploratory Analysis Of**\\n**Genomic Data?**\\n\\n**Answer:**\\nAdegenet.\\n\\n**Question 118. What Is The Difference Between Data Frame And A Matrix**\\n**In R?**\\n\\n**Answer:**\\nData frame can contain heterogeneous inputs while a matrix cannot. In matrix\\nonly similar data types can be stored whereas in a data frame there can be\\ndifferent data types like characters, integers or other data frames.\\n\\n**Question 119. How Can You Add Datasets In R?**\\n\\n**Answer:**\\nrbind () function can be used add datasets in R language provided the columns\\nin the datasets should be same.\\n\\n**Question 120. What Are Factor Variable In R Language?**\\n\\n**Answer:**\\nFactor variables are categorical variables that hold either string or numeric\\nvalues. Factor variables are used in various types of graphics and particularly\\nfor statistical modelling where the correct number of degrees of freedom is\\nassigned to them.\\n\\n\\n-----\\n\\n**Question 121. What Is The Memory Limit In R?**\\n\\n**Answer:**\\n8TB is the memory limit for 64-bit system memory and 3GB is the limit for\\n32-bit system memory.\\n\\n**Question 122. What Are The Data Types In R On Which Binary Operators**\\n**Can Be Applied?**\\n\\n**Answer:**\\nScalars, Matrices ad Vectors.\\n\\n**Question 123. How Do You Create Log Linear Models In R Language?**\\n\\n**Answer:**\\nUsing the loglm () function\\n\\n**Question 124. What Will Be The Class Of The Resulting Vector If You**\\n**Concatenate A Number And Na?**\\n\\n**Answer:**\\nnumber\\n\\n**Question 125. What Is Meant By K-nearest Neighbour?**\\n\\n**Answer:**\\nK-Nearest Neighbour is one of the simplest machine learning classification\\nalgorithms that is a subset of supervised learning based on lazy learning. In\\nthis algorithm the function is approximated locally and any computations are\\ndeferred until classification.\\n\\n**Question 126. What Will Be The Class Of The Resulting Vector If You**\\n**Concatenate A Number And A Character?**\\n\\n**Answer:**\\ncharacter\\n\\n**Question 127. If You Want To Know All The Values In C (1, 3, 5, 7, 10)**\\n**That Are Not In C (1, 5, 10, 12, 14). Which In-built Function In R Can Be**\\n**Used To Do This? Also, How This Can Be Achieved Without Using The In-**\\n**built Function?**\\n\\n**Answer:**\\nUsing in-built function - setdiff(c (1, 3, 5, 7, 10), c (1, 5, 10, 11, 13))\\nWithout using in-built function - c (1, 3, 5, 7, 10) [! c (1, 3, 5, 7, 10) %in% c\\n(1, 5, 10, 11, 13).\\n\\n**Question 128. How Can You Debug And Test R Programming Code?**\\n\\n**Answer:**\\nR code can be tested using Hadley’s testthat package.\\n\\n**Question 129. What Will Be The Class Of The Resulting Vector If You**\\n\\n\\n-----\\n\\n**Concatenate A Number And A Logical?**\\n\\n**Answer:**\\nNumber.\\n\\n**Question 130. Write A Function In R Language To Replace The Missing**\\n**Value In A Vector With The Mean Of That Vector?**\\n\\n**Answer:**\\nmean impute <- function(x) {x [is.na(x)] <- mean(x, na.rm = TRUE); x}\\n\\n**Question 131. What Happens If The Application Object Is Not Able To**\\n**Handle An Event?**\\n\\n**Answer:**\\nThe event is dispatched to the delegate for processing.\\n\\n**Question 132. Differentiate Between Lapply And Sapply?**\\n\\n**Answer:**\\nIf the programmers want the output to be a data frame or a vector, then sapply\\nfunction is used whereas if a programmer wants the output to be a list then\\nlapply is used. There one more function known as vapply which is preferred\\nover sapply as vapply allows the programmer to specific the output type. The\\ndisadvantage of using vapply is that it is difficult to be implemented and more\\nverbose.\\n\\n**Question 133. Differentiate Between Seq (6) And Seq_along (6)?**\\n\\n**Answer:**\\nSeq_along(6) will produce a vector with length 6 whereas seq(6) will produce\\na sequential vector from 1 to 6 c( (1,2,3,4,5,6)).\\n\\n**Question 134. How Will You Read A .csv File In R Language?**\\n\\n**Answer:**\\nread.csv () function is used to read a .csv file in R language.\\n**Below is a simple example –**\\nfilcontent\\nprint (filecontent)\\n\\n**Question 135. How Do You Write R Commands?**\\n\\n**Answer:**\\nThe line of code in R language should begin with a hash symbol (#).\\n\\n**Question 136. How Can You Verify If A Given Object “x” Is A Matric Data**\\n**Object?**\\n\\n**Answer:**\\nIf the function call is.matrix(X ) returns TRUE then X can be termed as a\\nmatrix data object.\\n\\n\\n-----\\n\\n**Question 137. What Do You Understand By Element Recycling In R?**\\n\\n**Answer:**\\nIf two vectors with different lengths perform an operation –the elements of the\\nshorter vector will be re-used to complete the operation. This is referred to as\\nelement recycling.\\nExample – Vector A <-c(1,2,0,4) and Vector B<-(3,6) then the result of A*B\\nwill be ( 3,12,0,24). Here 3 and 6 of vector B are repeated when computing\\nthe result.\\n\\n**Question 138. How Can You Verify If A Given Object “x” Is A Matrix**\\n**Data Object?**\\n\\n**Answer:**\\nIf the function call is.matrix(X) returns true then X can be considered as a\\nmatrix data object otheriwse not.\\n\\n**Question 139. How Will You Measure The Probability Of A Binary**\\n**Response Variable In R Language?**\\n\\n**Answer:**\\nLogistic regression can be used for this and the function glm () in R language\\nprovides this functionality.\\n\\n**Question 140. What Is The Use Of Sample And Subset Functions In R**\\n**Programming Language?**\\n\\n**Answer:**\\n\\nSample () function can be used to select a random sample of size ‘n’\\nfrom a huge dataset.\\nSubset () function is used to select variables and observations from a\\ngiven dataset.\\n\\n**Question 141. How Can You Resample Statistical Tests In R Language?**\\n\\n**Answer:**\\nCoin package in R provides various options for re-randomization and\\npermutations based on statistical tests. When test assumptions cannot be met\\nthen this package serves as the best alternative to classical methods as it does\\nnot assume random sampling from well-defined populations.\\n\\n**Question 142. What Is The Purpose Of Using Next Statement In R**\\n**Language?**\\n\\n**Answer:**\\nIf a developer wants to skip the current iteration of a loop in the code without\\nterminating it then they can use the next statement. Whenever the R parser\\ncomes across the next statement in the code, it skips evaluation of the loop\\nfurther and jumps to the next iteration of the loop.\\n\\n\\n-----\\n\\n**Question 143. How Will You Create Scatter Plot Matrices In R Language?**\\n\\n**Answer:**\\nA matrix of scatter plots can be produced using pairs. Pairs function takes\\nvarious parameters like formula, data, subset, labels, etc.\\n**The two key parameters required to build a scatter plot matrix are –**\\n**formula- A formula basically like ~a+b+c . Each term gives a separate**\\nvariable in the pairs plots where the terms should be numerical vectors. It\\nbasically represents the series of variables used in pairs.\\n**data- It basically represents the dataset from which the variables have to be**\\ntaken for building a scatterplot.\\n\\n**Question 144. How Will You Check If An Element 25 Is Present In A**\\n**Vector?**\\n\\n**Answer:**\\n**There are various ways to do this-**\\n\\nIt can be done using the match () function- match () function returns the\\nfirst appearance of a particular element.\\nThe other is to use %in% which returns a Boolean value either true or\\nfalse.\\nIs.element () function also returns a Boolean value either true or false\\nbased on whether it is present in a vector or not.\\n\\n**Question 145. What Is The Difference Between Library() And Require()**\\n**Functions In R Language?**\\n\\n**Answer:**\\nThere is no real difference between the two if the packages are not being\\nloaded inside the function. require () function is usually used inside function\\nand throws a warning whenever a particular package is not found. On the flip\\nside, library () function gives an error message if the desired package cannot\\nbe loaded.\\n\\n**Question 146. What Are The Rules To Define A Variable Name In R**\\n**Programming Language?**\\n\\n**Answer:**\\nA variable name in R programming language can contain numeric and\\nalphabets along with special characters like dot (.) and underline (-). Variable\\nnames in R language can begin with an alphabet or the dot symbol. However,\\nif the variable name begins with a dot symbol it should not be a followed by a\\nnumeric digit.\\n\\n**Question 147. What Do You Understand By A Workspace In R**\\n**Programming Language?**\\n\\n\\n-----\\n\\n**Answer:**\\nThe current R working environment of a user that has user defined objects\\nlike lists, vectors, etc. is referred to as Workspace in R language.\\n\\n**Question 148. Which Function Helps You Perform Sorting In R**\\n**Language?**\\n\\n**Answer:**\\nOrder ()\\n\\n**Question 149. How Will You List All The Data Sets Available In All R**\\n**Packages?**\\n\\n**Answer:**\\n**Using the below line of code-**\\ndata(package = .packages(all.available = TRUE))\\n\\n**Question 150. Which Function Is Used To Create A Histogram**\\n**Visualisation In R Programming Language?**\\n\\n**Answer:**\\nHist()\\n\\n**Question 151. Write The Syntax To Set The Path For Current Working**\\n**Directory In R Environment?**\\n\\n**Answer:**\\nSetwd(“dir_path”)\\n\\n**Question 152. What Will Be The Output Of Runif (7)?**\\n\\n**Answer:**\\nIt will generate 7 random numbers between 0 and 1.\\n\\n**Question 153. What Is The Difference Between Rnorm And Runif**\\n**Functions?**\\n\\n**Answer:**\\nrnorm function generates \"n\" normal random numbers based on the mean and\\nstandard deviation arguments passed to the function.\\n**Syntax of rnorm function -**\\nrnorm(n, mean =, sd = )\\nrunif function generates \"n\" unform random numbers in the interval of\\nminimum and maximum values passed to the function.\\n**Syntax of runif function -**\\nrunif(n, min =, max = )\\n\\n**Question 157. What Will Be The Output On Executing The Following R**\\n**Programming Code ?**\\n\\n**Answer:**\\n\\n\\n-----\\n\\nmat<-matrix(rep(c(TRUE,FALSE),8),nrow=4)\\nsum(mat)\\n8\\n\\n**Question 158. How Will You Combine Multiple Different String Like**\\n**“data”, “science”, “in”,“r”, “programming” As A Single String**\\n**“data_science_in_r_programmming” ?**\\n\\n**Answer:**\\npaste(“Data”, “Science”, “in”,“R”, “Programming”,sep=\"_\")\\n\\n**Question 160. Write A Function To Extract The First Name From The**\\n**String “mr. Tom White”?**\\n\\n**Answer:**\\nsubstr (“Mr. Tom White”,start=5, stop=7)\\n\\n**Question 161. Can You Tell If The Equation Given Below Is Linear Or Not**\\n**?**\\n\\n**Answer:**\\nEmp_sal= 2000+2.5(emp_age)2\\nYes it is a linear equation as the coefficients are linear.\\n\\n**Question 162. What Is R Base Package?**\\n\\n**Answer:**\\nR Base package is the package that is loaded by default whenever R\\nprogramming environent is loaded .R base package provides basic\\nfucntionalites in R environment like arithmetic calcualtions, input/output.\\n\\n**Question 164. How Will You Merge Two Dataframes In R Programming**\\n**Language?**\\n\\n**Answer:**\\nMerge () function is used to combine two dataframes and it identifies common\\nrows or columns between the 2 dataframes. Merge () function basically finds\\nthe intersection between two different sets of data.\\n**Merge () function in R language takes a long list of arguments as follows**\\n**–**\\n**Syntax for using Merge function in R language -**\\nmerge (x, y, by.x, by.y, all.x or all.y or all )\\nX represents the first dataframe.\\nY represents the second dataframe.\\n**by.X- Variable name in dataframe X that is common in Y.**\\n**by.Y- Variable name in dataframe Y that is common in X.**\\n**all.x - It is a logical value that specifies the type of merge. all.X should be set**\\n\\n\\n-----\\n\\nto true, if we want all the observations from dataframe X . This results in Left\\nJoin.\\n**all.y - It is a logical value that specifies the type of merge. all.y should be set**\\nto true, if we want all the observations from dataframe Y . This results in\\nRight Join.\\n**all – The default value for this is set to FALSE which means that only**\\nmatching rows are returned resulting in Inner join. This should be set to true if\\nyou want all the observations from dataframe X and Y resulting in Outer join.\\n\\n**Question 167. What Will Be The Result Of Multiplying Two Vectors In R**\\n**Having Different Lengths?**\\n\\n**Answer:**\\nThe multiplication of the two vectors will be performed and the output will be\\ndisplayed with a warning message like – “Longer object length is not a\\nmultiple of shorter object length.” Suppose there is a vector a<-c (1, 2, 3) and\\nvector b <- (2, 3) then the multiplication of the vectors a*b will give the\\nresultant as 2 6 6 with the warning message. The multiplication is performed\\nin a sequential manner but since the length is not same, the first element of the\\nsmaller vector b will be multiplied with the last element of the larger vector a.\\n\\n**Question 168. R Programming Language Has Several Packages For Data**\\n**Science Which Are Meant To Solve A Specific Problem, How Do You**\\n**Decide Which One To Use?**\\n\\n**Answer:**\\nCRAN package repository in R has more than 6000 packages, so a data\\nscientist needs to follow a well-defined process and criteria to select the right\\none for a specific task. When looking for a package in the CRAN repository a\\ndata scientist should list out all the requirements and issues so that an ideal R\\npackage can address all those needs and issues.\\nThe best way to answer this question is to look for an R package that follows\\ngood software development principles and practices. For example, you might\\nwant to look at the quality documentation and unit tests. The next step is to\\ncheck out how a particular R package is used and read the reviews posted by\\nother users of the R package. It is important to know if other data scientists or\\ndata analysts have been able to solve a similar problem as that of yours. When\\nyou in doubt choosing a particular R package, I would always ask for\\nfeedback from R community members or other colleagues to ensure that I am\\nmaking the right choice.\\n\\n**Question 189. How Can You Merge Two Data Frames In R Language?**\\n\\n**Answer:**\\n\\n\\n-----\\n\\nData frames in R language can be merged manually using cbind () functions\\nor by using the merge () function on common rows or columns.\\n\\n**Question 170. Explain The Usage Of Which() Function In R Language?**\\n\\n**Answer:**\\nwhich() function determines the position of elements in a logical vector that\\nare TRUE. In the below example, we are finding the row number wherein the\\nmaximum value of variable v1 is recorded.\\nmydata=data.frame(v1 = c(2,4,12,3,6))\\nwhich(mydata$v1==max(mydata$v1))\\nIt returns 3 as 12 is the maximum value and it is at 3rd row in the variable\\nx=v1.\\n\\n**Question 170. How Will You Convert A Factor Variable To Numeric In R**\\n**Language ?**\\n\\n**Answer:**\\nA factor variable can be converted to numeric using the as.numeric() function\\nin R language. However, the variable first needs to be converted to character\\nbefore being converted to numberic because the as.numeric() function in R\\ndoes not return original values but returns the vector of the levels of the factor\\nvariable.\\nX <- factor(c(4, 5, 6, 6, 4))\\nX1 = as.numeric(as.character(X))\\n\\n**171. What is the difference between supervised and unsupervised machine**\\n**learning?**\\n\\nSupervised Machine learning:\\n\\nSupervised machine learning requires training labeled data.\\n\\nUnsupervised Machine learning:\\n\\nUnsupervised machine learning doesn’t required labeled data.\\n\\n**172. What is bias, variance trade off ?**\\n\\nBias:\\n\\n“Bias is error introduced in your model due to over simplification of machine\\nlearning algorithm.” It can lead to underfitting. When you train your model at\\n\\n\\n-----\\n\\nthat time model makes simplified assumptions to make the target function easier\\nto understand.\\n\\nLow bias machine learning algorithms - Decision Trees, k-NN and SVM\\n\\nHight bias machine learning algorithms - Liear Regression, Logistic Regression\\n\\nVariance:\\n\\n“Variance is error introduced in your model due to complex machine learning\\nalgorithm, your model learns noise also from the training dataset and performs\\nbad on test dataset.” It can lead high sensitivity and overfitting.\\n\\nNormally, as you increase the complexity of your model, you will see a\\nreduction in error due to lower bias in the model. However, this only happens till\\na particular point. As you continue to make your model more complex, you end\\nup over-fitting your model and hence your model will start suffering from high\\nvariance.\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-48-0.png)\\n\\nBias, Variance trade off:\\n\\nThe goal of any supervised machine learning algorithm is to have low bias and\\nlow variance to achive good prediction performance.\\n\\n1. The k-nearest neighbors algorithm has low bias and high variance, but the\\n\\ntrade-off can be changed by increasing the value of k which increases the\\nnumber of neighbors that contribute to the prediction and in turn increases the\\n\\n\\n-----\\n\\nbias of the model.\\n\\n2. The support vector machine algorithm has low bias and high variance, but the\\n\\ntrade-off can be changed by increasing the C parameter that influences the\\nnumber of violations of the margin allowed in the training data which\\nincreases the bias but decreases the variance.\\n\\nThere is no escaping the relationship between bias and variance in machine\\nlearning.\\n\\nIncreasing the bias will decrease the variance. Increasing the variance will\\ndecrease the bias.\\n\\n**173. What is exploding gradients ?**\\n\\n“Exploding gradients are a problem where large error gradients accumulate\\nand result in very large updates to neural network model weights during\\ntraining.” At an extreme, the values of weights can become so large as to\\noverflow and result in NaN values.\\n\\nThis has the effect of your model being unstable and unable to learn from your\\ntraining data. Now let’s understand what is the gradient.\\n\\nGradient:\\n\\nGradient is the direction and magnitude calculated during training of a neural\\nnetwork that is used to update the network weights in the right direction and by\\nthe right amount.\\n\\n**174. What is a confusion matrix ?**\\n\\nThe confusion matrix is a 2X2 table that contains 4 outputs provided by\\nthe binary classifier. Various measures, such as error-rate, accuracy, specificity,\\nsensitivity, precision and recall are derived from it. Confusion Matrix\\n\\n\\n-----\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-50-0.png)\\n\\nA dataset used for performance evaluation is called test dataset. It should contain\\nthe correct labels and predicted labels.\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-50-1.png)\\n\\nThe predicted labels will exactly the same if the performance of a binary\\nclassfier is perfect.\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-50-2.png)\\n\\nThe predicted labels usually match with part of the observed labels in real world\\nscenarios.\\n\\n\\n-----\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-51-0.png)\\n\\nA binary classifier predicts all data instances of a test dataset as either positive or\\nnegative. This produces four outcomes\\n1. True positive(TP) - Correct positive prediction\\n2. False positive(FP) - Incorrect positive prediction\\n3. True negative(TN) - Correct negative prediction\\n4. False negative(FN) - Incorrect negative prediction\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-51-1.png)\\n\\n**Basic measures derived from the confusion matrix**\\n1. Error Rate = (FP+FN)/(P+N)\\n2. Accuracy = (TP+TN)/(P+N)\\n3. Sensitivity(Recall or True positive rate) = TP/P\\n4. Specificity(True negative rate) = TN/N\\n5. Precision(Positive predicted value) = TP/(TP+FP)\\n6. F-Score(Harmonic mean of precision and recall) = (1+b)\\n\\n(PREC.REC)/(b^2PREC+REC) where b is commonly 0.5, 1, 2.\\n\\n**176. Explain how a ROC curve works ?**\\n\\nThe ROC curve is a graphical representation of the contrast between true\\npositive rates and false positive rates at various thresholds. It is often used as a\\n\\n\\n-----\\n\\nproxy for the trade-off between the sensitivity(true positive rate) and false\\npositive rate.\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-52-0.png)\\n\\n**177. What is selection Bias ?**\\n\\nSelection bias occurs when sample obtained is not represantative of the\\npopulation intended to be analyzed.\\n\\n**178. Explain SVM machine learning algorithm in detail.**\\n\\nSVM stands for support vector machine, it is a supervised machine learning\\nalgorithm which can be used for both Regression and Classification. If you\\nhave n features in your training dataset, SVM tries to plot it in n-dimentional\\nspace with the value of each feature being the value of a particular coordinate.\\nSVM uses hyper planes to seperate out different classes based on the provided\\nkernel function.\\n\\n\\n-----\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-53-0.png)\\n\\n**179. What are support vectors in SVM.**\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-53-1.png)\\n\\nIn the above diagram we see that the thinner lines mark the distance from the\\nclassifier to the closest data points called the support vectors (darkened data\\npoints). The distance between the two thin lines is called the margin.\\n\\n**180. What are the different kernels functions in SVM ?**\\n\\nThere are four types of kernels in SVM.\\n\\n1. Linear Kernel\\n\\n\\n-----\\n\\n2. Polynomial kernel\\n3. Radial basis kernel\\n4. Sigmoid kernel\\n\\n**181. Explain Decision Tree algorithm in detail.**\\n\\nDecision tree is a supervised machine learning algorithm mainly used for\\nthe Regression and Classification.It breaks down a dataset into smaller and\\nsmaller subsets while at the same time an associated decision tree is\\nincrementally developed. The final result is a tree with decision nodes and leaf\\nnodes. Decision tree can handle both categorical and numerical data.\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-54-0.png)\\n\\n**182. What is Entropy and Information gain in Decision tree algorithm ?**\\n\\nThe core algorithm for building decision tree is\\ncalled ID3. ID3 uses Enteropy and Information Gain to construct a decision\\ntree.\\n**Entropy**\\nA decision tree is built top-down from a root node and involve partitioning of\\ndata into homogenious subsets. ID3 uses enteropy to check the homogeneity of a\\nsample. If the sample is completely homogenious then entropy is zero and if the\\nsample is an equally divided it has entropy of one.\\n\\n\\n-----\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-55-0.png)\\n\\n**Information Gain**\\nThe Information Gain is based on the decrease in entropy after a dataset is split\\non an attribute. Constructing a decision tree is all about finding attributes that\\nreturns the highest information gain.\\n\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-55-1.png)\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-55-2.png)\\n\\n-----\\n\\n**183. What is pruning in Decision Tree ?**\\n\\nWhen we remove sub-nodes of a decision node, this procsss is called pruning or\\nopposite process of splitting.\\n\\n**184. What is Ensemble Learning ?**\\n\\nEnsemble is the art of combining diverse set of learners(Individual models)\\ntogather to improvise on the stability and predictive power of the model.\\nEnsemble learning has many types but two more popular ensemble learning\\ntechniques are mentioned below.\\n\\n**Bagging**\\n\\nBagging tries to implement similar learners on small sample populations and\\nthen takes a mean of all the predictions. In generalized bagging, you can use\\ndifferent learners on different population. As you expect this helps us to reduce\\nthe variance error.\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-56-0.png)\\n\\n**Boosting**\\n\\nBoosting is an iterative technique which adjust the weight of an observation\\nbased on the last classification. If an observation was classfied incorrectly, it\\ntries to increase the weight of this observation and vice versa. Boosting in\\ngeneral decreases the bias error and builds strong predictive models. However,\\n\\n\\n-----\\n\\nthey may overfit on the training data.\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-57-0.png)\\n\\n**185. What is Random Forest? How does it work ?**\\n\\nRandom forest is a versatile machine learning method capable of performing\\nboth regression and classification tasks. It is also used for dimentionality\\nreduction, treats missing values, outlier values. It is a type of ensemble learning\\nmethod, where a group of weak models combine to form a powerful model.\\n\\nIn Random Forest, we grow multiple trees as opposed to a single tree. To\\nclassify a new object based on attributes, each tree gives a classification. The\\nforest chooses the classification having the most votes(Over all the trees in the\\nforest) and in case of regression, it takes the average of outputs by different\\ntrees.\\n\\n**186. What cross-validation technique would you use on a time series dataset.**\\n\\nInstead of using k-fold cross-validation, you should be aware to the fact that a\\ntime series is not randomly distributed data - It is inherently ordered by\\nchronological order.\\n\\nIn case of time series data, you should use techniques like forward chaining –\\nWhere you will be model on past data then look at forward-facing data.\\n\\nfold 1: training[1], test[2]\\n\\nfold 1: training[1 2], test[3]\\n\\nfold 1: training[1 2 3], test[4]\\n\\nfold 1: training[1 2 3 4], test[5]\\n\\n\\n-----\\n\\n**187. What is logistic regression? Or State an example when you have used**\\n**logistic regression recently.**\\n\\nLogistic Regression often referred as logit model is a technique to predict the\\nbinary outcome from a linear combination of predictor variables. For example, if\\nyou want to predict whether a particular political leader will win the election or\\nnot. In this case, the outcome of prediction is binary i.e. 0 or 1 (Win/Lose). The\\npredictor variables here would be the amount of money spent for election\\ncampaigning of a particular candidate, the amount of time spent in campaigning,\\netc.\\n\\n**188. What do you understand by the term Normal Distribution?**\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-58-0.png)\\n\\nData is usually distributed in different ways with a bias to the left or to the right\\nor it can all be jumbled up. However, there are chances that data is distributed\\naround a central value without any bias to the left or right and reaches normal\\ndistribution in the form of a bell shaped curve. The random variables are\\ndistributed in the form of an symmetrical bell shaped curve.\\n\\n**189. What is a Box Cox Transformation?**\\n\\nDependent variable for a regression analysis might not satisfy one or more\\nassumptions of an ordinary least squares regression. The residuals could either\\ncurve as the prediction increases or follow skewed distribution. In such\\nscenarios, it is necessary to transform the response variable so that the data\\nmeets the required assumptions. A Box cox transformation is a statistical\\ntechnique to transform non-normal dependent variables into a normal shape. If\\nthe given data is not normal then most of the statistical techniques assume\\nnormality. Applying a box cox transformation means that you can run a broader\\nnumber of tests.\\n\\n\\n-----\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-59-0.png)\\n\\nA Box Cox transformation is a way to transform non-normal dependent variables\\ninto a normal shape. Normality is an important assumption for many statistical\\ntechniques, if your data isn’t normal, applying a Box-Cox means that you are\\nable to run a broader number of tests. The Box Cox transformation is named\\nafter statisticians George Box and Sir David Roxbee Cox who collaborated on a\\n1964 paper and developed the technique.\\n\\n**190. How will you define the number of clusters in a clustering algorithm?**\\n\\nThough the Clustering Algorithm is not specified, this question will mostly be\\nasked in reference to K-Means clustering where “K” defines the number of\\nclusters. For example, the following image shows three different groups.\\n\\n\\n-----\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-60-0.png)\\n\\nWithin Sum of squares is generally used to explain the homogeneity within a\\ncluster. If you plot WSS for a range of number of clusters, you will get the plot\\nshown below. The Graph is generally known as Elbow Curve.\\n\\nRed circled\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-60-1.png)\\n\\npoint in above graph i.e. Number of Cluster =6 is the point after which you don’t\\nsee any decrement in WSS. This point is known as bending point and taken as K\\nin K – Means.This is the widely used approach but few data scientists also use\\nHierarchical clustering first to create dendograms and identify the distinct groups\\nfrom there.\\n\\n**191. What is deep learning?**\\n\\nDeep learning is subfield of machine learning inspired by structure and function\\nof brain called artificial neural network. We have a lot numbers of algorithms\\nunder machine learning like Linear regression, SVM, Neural network etc and\\ndeep learning is just an extention of Neural networks. In neural nets we consider\\n\\n\\n-----\\n\\nsmall number of hidden layers but when it comes to deep learning algorithms we\\nconsider a huge number of hidden latyers to better understand the input output\\nrelationship.\\n\\n**192. What are Recurrent Neural Networks(RNNs) ?**\\n\\nRecurrent nets are type of artifical neural networks designed to recognize pattern\\nfrom the sequence of data such as Time series, stock market and goverment\\nagencis etc. To understand recurrent nets, first you have to understand the basics\\nof feedforward nets. Both these networks RNN and feedforward named after the\\nway they channel information throgh a series of mathematical oprations\\nperformed at the nodes of the network. One feeds information throgh\\nstraight(never touching same node twice), while the other cycles it throgh loop,\\nand the latter are called recurrent.\\n\\n\\n-----\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-62-0.png)\\n\\nRecurrent networks on the other hand, take as their input not just the current\\ninput example they see, but also the what they have percieved previously in\\ntime. The BTSXPE at the bottom of the drawing represents the input example in\\nthe current moment, and CONTEXT UNIT represents the output of the previous\\nmoment. The decision a recurrent neural network reached at time t-1 affects the\\ndecision that it will reach one moment later at time t. So recurrent networks have\\ntwo sources of input, the present and the recent past, which combine to\\ndetermine how they respond to new data, much as we do in life.\\n\\nThe error they generate will return via backpropagation and be used to adjust\\ntheir weights until error can’t go any lower. Remember, the purpose of recurrent\\nnets is to accurately classify sequential input. We rely on the backpropagation of\\nerror and gradient descent to do so.\\n\\n\\n-----\\n\\nBackpropagation in feedforward networks moves backward from the final error\\nthrough the outputs, weights and inputs of each hidden layer, assigning those\\nweights responsibility for a portion of the error by calculating their partial\\nderivatives – ∂E/∂w, or the relationship between their rates of change. Those\\nderivatives are then used by our learning rule, gradient descent, to adjust the\\nweights up or down, whichever direction decreases error.\\n\\nRecurrent networks rely on an extension of backpropagation called\\nbackpropagation through time, or BPTT. Time, in this case, is simply expressed\\nby a well-defined, ordered series of calculations linking one time step to the\\nnext, which is all backpropagation needs to work.\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-63-0.png)\\n\\n**193. What is the difference between machine learning and deep learning?**\\n\\nMachine learning:\\n\\nMachine learning is a field of computer science that gives computers the ability\\nto learn without being explicitly programmed. Machine learning can be\\ncategorized in following three categories.\\n\\n1. Supervised machine learning,\\n2. Unsupervised machine learning,\\n3. Reinforcement learning\\n\\nDeep learning:\\n\\nDeep Learning is a subfield of machine learning concerned with algorithms\\n\\n\\n-----\\n\\ninspired by the structure and function of the brain called artificial neural\\nnetworks.\\n\\n**194. What is reinforcement learning ?**\\n\\n**Reinforcement learning**\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-64-0.png)\\n\\nReinforcement Learning is learning what to do and how to map situations to\\nactions. The end result is to maximize the numerical reward signal. The learner\\nis not told which action to take, but instead must discover which action will yield\\nthe maximum reward.Reinforcement learning is inspired by the learning of\\nhuman beings, it is based on the reward/panelity mechanism.\\n\\n**195. What is selection bias ?**\\n\\n**Selection Bias**\\n\\nSelection bias is the bias introduced by the selection of individuals, groups or\\ndata for analysis in such a way that proper randomization is not achieved,\\nthereby ensuring that the sample obtained is not representative of the population\\nintended to be analyzed. It is sometimes referred to as the selection effect. The\\nphrase “selection bias” most often refers to the distortion of a statistical analysis,\\nresulting from the method of collecting samples. If the selection bias is not taken\\ninto account, then some conclusions of the study may not be accurate.\\n\\n**196. Explain what regularization is and why it is useful.**\\n\\n**Regularization**\\n\\nRegularization is the process of adding tunning parameter to a model to induce\\nsmoothness in order to prevent overfitting. This is most often done by adding a\\n\\n\\n-----\\n\\nconstant multiple to an existing weight vector. This constant is often the\\nL1(Lasso) or L2(ridge). The model predictions should then minimize the loss\\nfunction calculated on the regularized training set.\\n\\n**197. What is TF/IDF vectorization ?**\\n\\ntf–idf is short for term frequency–inverse document frequency, is a numerical\\nstatistic that is intended to reflect how important a word is to a document in a\\ncollection or corpus. It is often used as a weighting factor in information\\nretrieval and text mining. The tf-idf value increases proportionally to the number\\nof times a word appears in the document, but is offset by the frequency of the\\nword in the corpus, which helps to adjust for the fact that some words appear\\nmore frequently in general.\\n\\n**198. What are Recommender Systems?**\\n\\nA subclass of information filtering systems that are meant to predict the\\npreferences or ratings that a user would give to a product. Recommender\\nsystems are widely used in movies, news, research articles, products, social tags,\\nmusic, etc.\\n\\n**199. What is the difference between Regression and classification ML**\\n**techniques.**\\n\\nBoth Regression and classification machine learning techniques come\\nunder Supervised machine learning algorithms. In Supervised machine\\nlearning algorithm, we have to train the model using labeled dataset, While\\ntraining we have to explicitly provide the correct labels and algorithm tries to\\nlearn the pattern from input to output. If our labels are discreate values then it\\nwill a classification problem, e.g A,B etc. but if our labels are continuous values\\nthen it will be a regression problem, e.g 1.23, 1.333 etc.\\n\\n**200. If you are having 4GB RAM in your machine and you want to train**\\n**your model on 10GB dataset. How would you go about this problem. Have**\\n**you ever faced this kind of problem in your machine learning/data science**\\n**experience so far ?**\\n\\nFirst of all you have to ask which ML model you want to train.\\n\\n**For Neural networks: Batch size with Numpy array will work.**\\n**Steps:**\\n1. Load the whole data in Numpy array. Numpy array has property to create\\n\\nmapping of complete dataset, it doesn’t load complete dataset in memory.\\n\\n\\n-----\\n\\n2. You can pass index to Numpy array to get required data.\\n3. Use this data to pass to Neural network.\\n4. Have small batch size.\\n**For SVM: Partial fit will work**\\n**Steps:**\\n1. Divide one big dataset in small size datasets.\\n2. Use partialfit method of SVM, it requires subset of complete dataset.\\n3. Repeat step 2 for other subsets.\\n\\n**31. What is p-value?**\\n\\nWhen you perform a hypothesis test in statistics, a p-value can help you\\ndetermine the strength of your results. p-value is a number between 0 and 1.\\nBased on the value it will denote the strength of the results. The claim which is\\non trial is called Null Hypothesis.\\n\\nLow p-value (≤ 0.05) indicates strength against the null hypothesis which means\\nwe can reject the null Hypothesis. High p-value (≥ 0.05) indicates strength for\\nthe null hypothesis which means we can accept the null Hypothesis p-value of\\n0.05 indicates the Hypothesis could go either way. To put it in another way,\\n\\nHigh P values: your data are likely with a true null. Low P values: your data are\\nunlikely with a true null.\\n\\n**32. What is ‘Naive’ in a Naive Bayes ?**\\n\\nThe Naive Bayes Algorithm is based on the Bayes Theoram. Bayes’ theoram\\ndescribes the probablitiy of an event, based on prior knowledge of conditions\\nthat might be related to the event.\\n\\n**201. What is Data Science? Also, list the differences between supervised and**\\n**unsupervised learning.**\\n\\nData Science is a blend of various tools, algorithms, and machine learning\\nprinciples with the goal to discover hidden patterns from the raw data. How is\\nthis different from what statisticians have been doing for years?\\n\\nThe answer lies in the difference between explaining and predicting.\\n\\n\\n-----\\n\\n|Supervised Learning vs Unsupervised Learning|Col2|\\n|---|---|\\n|Supervised Learning|Unsupervised Learning|\\n|1. Input data is labeled.|1. Input data is unlabeled.|\\n|2. Uses training dataset.|2. Uses the input data set.|\\n|3. Used for prediction.|3. Used for analysis.|\\n|4. Enables classification and regression.|4. Enables Classification, Density Estimation, & Dimension Reduction|\\n\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-67-0.png)\\n\\n**202. What are the important skills to have in Python with regard to data**\\n**analysis?**\\n\\nThe following are some of the important skills to possess which will come handy\\nwhen performing data analysis using Python.\\n\\nGood understanding of the built-in data types especially lists, dictionaries,\\ntuples, and sets.\\n[Mastery of N-dimensional NumPy Arrays.](https://www.edureka.co/blog/python-numpy-tutorial/)\\n[Mastery of Pandas dataframes.](https://www.edureka.co/blog/python-pandas-tutorial/)\\nAbility to perform element-wise vector and matrix operations on NumPy\\narrays.\\nKnowing that you should use the Anaconda distribution and the conda\\npackage manager.\\n\\n\\n-----\\n\\n[Familiarity with Scikit-learn. **Scikit-Learn Cheat Sheet**](https://www.edureka.co/blog/scikit-learn-machine-learning/)\\nAbility to write efficient list comprehensions instead of traditional for\\nloops.\\nAbility to write small, clean functions (important for any developer),\\npreferably pure functions that don’t alter objects.\\nKnowing how to profile the performance of a Python script and how to\\noptimize bottlenecks.\\n\\nThe following will help to tackle any problem in data analytics and machine\\nlearning.\\n\\n**203. What is Selection Bias?**\\n\\nSelection bias is a kind of error that occurs when the researcher decides who is\\ngoing to be studied. It is usually associated with research where the selection of\\nparticipants isn’t random. It is sometimes referred to as the selection effect. It is\\nthe distortion of statistical analysis, resulting from the method of collecting\\nsamples. If the selection bias is not taken into account, then some conclusions of\\nthe study may not be accurate.\\n\\nThe types of selection bias include:\\n\\n1. **Sampling bias: It is a systematic error due to a non-random sample of a**\\n\\npopulation causing some members of the population to be less likely to be\\nincluded than others resulting in a biased sample.\\n\\n2. **Time interval: A trial may be terminated early at an extreme value (often**\\n\\nfor ethical reasons), but the extreme value is likely to be reached by the\\nvariable with the largest variance, even if all variables have a similar\\nmean.\\n\\n3. **Data: When specific subsets of data are chosen to support a conclusion or**\\n\\nrejection of bad data on arbitrary grounds, instead of according to\\npreviously stated or generally agreed criteria.\\n\\n4. **Attrition: Attrition bias is a kind of selection bias caused by attrition (loss**\\n\\nof participants) discounting trial subjects/tests that did not run to\\ncompletion.\\n\\n**Data Scientist Masters**\\n\\n**204. What is the difference between “long” and “wide” format data?**\\n\\nIn the **wide format, a subject’s repeated responses will be in a single row, and**\\n\\n\\n-----\\n\\neach response is in a separate column. In the long format, each row is a one-time\\npoint per subject. You can recognize data in wide format by the fact that columns\\ngenerally represent groups.\\n\\n**205. What do you understand by the term Normal Distribution?**\\n\\nData is usually distributed in different ways with a bias to the left or to the right\\nor it can all be jumbled up.\\nHowever, there are chances that data is distributed around a central value\\nwithout any bias to the left or right and reaches normal distribution in the form\\nof a bell-shaped curve.\\n\\nThe random variables are distributed in the form of a symmetrical bell-shaped\\ncurve.\\n\\nProperties of Nornal Distribution:\\n\\n1. Unimodal -one mode\\n2. Symmetrical -left and right halves are mirror images\\n3. Bell-shaped -maximum height (mode) at the mean\\n4. Mean, Mode, and Median are all located in the center\\n5. Asymptotic\\n\\n**206. What is the goal of A/B Testing?**\\nIt is a statistical hypothesis testing for a randomized experiment with two\\nvariables A and B.\\n\\nThe goal of A/B Testing is to identify any changes to the web page to maximize\\nor increase the outcome of an interest. A/B testing is a fantastic method for\\nfiguring out the best online promotional and marketing strategies for your\\nbusiness. It can be used to test everything from website copy to sales emails to\\nsearch ads\\n\\nAn example of this could be identifying the click-through rate for a banner ad.\\n\\n**207. What do you understand by statistical power of sensitivity and how do**\\n**you calculate it?**\\n\\nSensitivity is commonly used to validate the accuracy of a classifier (Logistic,\\nSVM, Random Forest etc.).\\n\\n\\n-----\\n\\nSensitivity is nothing but “Predicted True events/ Total events”. True events here\\nare the events which were true and model also predicted them as true.\\nCalculation of seasonality is pretty straightforward.\\n\\n**Seasonality = ( True Positives ) / ( Positives in Actual Dependent Variable )**\\n\\n*where true positives are positive events which are correctly classified as\\npositives.\\n\\n**208. What are the differences between overfitting and underfitting?**\\n\\nIn statistics and machine learning, one of the most common tasks is to fit\\na model to a set of training data, so as to be able to make reliable predictions on\\ngeneral untrained data.\\nIn overfitting, a statistical model describes random error or noise instead of the\\nunderlying relationship. Overfitting occurs when a model is excessively\\ncomplex, such as having too many parameters relative to the number of\\nobservations. A model that has been overfit has poor predictive performance, as\\nit overreacts to minor fluctuations in the training data.\\n\\n**209. Python or R – Which one would you prefer for text analytics?**\\n\\nWe will prefer Python because of the following reasons:\\n\\nPython would be the best option because it has Pandas library that\\nprovides easy to use data structures and high-performance data analysis\\ntools.\\nR is more suitable for machine learning than just text analysis.\\nPython performs faster for all types of text analytics.\\n\\n**[Python vs R](https://www.edureka.co/blog/r-vs-python/)**\\n\\n**210. How does data cleaning plays a vital role in analysis?**\\n\\nData cleaning can help in analysis because:\\n\\nCleaning data from multiple sources helps to transform it into a format\\nthat data analysts or data scientists can work with.\\nData Cleaning helps to increase the accuracy of the model in machine\\nlearning.\\nIt is a cumbersome process because as the number of data sources\\nincreases, the time taken to clean the data increases exponentially due to\\nthe number of sources and the volume of data generated by these sources.\\n\\n\\n-----\\n\\nIt might take up to 80% of the time for just cleaning data making it a\\ncritical part of analysis task.\\n\\n**211. Differentiate between univariate, bivariate and multivariate analysis.**\\n\\n**Univariate analyses are descriptive statistical analysis techniques which can be**\\ndifferentiated based on the number of variables involved at a given point of time.\\nFor example, the pie charts of sales based on territory involve only one variable\\nand can the analysis can be referred to as univariate analysis.\\n\\nThe **bivariate analysis attempts to understand the difference between two**\\nvariables at a time as in a scatterplot. For example, analyzing the volume of sale\\nand spending can be considered as an example of bivariate analysis.\\n\\n**Multivariate analysis deals with the study of more than two variables to**\\nunderstand the effect of variables on the responses.\\n\\n**212. What is Cluster Sampling?**\\n\\nCluster sampling is a technique used when it becomes difficult to study the target\\npopulation spread across a wide area and simple random sampling cannot be\\napplied. Cluster Sample is a probability sample where each sampling unit is a\\ncollection or cluster of elements.\\n\\nFor eg., A researcher wants to survey the academic performance of high school\\nstudents in Japan. He can divide the entire population of Japan into different\\nclusters (cities). Then the researcher selects a number of clusters depending on\\nhis research through simple or systematic random sampling.\\n\\nLet’s continue our Data Science Interview Questions blog with some more\\nstatistics questions.\\n\\n**213. What is Systematic Sampling?**\\n\\nSystematic sampling is a statistical technique where elements are selected from\\nan ordered sampling frame. In systematic sampling, the list is progressed in a\\ncircular manner so once you reach the end of the list, it is progressed from the\\ntop again. The best example of systematic sampling is equal probability method.\\n\\n**214. What are Eigenvectors and Eigenvalues?**\\n\\nEigenvectors are used for understanding linear transformations. In data analysis,\\nwe usually calculate the eigenvectors for a correlation or covariance\\nmatrix. Eigenvectors are the directions along which a particular linear\\ntransformation acts by flipping, compressing or stretching.\\n\\n\\n-----\\n\\n**215. Can you cite some examples where a false positive is important than a**\\n**false negative?**\\nLet us first understand what false positives and false negatives are.\\n\\n**False Positives are the cases where you wrongly classified a non-event as**\\nan event a.k.a Type I error.\\n**False Negatives are the cases where you wrongly classify events as non-**\\nevents, a.k.a Type II error.\\n\\n**Example 1: In the medical field, assume you have to give chemotherapy to**\\npatients. Assume a patient comes to that hospital and he is tested positive for\\ncancer, based on the lab prediction but he actually doesn’t have cancer. This is a\\ncase of false positive. Here it is of utmost danger to start chemotherapy on this\\npatient when he actually does not have cancer. In the absence of cancerous cell,\\nchemotherapy will do certain damage to his normal healthy cells and might lead\\nto severe diseases, even cancer.\\n\\n**Example 2: Let’s say an e-commerce company decided to give $1000 Gift**\\nvoucher to the customers whom they assume to purchase at least $10,000 worth\\nof items. They send free voucher mail directly to 100 customers without any\\nminimum purchase condition because they assume to make at least 20% profit\\non sold items above $10,000. Now the issue is if we send the $1000 gift\\nvouchers to customers who have not actually purchased anything but are marked\\nas having made $10,000 worth of purchase.\\n\\n**216. Can you cite some examples where a false negative important than a**\\n**false positive?**\\n\\n**Example 1: Assume there is an airport ‘A’ which has received high-security**\\nthreats and based on certain characteristics they identify whether a particular\\npassenger can be a threat or not. Due to a shortage of staff, they decide to scan\\npassengers being predicted as risk positives by their predictive model. What will\\nhappen if a true threat customer is being flagged as non-threat by airport model?\\n\\n**Example 2: What if Jury or judge decides to make a criminal go free?**\\n\\n**Example 3: What if you rejected to marry a very good person based on your**\\npredictive model and you happen to meet him/her after a few years and realize\\nthat you had a false negative?\\n\\n**217. Can you cite some examples where both false positive and false**\\n**negatives are equally important?**\\n\\nIn the Banking industry giving loans is the primary source of making money but\\n\\n\\n-----\\n\\nat the same time if your repayment rate is not good you will not make any profit,\\nrather you will risk huge losses.\\nBanks don’t want to lose good customers and at the same point in time, they\\ndon’t want to acquire bad customers. In this scenario, both the false positives and\\nfalse negatives become very important to measure.\\n\\n**218. Can you explain the difference between a Validation Set and a Test Set?**\\n\\nA Validation **set can be considered as a part of the training set as it is used for**\\nparameter selection and to avoid overfitting of the model being built.\\n\\nOn the other hand, a Test Set is used for testing or evaluating the performance of\\na trained machine learning model.\\n\\nIn simple terms, the differences can be summarized as; training set is to fit the\\nparameters i.e. weights and test set is to assess the performance of the model i.e.\\nevaluating the predictive power and generalization.\\n\\n**219. Explain cross-validation.**\\n**Cross-validation is a model validation technique for evaluating how the**\\noutcomes of statistical analysis will **generalize to an** **Independent**\\n**dataset. Mainly used in backgrounds where the objective is forecast and one**\\nwants to estimate how accurately a model will accomplish in practice.\\nThe goal of cross-validation is to term a data set to test the model in the training\\nphase (i.e. validation data set) in order to limit problems like overfitting and get\\nan insight on how the model will generalize to an independent data set.\\n\\n**220. What is Machine Learning?**\\n\\nMachine Learning explores the study and construction of algorithms that can\\nlearn from and make predictions on data. Closely related to computational\\nstatistics. Used to devise complex models and algorithms that lend themselves to\\na prediction which in commercial use is known as predictive analytics.\\n\\n\\n-----\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-74-0.png)\\n\\n**Figure: Applications of Machine Learning**\\n\\n**221. What is the Supervised Learning?**\\n\\nSupervised learning is the machine learning task of inferring a function from\\nlabeled training data. The training data consist of a set of training examples.\\n\\nAlgorithms: Support Vector Machines, Regression, Naive Bayes, Decision\\nTrees, K-nearest Neighbor Algorithm and Neural Networks\\n\\nE.g. If you built a fruit classifier, the labels will be “this is an orange, this is an\\napple and this is a banana”, based on showing the classifier examples of apples,\\noranges and bananas.\\n\\n**222. What is Unsupervised learning?**\\n\\nUnsupervised learning is a type of machine learning algorithm used to draw\\ninferences from datasets consisting of input data without labeled responses.\\nAlgorithms: Clustering, Anomaly Detection, Neural Networks and Latent\\nVariable Models\\n\\nE.g. In the same example, a fruit clustering will categorize as “fruits with soft\\nskin and lots of dimples”, “fruits with shiny hard skin” and “elongated yellow\\nfruits”.\\n\\n**223. What are the various classification algorithms?**\\n\\nThe below diagram lists the most important classification algorithms.\\n\\n**224. What is logistic regression? State an example when you have used**\\n**logistic regression recently.**\\n\\nLogistic Regression often referred as logit model is a technique to predict the\\nbinary outcome from a linear combination of predictor variables.\\nFor example, if you want to predict whether a particular political leader will win\\n\\n\\n-----\\n\\nthe election or not. In this case, the outcome of prediction is binary i.e. 0 or 1\\n(Win/Lose). The predictor variables here would be the amount of money spent\\nfor election campaigning of a particular candidate, the amount of time spent in\\ncampaigning, etc.\\n\\n**225. What are Recommender Systems?**\\n\\nRecommender Systems are a subclass of information filtering systems that are\\nmeant to predict the preferences or ratings that a user would give to a\\nproduct. Recommender systems are widely used in movies, news, research\\narticles, products, social tags, music, etc.\\nExamples include movie recommenders in IMDB, Netflix & BookMyShow,\\nproduct recommenders in e-commerce sites like Amazon, eBay & Flipkart,\\nYouTube video recommendations and game recommendations in Xbox.\\n\\n**226. What is Linear Regression?**\\n\\n**[Linear regression is a statistical technique where the score of a variable Y is](https://www.edureka.co/blog/linear-regression-in-python/)**\\npredicted from the score of a second variable X. X is referred to as the predictor\\nvariable and Y as the criterion variable.\\n\\n**227. What is Collaborative filtering?**\\nThe process of filtering used by most of the recommender systems to find\\npatterns or information by collaborating viewpoints, various data sources and\\nmultiple agents.\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-75-0.png)\\n\\nAn example of collaborative filtering can be to predict the rating of a particular\\nuser based on his/her ratings for other movies and others’ ratings for all movies.\\nThis concept is widely used in recommending movies in IMDB, Netflix &\\nBookMyShow, product recommenders in e-commerce sites like Amazon, eBay\\n& Flipkart, YouTube video recommendations and game recommendations in\\nXbox.\\n\\n**228. How can outlier values be treated?**\\n\\nOutlier values can be identified by using univariate or any other graphical\\nanalysis method. If the number of outlier values is few then they can be assessed\\n\\n\\n-----\\n\\nindividually but for a large number of outliers, the values can be substituted with\\neither the 99th or the 1st percentile values.\\nAll extreme values are not outlier values. The most common ways to treat outlier\\nvalues\\n\\n1. To change the value and bring in within a range.\\n2. To just remove the value.\\n\\n**229. What are the various steps involved in an analytics project?**\\n\\nThe following are the various steps involved in an analytics project:\\n\\n1. Understand the Business problem\\n2. Explore the data and become familiar with it.\\n3. Prepare the data for modeling by detecting outliers, treating missing\\n\\nvalues, transforming variables, etc.\\n\\n4. After data preparation, start running the model, analyze the result and\\n\\ntweak the approach. This is an iterative step until the best possible\\noutcome is achieved.\\n\\n5. Validate the model using a new data set.\\n6. Start implementing the model and track the result to analyze the\\n\\nperformance of the model over the period of time.\\n\\n**230. During analysis, how do you treat missing values?**\\n\\nThe extent of the missing values is identified after identifying the variables with\\nmissing values. If any patterns are identified the analyst has to concentrate on\\nthem as it could lead to interesting and meaningful business insights.\\n\\nIf there are no patterns identified, then the missing values can be substituted with\\nmean or median values (imputation) or they can simply be ignored. Assigning a\\ndefault value which can be mean, minimum or maximum value. Getting into the\\ndata is important.\\n\\nIf it is a categorical variable, the default value is assigned. The missing value is\\nassigned a default value. If you have a distribution of data coming, for normal\\ndistribution give the mean value.\\n\\nIf 80% of the values for a variable are missing then you can answer that you\\nwould be dropping the variable instead of treating the missing values.\\n\\n**231. How will you define the number of clusters in a clustering algorithm?**\\n\\n\\n-----\\n\\nThough the Clustering Algorithm is not specified, this question is mostly in\\nreference to K-Means clustering where “K” defines the number of clusters. The\\nobjective of clustering is to group similar entities in a way that the entities within\\na group are similar to each other but the groups are different from each other.\\n\\nFor example, the following image shows three different groups.\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-77-0.png)\\n\\nWithin Sum of squares is generally used to explain the homogeneity within a\\ncluster. If you plot WSS for a range of number of clusters, you will get the plot\\nshown below.\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-77-1.png)\\n\\nThe Graph is generally known as Elbow Curve.\\nRed circled point in above graph i.e. Number of Cluster =6 is the point\\nafter which you don’t see any decrement in WSS.\\nThis point is known as the bending point and taken as K in K – Means.\\n\\nThis is the widely used approach but few data scientists also use Hierarchical\\nclustering first to create dendrograms and identify the distinct groups from there.\\n\\nNow that we have seen the Machine Learning Questions, Let’s continue our\\nData Science Interview Questions blog with some Probability questions.\\n\\n**232. In any 15-minute interval, there is a 20% probability that you will see**\\n**at least one shooting star. What is the probability that you see at least one**\\n**shooting star in the period of an hour?**\\n\\nProbability of not seeing any shooting star in 15 minutes is\\n\\n=  1 – P( Seeing one shooting star )\\n=  1 – 0.2     =  0.8\\n\\n\\n-----\\n\\nProbability of not seeing any shooting star in the period of one hour\\n=  (0.8) ^ 4    =  0.4096\\nProbability of seeing at least one shooting star in the one hour\\n\\n=  1 – P( Not seeing any star )\\n=  1 – 0.4096   =  0.5904\\n\\n**233. How can you generate a random number between 1 – 7 with only a die?**\\n\\nAny die has six sides from 1-6. There is no way to get seven equal\\noutcomes from a single rolling of a die. If we roll the die twice and\\nconsider the event of two rolls, we now have 36 different outcomes.\\nTo get our 7 equal outcomes we have to reduce this 36 to a number\\ndivisible by 7. We can thus consider only 35 outcomes and exclude the\\nother one.\\nA simple scenario can be to exclude the combination (6,6), i.e., to roll the\\ndie again if 6 appears twice.\\nAll the remaining combinations from (1,1) till (6,5) can be divided into 7\\nparts of 5 each. This way all the seven sets of outcomes are equally likely.\\n\\n**234. A certain couple tells you that they have two children, at least one of**\\n**which is a girl. What is the probability that they have two girls?**\\n\\nIn the case of two children, there are 4 equally likely possibilities\\n\\n**BB, BG, GB and GG;**\\n\\nwhere B = Boy and G = Girl and the first letter denotes the first child.\\n\\nFrom the question, we can exclude the first case of BB. Thus from the remaining\\n3 possibilities of BG, GB &BB, we have to find the probability of the case with\\ntwo girls.\\n\\nThus, P(Having two girls given one girl)  =  1 / 3\\n\\n**235. A jar has 1000 coins, of which 999 are fair and 1 is double headed. Pick**\\n**a coin at random, and toss it 10 times. Given that you see 10 heads, what is**\\n**the probability that the next toss of that coin is also a head?**\\nThere are two ways of choosing the coin. One is to pick a fair coin and the other\\nis to pick the one with two heads.\\n\\nProbability of selecting fair coin = 999/1000 = 0.999\\nProbability of selecting unfair coin = 1/1000 = 0.001\\n\\nSelecting 10 heads in a row = Selecting fair coin * Getting 10 heads + Selecting\\n\\n\\n-----\\n\\nan unfair coin\\n\\nP (A) = 0.999 * (1/2)^5 = 0.999 * (1/1024) = 0.000976\\nP (B) = 0.001 * 1 = 0.001\\nP( A / A + B ) = 0.000976 / (0.000976 + 0.001) = 0.4939\\nP( B / A + B ) = 0.001 / 0.001976 = 0.5061\\n\\nProbability of selecting another head = P(A/A+B) * 0.5 + P(B/A+B) * 1 =\\n0.4939 * 0.5 + 0.5061 = 0.7531\\n\\n**236. What do you mean by Deep Learning and Why has it become popular**\\n**now?**\\n\\nDeep Learning is nothing but a paradigm of machine learning which has shown\\nincredible promise in recent years. This is because of the fact that Deep Learning\\nshows a great analogy with the functioning of the human brain.\\n\\nNow although Deep Learning has been around for many years, the major\\nbreakthroughs from these techniques came just in recent years. This is because\\nof two main reasons:\\n\\nThe increase in the amount of data generated through various sources\\nThe growth in hardware resources required to run these models\\n\\nGPUs are multiple times faster and they help us build bigger and deeper deep\\nlearning models in comparatively less time than we required previously\\n\\n**237. What are Artificial Neural Networks?**\\nArtificial Neural networks are a specific set of algorithms that have\\nrevolutionized machine learning. They are inspired by biological neural\\nnetworks. **[Neural Networks can adapt to changing input so the network](https://www.edureka.co/blog/neural-network-tutorial/)**\\ngenerates the best possible result without needing to redesign the output criteria.\\n\\n**238. Describe the structure of Artificial Neural Networks?**\\n\\nArtificial Neural Networks works on the same principle as a biological Neural\\nNetwork. It consists of inputs which get processed with weighted sums and Bias,\\nwith the help of Activation Functions.\\n\\n\\n-----\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-80-0.png)\\n\\n**239. Explain Gradient Descent.**\\n\\nTo Understand Gradient Descent, Let’s understand what is a Gradient first.\\n\\nA gradient measures how much the output of a function changes if you change\\nthe inputs a little bit. It simply measures the change in all weights with regard to\\nthe change in error. You can also think of a gradient as the slope of a function.\\n\\n**Gradient Descent can be thought of climbing down to the bottom of a valley,**\\ninstead of climbing up a hill. This is because it is a minimization algorithm that\\nminimizes a given function (Activation Function).\\n\\n**240. What is Back Propagation and Explain it’s Working.**\\n\\n**[Backpropagation is a training algorithm used for multilayer neural network. In](https://www.edureka.co/blog/backpropagation/)**\\nthis method, we move the error from an end of the network to all weights inside\\nthe network and thus allowing efficient computation of the gradient.\\n\\nIt has the following steps:\\n\\nForward Propagation of Training Data\\nDerivatives are computed using output and target\\nBack Propagate for computing derivative of error wrt output activation\\nUsing previously calculated derivatives for output\\nUpdate the Weights\\n\\n**241. What are the variants of Back Propagation?**\\n\\n\\n-----\\n\\n**Stochastic Gradient Descent: We use only single training example for**\\ncalculation of gradient and update parameters.\\n**Batch Gradient Descent: We calculate the gradient for the whole dataset**\\nand perform the update at each iteration.\\n**Mini-batch Gradient Descent: It’s one of the most popular optimization**\\nalgorithms. It’s a variant of Stochastic Gradient Descent and here instead\\nof single training example, mini-batch of samples is used.\\n\\n**242. What are the different Deep Learning Frameworks?**\\n\\nPytorch\\nTensorFlow\\nMicrosoft Cognitive Toolkit\\nKeras\\nCaffe\\nChainer\\n\\n**243. What is the role of Activation Function?**\\n\\nThe Activation function is used to introduce non-linearity into the neural\\nnetwork helping it to learn more complex function. Without which the neural\\nnetwork would be only able to learn linear function which is a linear\\ncombination of its input data. An activation function is a function in an artificial\\nneuron that delivers an output based on inputs\\n\\n**244. What is an Auto-Encoder?**\\n**Autoencoders are simple learning networks that aim to transform inputs into**\\noutputs with the minimum possible error. This means that we want the output to\\nbe as close to input as possible. We add a couple of layers between the input and\\nthe output, and the sizes of these layers are smaller than the input layer. The\\nautoencoder receives unlabeled input which is then encoded to reconstruct the\\ninput.\\n\\n**245. What is a Boltzmann Machine?**\\n\\nBoltzmann machines have a simple learning algorithm that allows them to\\ndiscover interesting features that represent complex regularities in the training\\ndata. The Boltzmann machine is basically used to optimize the weights and the\\nquantity for the given problem. The learning algorithm is very slow in networks\\nwith many layers of feature detectors. “Restricted Boltzmann Machines”\\nalgorithm has a single layer of feature detectors which makes it faster than the\\n\\n\\n-----\\n\\nrest.\\n\\n**251) Which of these measures are used to analyze the central tendency of**\\n**data?**\\n\\nA) Mean and Normal Distribution\\n\\nB) Mean, Median and Mode\\n\\nC) Mode, Alpha & Range\\n\\nD) Standard Deviation, Range and Mean\\n\\nE) Median, Range and Normal Distribution\\n\\n**Solution: (B)**\\n\\nThe mean, median, mode are the three statistical measures which help us to\\nanalyze the central tendency of data. We use these measures to find the central\\nvalue of the data to summarize the entire data set.\\n\\n**252) Five numbers are given: (5, 10, 15, 5, 15). Now, what would be the sum**\\n**of deviations of individual data points from their mean?**\\n\\nA) 10\\n\\nB)25\\n\\nC) 50\\n\\nD) 0\\n\\nE) None of the above\\n\\n**Solution: (D)**\\n\\nThe sum of deviations of the individual will always be 0.\\n\\n**253) A test is administered annually. The test has a mean score of 150 and a**\\n**standard deviation of 20. If Ravi’s z-score is 1.50, what was his score on the**\\n\\n\\n-----\\n\\n**test?**\\n\\nA) 180\\nB) 130\\nC) 30\\nD) 150\\nE) None of the above\\n\\n**Solution: (A)**\\n\\nX= μ+Zσ where μ is the mean, σ is the standard deviation and X is the score\\nwe’re calculating. Therefore X = 150+20*1.5 = 180\\n\\n**254) Which of the following measures of central tendency will always**\\n**change if a single value in the data changes?**\\n\\nA) Mean\\n\\nB) Median\\n\\nC) Mode\\n\\nD) All of these\\n\\n**Solution: (A)**\\n\\nThe mean of the dataset would always change if we change any value of the data\\nset. Since we are summing up all the values together to get it, every value of the\\ndata set contributes to its value. Median and mode may or may not change with\\naltering a single value in the dataset.\\n\\n**255) Below, we have represented six data points on a scale where vertical**\\n**lines on scale represent unit.**\\n\\n\\n-----\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-84-0.png)\\n\\n**Which of the following line represents the mean of the given data points,**\\n**where the scale is divided into same units?**\\n\\nA) A\\nB) B\\nC) C\\nD) D\\n\\n**Solution: (C)**\\n\\nIt’s a little tricky to visualize this one by just looking at the data points. We can\\nsimply substitute values to understand the mean. Let A be 1, B be 2, C be 3 and\\nso on. The data values as shown will become {1,1,1,4,5,6} which will have\\nmean to be 18/6 = 3 i.e. C.\\n\\n**256) If a positively skewed distribution has a median of 50, which of the**\\n**following statement is true?**\\n\\nA) Mean is greater than 50\\nB) Mean is less than 50\\nC) Mode is less than 50\\nD) Mode is greater than 50\\nE) Both A and C\\nF) Both B and D\\n\\n**Solution: (E)**\\n\\nBelow are the distributions for Negatively, Positively and no skewed curves.\\n\\n\\n-----\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-85-0.png)\\n\\nAs we can see for a positively skewed curve, Mode<Median<Mean. So if\\nmedian is 50, mean would be more than 50 and mode will be less than 50.\\n\\n**257) Which of the following is a possible value for the median of the below**\\n**distribution?**\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-85-1.png)\\n\\nA) 32\\nB) 26\\nC) 17\\nD) 40\\n\\n**Solution: (B)**\\n\\nTo answer this one we need to go to the basic definition of a median. Median is\\nthe value which has roughly half the values before it and half the values after.\\n\\n\\n-----\\n\\nThe number of values less than 25 are (36+54+69 = 159) and the number of\\nvalues greater than 30 are (55+43+25+22+17= 162). So the median should lie\\nsomewhere between 25 and 30. Hence 26 is a possible value of the median.\\n\\n**258) Which of the following statements are true about Bessels Correction**\\n**while calculating a sample standard deviation?**\\n\\n1. **Bessels correction is always done when we perform any operation on**\\n\\n**a sample data.**\\n\\n2. **Bessels correction is used when we are trying to estimate population**\\n\\n**standard deviation from the sample.**\\n\\n3. **Bessels corrected standard deviation is less biased.**\\n\\nA) Only 2\\n\\nB) Only 3\\n\\nC) Both 2 and 3\\n\\nD) Both 1 and 3\\n\\n**Solution: (C)**\\n\\nContrary to the popular belief Bessel’s correction should not be always done. It’s\\nbasically done when we’re trying to estimate the population standard deviation\\nusing the sample standard deviation. The bias is definitely reduced as the\\nstandard deviation will now(after correction) be depicting the dispersion of the\\npopulation more than that of the sample.\\n\\n**259) If the variance of a dataset is correctly computed with the formula**\\n**using (n – 1) in the denominator, which of the following option is true?**\\n\\nA) Dataset is a sample\\nB) Dataset is a population\\nC) Dataset could be either a sample or a population\\nD) Dataset is from a census\\nE) None of the above\\n\\n**Solution: (A)**\\n\\n\\n-----\\n\\nIf the variance has n-1 in the formula, it means that the set is a sample. We try to\\nestimate the population variance by dividing the sum of squared difference with\\nthe mean with n-1.\\n\\nWhen we have the actual population data we can directly divide the sum of\\nsquared differences with n instead of n-1.\\n\\n**260) [True or False] Standard deviation can be negative.**\\n\\nA) TRUE\\n\\nB) FALSE\\n\\n**Solution: (B)**\\n\\nBelow is the formula for standard deviation\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-87-0.png)\\n\\nSince the differences are squared, added and then rooted, negative standard\\ndeviations are not possible.\\n\\n**261) Standard deviation is robust to outliers?**\\n\\nA) True\\n\\nB) False\\n\\n**Solution: (B)**\\n\\nIf you look at the formula for standard deviation above, a very high or a very\\nlow value would increase standard deviation as it would be very different from\\nthe mean. Hence outliers will effect standard deviation.\\n\\n**262) For the below normal distribution, which of the following option holds**\\n**true ?**\\n\\n**σ1, σ2 and σ3 represent the standard deviations for curves 1, 2 and 3**\\n\\n\\n-----\\n\\n**respectively.**\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-88-0.png)\\n\\nA) σ1> σ2> σ3\\n\\nB) σ1< σ2< σ3\\n\\nC) σ1= σ2= σ3\\n\\nD) None\\n\\n**Solution: (B)**\\n\\nFrom the definition of normal distribution, we know that the area under the\\ncurve is 1 for all the 3 shapes. The curve 3 is more spread and hence more\\ndispersed (most of values being within 40-160). Therefore it will have the\\nhighest standard deviation. Similarly, Curve 1 has a very low range and all the\\nvalues are in a small range of 80-120. Hence, curve 1 has the least standard\\ndeviation.\\n\\n**263) What would be the critical values of Z for 98% confidence interval for**\\n**a two-tailed test ?**\\n\\nA) +/- 2.33\\nB) +/- 1.96\\nC) +/- 1.64\\nD) +/- 2.55\\n\\n**Solution: (A)**\\n\\nWe need to look at the z table for answering this. For a 2 tailed test, and a 98%\\n\\n\\n-----\\n\\nconfidence interval, we should check the area before the z value as 0.99 since\\n1% will be on the left side of the mean and 1% on the right side. Hence we\\nshould check for the z value for area>0.99. The value will be +/- 2.33\\n\\n**264) [True or False] The standard normal curve is symmetric about 0 and**\\n**the total area under it is 1.**\\n\\nA)TRUE\\n\\nB) FALSE\\n\\n**Solution: (A)**\\n\\nBy the definition of the normal curve, the area under it is 1 and is symmetric\\nabout zero. The mean, median and mode are all equal and 0. The area to the left\\nof mean is equal to the area on the right of mean. Hence it is symmetric.\\n\\n**Studies show that listening to music while studying can improve your**\\n**memory. To demonstrate this, a researcher obtains a sample of 36 college**\\n**students and gives them a standard memory test while they listen to some**\\n**background music. Under normal circumstances (without music), the mean**\\n**score obtained was 25 and standard deviation is 6. The mean score for the**\\n**sample after the experiment (i.e With music) is 28.**\\n\\n**265) What is the null hypothesis in this case?**\\n\\nA) Listening to music while studying will not impact memory.\\nB) Listening to music while studying may worsen memory.\\nC) Listening to music while studying may improve memory.\\nD) Listening to music while studying will not improve memory but can make it\\nworse.\\n\\n**Solution: (D)**\\n\\nThe null hypothesis is generally assumed statement, that there is no relationship\\nin the measured phenomena. Here the null hypothesis would be that there is no\\nrelationship between listening to music and improvement in memory.\\n\\n\\n-----\\n\\n**266) What would be the Type I error?**\\n\\nA) Concluding that listening to music while studying improves memory, and it’s\\nright.\\nB) Concluding that listening to music while studying improves memory when it\\nactually doesn’t.\\nC) Concluding that listening to music while studying does not improve memory\\nbut it does.\\n\\n**Solution: (B)**\\n\\nType 1 error means that we reject the null hypothesis when its actually true. Here\\nthe null hypothesis is that music does not improve memory. Type 1 error would\\nbe that we reject it and say that music does improve memory when it actually\\ndoesn’t.\\n\\n**267) After performing the Z-test, what can we conclude ____ ?**\\n\\nA) Listening to music does not improve memory.\\n\\nB)Listening to music significantly improves memory at p\\n\\nC) The information is insufficient for any conclusion.\\n\\nD) None of the above\\n\\n**Solution: (B)**\\n\\nLet’s perform the Z test on the given case. We know that the null hypothesis is\\nthat listening to music does not improve memory.\\n\\nAlternate hypothesis is that listening to music does improve memory.\\n\\nIn this case the standard error i.e.\\n\\nThe Z score for a sample mean of 28 from this population is\\n\\nZ critical value for α = 0.05 (one tailed) would be 1.65 as seen from the z table.\\n\\n\\n-----\\n\\nTherefore since the Z value observed is greater than the Z critical value, we can\\nreject the null hypothesis and say that listening to music does improve the\\nmemory with 95% confidence.\\n\\n**268) A researcher concludes from his analysis that a placebo cures AIDS.**\\n**What type of error is he making?**\\n\\nA) Type 1 error\\n\\nB) Type 2 error\\n\\nC) None of these. The researcher is not making an error.\\n\\nD) Cannot be determined\\n\\n**Solution: (D)**\\n\\nBy definition, type 1 error is rejecting the null hypothesis when its actually true\\nand type 2 error is accepting the null hypothesis when its actually false. In this\\ncase to define the error, we need to first define the null and alternate hypothesis.\\n\\n**269) What happens to the confidence interval when we introduce some**\\n**outliers to the data?**\\n\\nA) Confidence interval is robust to outliers\\n\\nB) Confidence interval will increase with the introduction of outliers.\\n\\nC) Confidence interval will decrease with the introduction of outliers.\\n\\nD) We cannot determine the confidence interval in this case.\\n\\n**Solution: (B)**\\n\\nWe know that confidence interval depends on the standard deviation of the data.\\nIf we introduce outliers into the data, the standard deviation increases, and hence\\nthe confidence interval also increases.\\n\\n**A medical doctor wants to reduce blood sugar level of all his patients by**\\n\\n\\n-----\\n\\n**altering their diet. He finds that the mean sugar level of all patients is 180**\\n**with a standard deviation of 18. Nine of his patients start dieting and the**\\n**mean of the sample is observed to 175. Now, he is considering to recommend**\\n**all his patients to go on a diet.**\\n\\n**Note: He calculates 99% confidence interval.**\\n\\n**270) What is the standard error of the mean?**\\n\\nA) 9\\nB) 6\\nC) 7.5\\nD) 18\\n\\n**Solution: (B)**\\n\\nThe standard error of the mean is the standard deviation by the square root of the\\nnumber of values. i.e.\\n\\nStandard error = = 6\\n\\n**271) What is the probability of getting a mean of 175 or less after all the**\\n**patients start dieting?**\\n\\nA) 20%\\nB) 25%\\nC) 15%\\nD) 12%\\n\\n**Solution: (A)**\\n\\nThis actually wants us to calculate the probability of population mean being 175\\nafter the intervention. We can calculate the Z value for the given mean.\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-92-0.png)\\n\\nIf we look at the z table, the corresponding value for z = -0.833 ~ 0.2033.\\n\\n\\n-----\\n\\nTherefore there is around 20% probability that if everyone starts dieting, the\\npopulation mean would be 175.\\n\\n**272) Which of the following statement is correct?**\\n\\nA) The doctor has a valid evidence that dieting reduces blood sugar level.\\n\\nB) The doctor does not have enough evidence that dieting reduces blood sugar\\nlevel.\\n\\nC) If the doctor makes all future patients diet in a similar way, the mean blood\\npressure will fall below 160.\\n\\n**Solution: (B)**\\n\\nWe need to check if we have sufficient evidence to reject the null. The null\\nhypothesis is that dieting has no effect on blood sugar. This is a two tailed test.\\nThe z critical value for a 2 tailed test would be ±2.58.\\n\\nThe z value as we have calculated is -0.833.\\n\\nSince Z value < Z critical value, we do not have enough evidence that dieting\\nreduces blood sugar.\\n\\n**A researcher is trying to examine the effects of two different teaching**\\n**methods. He divides 20 students into two groups of 10 each. For group 1, the**\\n**teaching method is using fun examples. Where as for group 2 the teaching**\\n**method is using software to help students learn. After a 20 minutes lecture**\\n**of both groups, a test is conducted for all the students.**\\n\\n**We want to calculate if there is a significant difference in the scores of both**\\n**the groups.**\\n\\n**It is given that:**\\n\\nAlpha=0.05, two tailed.\\nMean test score for group 1 = 10\\nMean test score for group 2 = 7\\nStandard error = 0.94\\n\\n**273) What is the value of t-statistic?**\\n\\n\\n-----\\n\\nA) 3.191\\nB) 3.395\\nC) Cannot be determined.\\nD) None of the above\\n\\n**Solution: (A)**\\n\\nThe t statistic of the given group is nothing but the difference between the group\\nmeans by the standard error.\\n\\n=(10-7)/0.94 = 3.191\\n\\n**274) Is there a significant difference in the scores of the two groups?**\\n\\nA) Yes\\nB) No\\n\\n**Solution: (A)**\\n\\nThe null hypothesis in this case would be that there is no difference between the\\ngroups, while the alternate hypothesis would be that the groups are significantly\\ndifferent.\\n\\nThe t critical value for a 2 tailed test at α = 0.05 is ±2.101. The t statistic\\nobtained is 3.191. Since the t statistic is more than the critical value of t, we can\\nreject the null hypothesis and say that the two groups are significantly different\\nwith 95% confidence.\\n\\n**275) What percentage of variability in scores is explained by the method of**\\n**teaching?**\\n\\nA) 36.13\\nB) 45.21\\nC) 40.33\\nD) 32.97\\n\\n**Solution: (A)**\\n\\nThe % variability in scores is given by the R[2] value. The formula for R[2] given\\nby\\n\\n\\n-----\\n\\nR[2 ]=\\n\\nThe degrees of freedom in this case would be 10+10 -2 since there are two\\ngroups with size 10 each. The degree of freedom is 18.\\n\\n**276) [True or False] F statistic cannot be negative.**\\n\\nA) TRUE\\n\\nB) FALSE\\n\\n**Solution: (A)**\\n\\nF statistic is the value we receive when we run an ANOVA test on different\\ngroups to understand the differences between them. The F statistic is given by\\nthe ratio of between group variability to within group variability\\n\\nBelow is the formula for f Statistic.\\n\\nSince both the numerator and denominator possess square terms, F statistic\\ncannot be negative.\\n\\n**277) Which of the graph below has very strong positive correlation?**\\n\\n\\n-----\\n\\nA)\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-96-2.png)\\n\\nB)\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-96-0.png)\\n\\nC)\\n\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-96-1.png)\\n\\n-----\\n\\nD)\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-97-1.png)\\n\\n**Solution: (B)**\\n\\nA strong positive correlation would occur when the following condition is met.\\nIf x increases, y should also increase, if x decreases, y should also decrease. The\\nslope of the line would be positive in this case and the data points will show a\\nclear linear relationship. Option B shows a strong positive relationship.\\n\\n**278) Correlation between two variables (Var1 and Var2) is 0.65. Now, after**\\n**adding numeric 2 to all the values of Var1, the correlation co-efficient**\\n**will_______ ?**\\n\\nA) Increase\\nB) Decrease\\nC) None of the above\\n\\n**Solution: (C)**\\n\\nIf a constant value is added or subtracted to either variable, the correlation\\ncoefficient would be unchanged. It is easy to understand if we look at the\\nformula for calculating the correlation.\\n\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-97-0.png)\\n\\n-----\\n\\nIf we add a constant value to all the values of x, the xi and will change by the\\nsame number, and the differences will remain the same. Hence, there is no\\nchange in the correlation coefficient.\\n\\n**279) It is observed that there is a very high correlation between math test**\\n**scores and amount of physical exercise done by a student on the test day.**\\n**What can you infer from this?**\\n\\n1. **High correlation implies that after exercise the test scores are high.**\\n2. **Correlation does not imply causation.**\\n3. **Correlation measures the strength of linear relationship between**\\n\\n**amount of exercise and test scores.**\\n\\nA) Only 1\\nB) 1 and 3\\nC) 2 and 3\\nD) All the statements are true\\n\\n**Solution: (C)**\\n\\nThough sometimes causation might be intuitive from a high correlation but\\nactually correlation does not imply any causal inference. It just tells us the\\nstrength of the relationship between the two variables. If both the variables move\\ntogether, there is a high correlation among them.\\n\\n**280) If the correlation coefficient (r) between scores in a math test and**\\n**amount of physical exercise by a student is 0.86, what percentage of**\\n**variability in math test is explained by the amount of exercise?**\\n\\nA) 86%\\nB) 74%\\nC) 14%\\nD) 26%\\n\\n**Solution: (B)**\\n\\nThe % variability is given by r[2], the square of the correlation coefficient. This\\nvalue represents the fraction of the variation in one variable that may be\\nexplained by the other variable. Therefore % variability explained would be\\n\\n\\n-----\\n\\n0.86[2].\\n\\n**281) Which of the following is true about below given histogram?**\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-99-0.png)\\n\\nA) Above histogram is unimodal\\n\\nB) Above histogram is bimodal\\n\\nC) Given above is not a histogram\\n\\nD) None of the above\\n\\n**Solution: (B)**\\n\\nThe above histogram is bimodal. As we can see there are two values for which\\nwe can see peaks in the histograms indicating high frequencies for those values.\\nTherefore the histogram is bimodal.\\n\\n**282) Consider a regression line y=ax+b, where a is the slope and b is the**\\n**intercept. If we know the value of the slope then by using which option can**\\n**we always find the value of the intercept?**\\n\\nA) Put the value (0,0) in the regression line True\\n\\n\\n-----\\n\\nB) Put any value from the points used to fit the regression line and compute the\\nvalue of b False\\n\\nC) Put the mean values of x & y in the equation along with the value a to get b\\nFalse\\n\\nD) None of the above can be used False\\n\\n**Solution: (C)**\\n\\nIn case of ordinary least squares regression, the line would always pass through\\nthe mean values of x and y. If we know one point on the line and the value of\\nslope, we can easily find the intercept.\\n\\n**283) What happens when we introduce more variables to a linear regression**\\n**model?**\\n\\nA) The r squared value may increase or remain constant, the adjusted r squared\\nmay increase or decrease.\\n\\nB) The r squared may increase or decrease while the adjusted r squared always\\nincreases.\\n\\nC) Both r square and adjusted r square always increase on the introduction of\\nnew variables in the model.\\n\\nD) Both might increase or decrease depending on the variables introduced.\\n\\n**Solution: (A)**\\n\\nThe R square always increases or at least remains constant because in case of\\nordinary least squares the sum of square error never increases by adding more\\nvariables to the model. Hence the R squared does not decrease. The adjusted Rsquared is a modified version of R-squared that has been adjusted for the number\\nof predictors in the model. The adjusted R-squared increases only if the new\\nterm improves the model more than would be expected by chance. It decreases\\nwhen a predictor improves the model by less than expected by chance.\\n\\n**284) In a scatter diagram, the vertical distance of a point above or below**\\n**regression line is known as ____ ?**\\n\\n\\n-----\\n\\nA) Residual\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-101-0.png)\\n\\nB) Prediction Error\\nC) Prediction\\nD) Both A and B\\nE) None of the above\\n\\n**Solution: (D)**\\n\\nThe lines as we see in the above plot are the vertical distance of points from the\\nregression line. These are known as the residuals or the prediction error.\\n\\n**285) In univariate linear least squares regression, relationship between**\\n**correlation coefficient and coefficient of determination is ______ ?**\\n\\nA) Both are unrelated False\\n\\nB) The coefficient of determination is the coefficient of correlation squared True\\n\\nC) The coefficient of determination is the square root of the coefficient of\\ncorrelation False\\n\\nD) Both are same F\\n\\n**Solution: (B)**\\n\\nThe coefficient of determination is the R squared value and it tells us the amount\\nof variability of the dependent variable explained by the independent variable.\\nThis is nothing but correlation coefficient squared. In case of multivariate\\nregression the r squared value represents the ratio of the sum of explained\\nvariance to the sum of total variance.\\n\\n**286) What is the relationship between significance level and confidence**\\n**level?**\\n\\n\\n-----\\n\\nA) Significance level = Confidence level\\nB) Significance level = 1- Confidence level\\nC) Significance level = 1/Confidence level\\nD) Significance level = sqrt (1 – Confidence level)\\n\\n**Solution: (B)**\\n\\nSignificance level is 1-confidence interval. If the significance level is 0.05, the\\ncorresponding confidence interval is 95% or 0.95. The significance level is the\\nprobability of obtaining a result as extreme as, or more extreme than, the result\\nactually obtained when the null hypothesis is true. The confidence interval is the\\nrange of likely values for a population parameter, such as the population mean.\\nFor example, if you compute a 95% confidence interval for the average price of\\nan ice cream, then you can be 95% confident that the interval contains the true\\naverage cost of all ice creams.\\n\\nThe significance level and confidence level are the complementary portions in\\nthe normal distribution.\\n\\n**287) [True or False] Suppose you have been given a variable V, along with**\\n**its mean and median. Based on these values, you can find whether the**\\n**variable “V” is left skewed or right skewed for the condition**\\n\\nmean(V) > median(V)\\n\\n\\nA) True\\nB) False\\n\\n**Solution: (B)**\\n\\nSince, its no where mentioned about the type distribution of the variable V, we\\ncannot say whether it is left skewed or right skewed for sure.\\n\\n**288) The line described by the linear regression equation (OLS) attempts to**\\n**____ ?**\\n\\nA) Pass through as many points as possible.\\n\\n\\n-----\\n\\nB) Pass through as few points as possible\\n\\nC) Minimize the number of points it touches\\n\\nD) Minimize the squared distance from the points\\n\\n**Solution: (D)**\\n\\nThe regression line attempts to minimize the squared distance between the points\\nand the regression line. By definition the ordinary least squares regression tries\\nto have the minimum sum of squared errors. This means that the sum of squared\\nresiduals should be minimized. This may or may not be achieved by passing\\nthrough the maximum points in the data. The most common case of not passing\\nthrough all points and reducing the error is when the data has a lot of outliers or\\nis not very strongly linear.\\n\\n**289) We have a linear regression equation ( Y = 5X +40) for the below table.**\\n\\n|X|Y|\\n|---|---|\\n|5|45|\\n|6|76|\\n|7|78|\\n|8|87|\\n|9|79|\\n\\n\\n\\n**Which of the following is a MAE (Mean Absolute Error) for this linear**\\n**model?**\\n\\nA) 8.4\\nB) 10.29\\nC) 42.5\\nD) None of the above\\n\\n**Solution: (A)**\\n\\nTo calculate the mean absolute error for this case, we should first calculate the\\nvalues of y with the given equation and then calculate the absolute error with\\nrespect to the actual values of y. Then the average value of this absolute error\\nwould be the mean absolute error. The below table summarises these values.\\n\\n\\n-----\\n\\n![](../../data/books/500 Data Science Interview Questions/500-Data-Science-Interview-Questions.pdf-104-0.png)\\n\\n**290) A regression analysis between weight (y) and height (x) resulted in the**\\n**following least squares line: y = 120 + 5x. This implies that if the height is**\\n**increased by 1 inch, the weight is expected to**\\n\\nA) increase by 1 pound\\nB) increase by 5 pound\\nC) increase by 125 pound\\nD) None of the above\\n\\n**Solution: (B)**\\n\\nLooking at the equation given y=120+5x. If the height is increased by 1 unit, the\\nweight will increase by 5 pounds. Since 120 will be the same in both cases and\\nwill go off in the difference.\\n\\n**291) [True or False] Pearson captures how linearly dependent two variables**\\n**are whereas Spearman captures the monotonic behaviour of the relation**\\n**between the variables.**\\n\\nA)TRUE\\n\\nB) FALSE\\n\\n**Solution: (A)**\\n\\nThe statement is true. Pearson correlation evaluated the linear relationship\\nbetween two continuous variables. A relationship is linear when a change in one\\nvariable is associated with a proportional change in the other variable.\\n\\nThe spearman evaluates a monotonic relationship. A monotonic relationship is\\none where the variables change together but not necessarily at a constant rate.\\n\\n\\n-----\\n\\n**292) What do you understand by long and wide data formats?**\\n**293) What do you understand by outliers and inliers? What would you do if**\\n**you find them in your dataset?**\\n**294) Write a program in Python which takes input as the diameter of a coin**\\n**and weight of the coin and produces output as the money value of the coin.**\\n**295) What are the basic assumptions to be made for linear regression?**\\nNormality of error distribution, statistical independence of errors, linearity and\\nadditivity.\\n\\n**296) Can you write the formula to calculat R-square?**\\nR-Square can be calculated using the below formular \\n1 - (Residual Sum of Squares/ Total Sum of Squares)\\n\\n**297) What is the advantage of performing dimensionality reduction before**\\n**fitting an SVM?**\\nSupport Vector Machine Learning Algorithm performs better in the reduced\\nspace. It is beneficial to perform dimensionality reduction before fitting an SVM\\nif the number of features is large when compared to the number of observations.\\n\\n**298) How will you assess the statistical significance of an insight whether it**\\n**is a real insight or just by chance?**\\nStatistical importance of an insight can be accessed using Hypothesis Testing.\\n\\n**299) How would you create a taxonomy to identify key customer trends in**\\n**unstructured data?**\\nThe best way to approach this question is to mention that it is good to check with\\nthe business owner and understand their objectives before categorizing the data.\\nHaving done this, it is always good to follow an iterative approach by pulling\\nnew data samples and improving the model accordingly by validating it for\\naccuracy by soliciting feedback from the stakeholders of the business. This helps\\nensure that your model is producing actionable results and improving over the\\ntime.\\n\\n**300) How will you find the correlation between a categorical variable and a**\\n**continuous variable ?**\\nYou can use the analysis of covariance technqiue to find the correlation between\\na categorical variable and a continuous variable.\\n\\n**[Q301. What are the different sampling methods?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n\\nRandom Sampling\\nSystematic Sampling\\n\\n\\n-----\\n\\nStratified Sampling\\nQuota Sampling\\n\\n**[Q302. Common Data Quality Issues](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n\\nMissing Values\\nNoise in the Data Set\\nOutliers\\nMixture of Different Languages (like English and Chinese)\\nRange Constraints\\n\\n**Q303. What is the difference between supervised learning and un-**\\n**supervised learning?**\\nSupervised learning: Target variable is available and the algorithm learns for the\\ntrain data\\nAnd applies to test data (unseen data).\\nUnsupervised learning: Target variable is not available and the algorithm does\\nnot need to learn\\nAnything beforehand.\\n**Q304. What is Imbalanced Data Set and how to handle them? Name Few**\\n**Examples?**\\n\\nFraud detection\\nDisease screening\\n\\nImbalanced Data Set means that the population of one class is extremely large\\nthan the other\\n(Eg: Fraud – 99% and Non-Fraud – 1%)\\nImbalanced dataset can be handled by either oversampling, undersampling and\\npenalized Machine Learning Algorithm.\\n**Q305. If you are dealing with 10M Data, then will you go for Machine**\\n**learning (or) Deep learning Algorithm?**\\n\\nMachine learning algorithmsuits well for small data and it might take huge\\namount of time to train for large data.\\nWhereas Deep learning algorithm takes less amount of data to train due to\\nthe help of GPU(Parallel Processing).\\n\\n**[Q306. Examples of Supervised learning algorithm?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n\\nLinear Regression and Logistic Regression\\nDecision Trees and Random Forest\\nSVM\\nNaïve Bayes\\nXGBoost\\n\\n\\n-----\\n\\n**Q307. In Logistic Regression, if you want to know the best features in your**\\n**dataset then what you would do?**\\nApply step function, which calculates the AIC for different permutation and\\ncombination of features and provides the best features for the dataset.\\n**[Q308. What is Feature Engineering? Explain with Example?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nFeature engineering is the process of using domain knowledge of the data to\\ncreate features for machine learning algorithm to work\\n\\nAdding more columns (or) removing columns from the existing column\\nOutlier Detection\\nNormalization etc\\n\\n**[Q309. How to select the important features in the given data set?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n\\nIn Logistic Regression, we can use step() which gives AIC score of set of\\nfeatures\\nIn Decision Tree, We can use information gain(which internally uses\\nentropy)\\nIn Random Forest, We can use varImpPlot\\n\\n**[Q310. When does multicollinearity problem occur and how to handle it?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nIt exists when 2 or more predictors are highly correlated with each other.\\nExample: In the Data Set if you have grades of 2[nd] PUC and marks of 2[nd] PUC,\\nThen both gives the same trend to capture, which might internally hamper the\\nspeed and time.so we need to check if the multi collinearity exists by using\\nVIF(variance Inflation Factor).\\nNote: if the Variance Inflation Factor is more than 4, then multi collinearity\\nproblem exists.\\n**[Q311. What is Variance inflation Factors (VIF)](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nMeasure how much the variance of the estimated regression coefficients are\\ninflated as compared to when the predictor variables are not linearly related.\\n**Q312. Examples of Parametric machine learning algorithm and non-**\\n**parametric machine learning algorithm**\\n\\nParametric machine learning algorithm– Linear Regression, Logistic\\nRegression\\nNon-Parametric machine learning algorithm – Decision Trees, SVM, Neural\\nNetwork\\n\\n**Q313. What are parametric and non-parametric machine learning**\\n**algorithm? And their importance**\\nAlgorithm which does not make strong assumptions are non-parametric\\nalgorithm and they are free to learn from training data. Algorithm that makes\\n\\n\\n-----\\n\\nstrong assumptions are parametric and it involves\\n\\n1. select the form for the function and\\n2. learn the coefficients for the function from training data.\\n\\n**[Q314. When does linear and logistic regression performs better, generally?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nIt works better when we remove the attributes which are unrelated to the output\\nvariable and highly co-related variable to each other.\\n**[Q315. Why you call naïve bayes as “naïve” ?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nReason: It assumes that the input variable is independent, but in real world it is\\nunrealistic, since all the features would be dependent on each other.\\n**Q316. Give some example for false positive, false negative, true positive,**\\n**true negative**\\n\\nFalse Positive – A cancer screening test comes back positive, but you don’t\\nhave cancer\\nFalse Negative – A cancer screening test comes back negative, but you have\\ncancer\\nTrue Positive – A Cancer Screening test comes back positive, and you have\\ncancer\\nTrue Negative – A Cancer Screening test comes back negative, and you\\ndon’t have cancer\\n\\n**[Q317. What is Sensitivity and Specificity?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nSensitivity means “proportion of actual positives that are correctly classified” in\\nother words “True Positive”\\nSpecificity means “proportion of actual negatives that are correctly classified”\\n“True Negative”\\n**[Q318. When to use Logistic Regression and when to use Linear Regression?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nIf you are dealing with a classification problem like (Yes/No, Fraud/Non Fraud,\\nSports/Music/Dance) then use Logistic Regression.\\nIf you are dealing with continuous/discrete values, then go for Linear\\nRegression.\\n**[Q319. What are the different imputation algorithm available?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nImputation algorithm means “replacing the Blank values by some values)\\n\\nMean imputation\\nMedian Imputation\\nMICE\\nmiss forest\\nAmelia\\n\\n**[Q320. What is AIC(Akaike Information Criteria)](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n\\n\\n-----\\n\\nThe analogous metric of adjusted R² in logistic regression is AIC.\\nAIC is the measure of fit which penalizes model for the number of model\\ncoefficients. Therefore, we always prefer model with minimum AIC value.\\n**Q321. Suppose you have 10 samples, where 8 are positive and 2 are**\\n**negative, how to calculate Entropy (important to know)**\\nE(S) = 8/10log(8/10) – 2/10log(2/10)\\nNote: Log is à base 2\\n**[Q322. What is perceptron in Machine Leaning?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nIn Machine Learning. Perceptron is an algorithm for supervised classification of\\nthe input into one of several possible non-binary outputs\\n**[Q323. How to ensure we are not over fitting the model?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n\\nKeep the attributes/Columns which are really important\\nUse K-Fold cross validation techniques\\nMake use of drop-put incase of neural network\\n\\n**[Q324.How the root node is predicted in Decision Tree Algorithm?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nMathematical Formula “Entropy” is utilized for predicting the root node of the\\ntree.\\n**[Q325. What are the different Backend Process available in Keras?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n\\nTensorFlow\\nTheano\\nCNTK\\n\\n**[Q326. Name Few Deep Learning Algorithm](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n\\nTensorFlow\\nTheano\\nLasagne\\nmxnet\\nblocks\\nKeras\\nCNTK\\nTFLearn\\n\\n**Q327. How to split the data with equal set of classes in both training and**\\n**testing data?**\\nUsing Stratified Shuffle package\\n**[Q328. What do you mean by giving “epoch = 1” in neural network?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nIt means that “traversing the data set one time\\n**[Q329. What do you mean by Ensemble Model? When to use?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nEnsemble Model is a combination of Different Models to predict correctly and\\n\\n\\n-----\\n\\nwith good accuracy.\\nEnsemble learning is used when you build component classifiers that are more\\naccurate and independent from each other.\\n**[Q330. When will you use SVM and when to use Random Forest?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n\\nSVM can be used if the data is outlier free whereas Naïve Bayes can be used\\neven if it has outliers (since it has built in package to take care).\\nSVM suits best for Text Classification Model and Random Forest suits for\\nBinomial/Multinomial Classification Problem.\\nRandom Forest takes care of over fitting problem with the help of tree\\npruning\\n\\n**[Q331. Applications of Machine Learning?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n\\nSelf Driving Cars\\nImage Classification\\nText Classification\\nSearch Engine\\nBanking, Healthcare Domain\\n\\n**Q332. If you are given with a use case – ‘Predict whether the transaction is**\\n**fraud (or) not fraud”, which algorithm would you choose**\\nLogistic Regression\\n**Q333. If you are given with a use case – ‘Predict the house price range in the**\\n**coming years”, which algorithm would you choose**\\nLinear Regression\\n**Q334. What is the underlying mathematical knowledge behind Naïve**\\n**Bayes?**\\nBayes Theorem\\n**[Q335. When to use Random Forest and when to Use XGBoost?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nIf you want all core processors in your system to be utilized, then go for\\nXGBoost(since it supports parallel processing) and if your data is small then go\\nfor random forest.\\n**Q336. If you are training model gives 90% accuracy and test model gives**\\n**60% accuracy? Then what problem you are facing with?**\\nOverfitting.\\nOverfitting and can be reduced by many methods like (Tree Pruning, Removing\\nthe minute information provided in the data set).\\n**Q337. In Google if you type “How are “it gives you the recommendation as**\\n**“How are you “/”How do you do”, this is based on what?**\\nThis kind of recommendation engine comes from collaborative filtering.\\n\\n\\n-----\\n\\n**[Q338. What is margin, kernels, Regularization in SVM?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n\\nMargin – Distance between the hyper plane and closest data points is\\nreferred as “margin”\\nKernels – there are three types of kernel which determines the type of data\\nyou are dealing with i) Linear, ii) Radial, iii) Polynomial\\nRegularization – The Regularization parameter (often termed as C parameter\\nin python’s sklearn library) tells the SVM optimization how much you want\\nto avoid misclassifying each training example\\n\\n**[Q339. What is Boosting? Explain how Boosting works?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nBoosting is a Ensemble technique that attempts to create strong classifier from a\\nnumber of weak classifiers\\n\\nAfter the first tree is created, the performance of the tree on each training\\ninstance is used to weight how much attention the next tree that is created\\nshould pay attention to each training instance by giving more weights to the\\nmisclassified one.\\nModels are created one after the other, each updating the weights on the\\ntraining instance\\n\\n**Q340. What is Null Deviance and Residual Deviance (Logistic Regression**\\n**Concept?)**\\nNull Deviance indicates the response predicted by a model with nothing but an\\nintercept\\nResidual deviance indicates the response predicted by a model on adding\\nindependent variables\\nNote:\\nLower the value, better the model\\n**[Q341. What are the different method to split the tree in decision tree?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nInformation gain and gini index\\n**[Q342. What is the weakness for Decision Tree Algorithm?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nNot suitable for continuous/Discrete variable\\nPerforms poorly on small data\\n**[Q343. Why do we use PCA(Principal Components Analysis) ?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nThese are important feature extraction techniques used for dimensionality\\nreduction.\\n**[Q344. During Imbalanced Data Set, will you](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n\\nCalculate the Accuracy only? (or)\\nPrecision, Recall, F1 Score separately\\n\\nWe need to calculate precision, Recall separately\\n\\n\\n-----\\n\\n**[Q345.How to ensure we are not over fitting the model?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n\\nKeep the attributes/Columns which are really important\\nUse K-Fold cross validation techniques\\nmake use of drop-put in case of neural network\\n\\n**[Q346. Steps involved in Decision Tree and finding the root node for the tree](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nStep 1:- How to find the Root Node\\nUse Information gain to understand the each attribute information w.r.t target\\nvariable and place the attribute with the highest information gain as root node.\\nStep 2:- How to Find the Information Gain\\nPlease apply the entropy (Mathematical Formulae) to calculate Information\\nGain. Gain (T,X) = Entropy(T) – Entropy(T,X) here represent target variable and\\nX represent features.\\nStep3: Identification of Terminal Node\\nBased on the information gain value obtained from the above steps, identify the\\nsecond most highest information gain and place it as the terminal node.\\nStep 4: Predicted Outcome\\nRecursively iterate the step4 till we obtain the leaf node which would be our\\npredicted target variable.\\nStep 5: Tree Pruning and optimization for good results\\nIt helps to reduce the size of decision trees by removing sections of the tree to\\navoid over fitting.\\n**[Q347. What is hyper plane in SVM?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nIt is a line that splits the input variable space and it is selected to best separate\\nthe points in the input variable space by their class(0/1,yes/no).\\n**[Q348. Explain Bigram with an Example?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nEg: I Love Data Science\\nBigram – (I Love) (Love Data) (Data Science)\\n**[Q349. What are the different activation functions in neural network?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nRelu, Leaky Relu, Softmax, Sigmoid\\n**[Q350. Which Algorithm Suits for Text Classification Problem?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nSVM, Naïve Bayes, Keras, Theano, CNTK, TFLearn(Tensorflow)\\n**Q351. You are given a train data set having lot of columns and rows. How**\\n**do you reduce the dimension of this data?**\\n\\nPrincipal Component Analysis(PCA) would help us here which can explain\\nthe maximum variance in the data set.\\nWe can also check the co-relation for numerical data and remove the\\n\\n\\n-----\\n\\nproblem of multi-collinearity(if exists) and remove some of the columns\\nwhich may not impact the model.\\nWe can create multiple dataset and execute them batch wise.\\n\\n**Q352. You are given a data set on fraud detection. Classification model**\\n**achieved accuracy of 95%.Is it good?**\\nAccuracy of 96% is good. But we may have to check the following items:\\n\\nwhat was the dataset for the classification problem\\nIs Sensitivity and Specificity are acceptable\\nif there are only less negative cases, and all negative cases are not correctly\\nclassified, then it might be a problem\\n\\nIn-Addition it is related to fraud detection, hence needs to be careful here in\\nprediction (i.e not wrongly predicting the fraud as non-fraud patient.\\n**[Q353. What is prior probability and likelihood?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nPrior probability:\\nThe proportion of dependent variable in the data set.\\nLikelihood:\\nIt is the probability of classifying a given observation as ‘1’ in the presence of\\nsome other variable.\\n**Q354. How can we know if your data is suffering from low bias and high**\\n**variance?**\\nRandom Forest Algorithm can be used to tackle high variance problem.in the\\ncases of low bias and high variance L1,L2 regularization can help.\\n**[Q355. How is kNN different from kmeans clustering?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nKmeans partitions a data set into clusters, which is homogeneous and points in\\nthe cluster are close to each other. Whereas KNN tries to classify unlabelled\\nobservation based on its K surrounding neighbours.\\n**Q356. Random Forest has 1000 trees, Training error: 0.0 and validation**\\n**error is 20.00.What is the issue here?**\\nIt is the classical example of over fitting. It is not performing well on the unseen\\ndata. We may have to tune our model using cross validation and other techniques\\nto overcome over fitting\\n**Q357. Data set consisting of variables having more than 30% missing**\\n**values? How will you deal with them?**\\nWe can remove them, if it does not impact our model\\nWe can apply imputation techniques (like MICE, MISSFOREST,AMELIA) to\\navoid missing values\\n**[Q358. What do you understand by Type I vs. Type II error?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n\\n\\n-----\\n\\nType I error occurs when – “we classify a value as positive, when the actual\\nvalue is negative”\\n(False Positive)\\nType II error occurs when – “we classify a value as negative, when the actual\\nvalue if positive”\\n(False Negative)\\n**[Q359. Based on the dataset, how will you know which algorithm to apply ?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n\\nIf it is classification related problem,then we can use logistic,decision trees\\netc…\\nIf it is Regression related problem, then we can use Linear Regression.\\nIf it is Clustering based, we can use KNN.\\nWe can also apply XGB, RF for better accuracy.\\n\\n**[Q360. Why normalization is important?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nData Set can have one column in the range (10,000/20,000) and other column\\nmight have data in the range (1, 2, 3).clearly these two columns are in different\\nrange and cannot accurately analyse the trend. So we can apply normalization\\nhere by using min-max normalization (i.e to convert it into 0-1 scale).\\n**[Q361. What is Data Science?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nFormally, It’s the way to Quantify your intuitions.\\nTechnically, Data Science is a combination of Machine Learning, Deep Learning\\n& Artificial\\nIntelligence. Where Deep Learning is the subset of AI.\\n\\n**[Q362. What is Machine Learning?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nMachine learning is the process of generating the predictive power using past\\ndata(memory). It is a\\none-time process where the predictions can fail in the future (if your data\\ndistribution changes).\\n\\n**[Q363. What is Deep Learning?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nDeep Learning is the process of adding one more logic to the machine learning,\\nwhere it iterates\\nitself with the new data and will not fail in future, even though your data\\ndistribution changes. The\\nmore it iterates, more it works better.\\n\\n**[Q364. Where to use R & Python?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nR can be used whenever the data is structed. Python is efficient to handle\\nunstructured data. R can’t\\n\\n\\n-----\\n\\nhandle high volume data. Python backend working with Theano/tensor made it\\neasy to perform it as\\nfast comparing with R.\\n\\n**[Q365. Which Algorithms are used to do a Binary classification?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nLogistic Regression, KNN, Random Forest, CART, C50 are few algorithms\\nwhich can perform Binary\\nclassification.\\n\\n**[Q366. Which Algorithms are used to do a Multinomial classification?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nNaïve Bayes, Random Forest are widely used for multinomial classification.\\n\\n**[Q367. What is LOGIT function?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nLOGIT function is Log of ODDS ratio. ODDS ratio can be termed as the\\nProbability of success divided\\nby Probability of failure. Which is the final probability value of your binary\\nclassification, where we\\nuse ROC curve to get the cut-Off value of the probability.\\n\\n**[Q368. What are all the pre-processing steps that are highly recommended?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n\\n-  Structural Analysis\\n\\n-  Outlier Analysis\\n\\n-  Missing value treatments\\n\\n-  Feature engineering\\n\\n**[Q369. What is Normal Distribution?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nWhenever data that defines with having Mean = Median = Mode, then the data is\\ncalled as normally\\ndistributed data.\\n\\n**[Q370. What is empirical Rule?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nEmpirical Rule says that whenever data is normally distributed, your data should\\nbe having the\\ndistribution in a way of,\\n68 percent of your data spread is within Plus or Minus 1 standard deviation\\n95 percent of your data spread is within Plus or Minus 2 standard deviation\\n99.7 percent of your data spread is within Plus or Minus 3 standard deviation\\n\\n**Question 371. What Is Bayesian?**\\n\\n\\n-----\\n\\n**Answer:**\\nBayesians condition on the data actually observed and consider the\\nprobability distribution on the hypotheses.\\n\\n**Question 372. What Is Frequentist?**\\n\\n**Answer:**\\nFrequentists condition on a hypothesis of choice and consider the probability\\ndistribution on the data, whether observed or not.\\n\\n**Question 373. What Is Likelihood?**\\n\\n**Answer:**\\nThe probability of some observed outcomes given a set of parameter values is\\nregarded as the likelihood of the set of parameter values given the observed\\noutcomes.\\n\\n**Question 374. What Is P-value?**\\n\\n**Answer:**\\nIn statistical significance testing, the p-value is the probability of obtaining a\\ntest statistic at least as extreme as the one that was actually observed,\\nassuming that the null hypothesis is true. If the p-value is less than 0.05 or\\n0.01, corresponding respectively to a 5% or 1% chance of rejecting the null\\nhypothesis when it is true.\\n\\n**Question 375. Give An Example Of P-value?**\\n\\n**Answer:**\\nSuppose that the experimental results show the coin turning up heads 14 times\\nout of 20 total flips\\n\\nnull hypothesis (H0): fair coin;\\nobservation O: 14 heads out of 20 flips; and\\np-value of observation O given H0 = Prob(≥ 14 heads or ≥ 14 tails) =\\n0.115.\\n\\nThe calculated p-value exceeds 0.05, so the observation is consistent with the\\nnull hypothesis - that the observed result of 14 heads out of 20 flips can be\\nascribed to chance alone - as it falls within the range of what would happen\\n95% of the time were this in fact the case. In our example, we fail to reject the\\nnull hypothesis at the 5% level. Although the coin did not fall evenly, the\\ndeviation from expected outcome is small enough to be reported as being \"not\\nstatistically significant at the 5% level\".\\n\\n**Question 376. What Is Sampling?**\\n\\n**Answer:**\\nSampling is that part of statistical practice concerned with the selection of an\\nunbiased or random subset of individual observations within a population of\\n\\n\\n-----\\n\\nindividuals intended to yield some knowledge about the population of\\nconcern.\\n\\n**Question 377. What Are Sampling Methods?**\\n\\n**Answer:**\\nThere are four sampling methods:\\n\\nSimple Random (purely random),\\nSystematic( every kth member of population),\\nCluster (population divided into groups or clusters)\\nStratified (divided by exclusive groups or strata, sample from each\\ngroup) samplings.\\n\\n**Question 378. What Is Mode?**\\n\\n**Answer:**\\nThe mode of a data sample is the element that occurs most often in the\\ncollection.\\nx=[1 2 3 3 3 4 4]\\nmode(x) % return 3, happen most.\\n\\n**Question 379. What Is Median?**\\n\\n**Answer:**\\nMedian is described as the numeric value separating the higher half of a\\nsample, a population, or a probability distribution, from the lower half. The\\nmedian of a finite list of numbers can be found by arranging all the\\nobservations from lowest value to highest value and picking the middle one\\nmedian(x) % return 3.\\n\\n**Question 380. What Is Quartile?**\\n\\n**Answer:**\\n\\nsecond quartile (50th percentile) .\\nthird quartile (75th percentile) .\\nkth percentile.\\nprctile(x, 25) % 25th percentile, return 2.25.\\nprctile(x, 50) % 50th percentile, return 3, i.e. median.\\n\\n**Question 381. What Is Skewness?**\\n\\n**Answer:**\\nSkewness is a measure of the asymmetry of the data around the sample mean.\\nIf skewness is negative, the data are spread out more to the left of the mean\\nthan to the right. If skewness is positive, the data are spread out more to the\\nright.\\nSkewness(x) % return-0.5954\\n\\n\\n-----\\n\\n**Question 382. What Is Variance?**\\n\\n**Answer:**\\nvariance describes how far values lie from the mean.\\nvar(x) %return 1.1429\\n\\n**Question 383. What Is Kurtosis?**\\n\\n**Answer:**\\nKurtosis is a measure of how outlier-prone a distribution is.\\nkurtosis(x) % return2.3594\\n\\n**Question 384. What Is Moment?**\\n\\n**Answer:**\\nQuantitative measure of the shape of a set of points.\\nmoment(x, 2); %return second moment\\n\\n**Question 385. What Is Covariance?**\\n\\n**Answer:**\\nMeasure of how much two variables change together.\\ny2=[1 3 4 5 6 7 8]\\ncov(x,y2) %return 2*2 matrix, diagonal represents variance.\\n\\n**Question 386. What Is One Sample T-test?**\\n\\n**Answer:**\\nT-test is any statistical hypothesis test in which the test statistic follows a\\nStudent\\'s t distribution if the null hypothesis is supported.\\n\\n[h,p,ci] = ttest(y2,0)% return 1 0.0018 ci =2.6280 7.0863\\n\\n**Question 387. What Is Alternative Hypothesis?**\\n\\n**Answer 9:**\\nThe Alternative hypothesis (denoted by H1 ) is the statement that must be true\\nif the null hypothesis is false.\\n\\n**Question 388. What Is Significance Level?**\\n\\n**Answer:**\\nThe probability of rejecting the null hypothesis when it is called the\\nsignificance level α, and very common choices are α = 0.05 and α = 0.01.\\n\\n**Question 389. Give Example Of Central Limit Theorem?**\\n\\n**Answer:**\\nGiven that the population of men has normally distributed weights, with a\\nmean of 173 lb and a standard deviation of 30 lb, find the probability that\\na. if 1 man is randomly selected, his weight is greater than 180 lb.\\nb. if 36 different men are randomly selected, their mean weight is greater that\\n\\n\\n-----\\n\\n180 lb.\\n**Solution: a) z = (x - μ)/ σ = (180-173)/30 = 0.23**\\nFor normal distribution P(Z>0.23) = 0.4090\\nb) σ x̄ = σ/√n = 20/√ 36 = 5\\nz= (180-173)/5 = 1.40\\nP(Z>1.4) = 0.0808\\n\\n**Question 390. What Is Binomial Probability Formula?**\\n\\n**Answer:**\\nP(x)= p x q n-x n!/[(n-x)!x!]\\nwhere n = number of trials.\\nx = number of successes among n trials.\\np = probability of success in any one trial.\\nq = 1 -p.\\n\\n**Question 391. Do You Know What Is Binary Search?**\\n\\n**Answer:**\\nFor binary search, the array should be arranged in ascending or descending\\norder. In each step, the algorithm compares the search key value with the key\\nvalue of the middle element of the array. If the keys match, then a matching\\nelement has been found and its index, or position, is returned. Otherwise, if\\nthe search key is less than the middle element\\'s key, then the algorithm repeats\\nits action on the sub-array to the left of the middle element or, if the search\\nkey is greater, on the sub-array to the right.\\n\\n**Question 392. Explain Hash Table?**\\n\\n**Answer:**\\nA hash table is a data structure used to implement an associative array, a\\nstructure that can map keys to values. A hash table uses a hash function to\\ncompute an index into an array of buckets or slots, from which the correct\\nvalue can be found.\\n\\n**Question 393. Explain Central Limit Theorem?**\\n\\n**Answer:**\\nAs the sample size increases, the sampling distribution of sample means\\napproaches a normal distribution.\\nIf all possible random samples of size n are selected from a population with\\nmean μ and standard deviation σ, the mean of the sample means is denoted by\\nμ x̄, so,\\nμ x̄ = μ\\n\\n\\n-----\\n\\nthe standard deviation of the sample means is:\\nσ x̄ = σ⁄√ n\\n\\n**Question 394. What Is Null Hypothesis?**\\n\\n**Answer:**\\nThe null hypothesis (denote by H0 ) is a statement about the value of a\\npopulation parameter (such as mean), and it must contain the condition of\\nequality and must be written with the symbol =, ≤, or ≤.\\n\\n**Question 395. What Is Linear Regression?**\\n\\n**Answer:**\\nModeling the relationship between a scalar variable y and one or more\\nvariables denoted X. In linear regression, models of the unknown parameters\\nare estimated from the data using linear functions.\\npolyfit( x,y2,1) %return 2.1667 -1.3333, i.e 2.1667x-1.3333\\n\\n**Question 396. When You Are Creating A Statistical Model How Do You**\\n**Prevent Over-fitting?**\\n\\n**Answer:**\\nOver-fitting can be prevented by cross-validation.\\n\\n**Question 397. What Is Descriptive Statistics?**\\n\\n**Answer:**\\nWe study in descriptive statistics the methods for organizing, displaying, and\\ndescribing data.\\n\\n**Question 398. What Is A Sample?**\\n\\n**Answer:**\\nWhen data are collected in a statistical study for only a portion or subset of all\\nelements of interest we are using a Sample.\\n\\n**Question 399. Give An Example Of Inferential Statistics?**\\n\\n**Answer:**\\nExample of Inferential Statistic :\\nYou asked five of your classmates about their height. On the basis of this\\ninformation, you stated that the average height of all students in your\\nuniversity or college is 67 inches.\\n\\n**Question 400. A Normal Population Distribution Is Needed For The Which**\\n**Of The Statistical Tests:**\\n\\n**Answer:**\\n\\nvariance estimation.\\nstandard error of the mean.\\nStudent\\'s t-test.\\n\\n\\n-----\\n\\n**Q401. (Given a Dataset) Analyze this dataset and give me a model that can**\\n**predict this response variable.**\\nStart by fitting a simple model (multivariate regression, logistic regression), do\\nsome feature engineering accordingly, and then try some complicated models.\\nAlways split the dataset into train, validation, test dataset and use cross\\nvalidation to check their performance.\\nDetermine if the problem is classification or regression\\nFavor simple models that run quickly and you can easily explain.\\nMention cross validation as a means to evaluate the model.\\nPlot and visualize the data.\\n**Q402. What could be some issues if the distribution of the test data is**\\n**significantly different than the distribution of the training data?**\\nThe model that has high training accuracy might have low test accuracy. Without\\nfurther knowledge, it is hard to know which dataset represents the population\\ndata and thus the generalizability of the algorithm is hard to measure. This\\nshould be mitigated by repeated splitting of train vs test dataset (as in cross\\nvalidation).\\nWhen there is a change in data distribution, this is called the dataset shift. If the\\ntrain and test data has a different distribution, then the classifier would likely\\noverfit to the train data.\\nThis issue can be overcome by using a more general learning method.\\nThis can occur when:\\nP(y|x) are the same but P(x) are different. (covariate shift)\\nP(y|x) are different. (concept shift)\\nThe causes can be:\\nTraining samples are obtained in a biased way. (sample selection bias)\\nTrain is different from test because of temporal, spatial changes. (non-stationary\\nenvironments)\\nSolution to covariate shift\\nimportance weighted cv\\n**[Q403. What are some ways I can make my model more robust to outliers?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nWe can have regularization such as L1 or L2 to reduce variance (increase bias).\\nChanges to the algorithm:\\nUse tree-based methods instead of regression methods as they are more resistant\\nto outliers. For statistical tests, use non parametric tests instead of parametric\\nones.\\nUse robust error metrics such as MAE or Huber Loss instead of MSE.\\n\\n\\n-----\\n\\nChanges to the data:\\nWinsorizing the data\\nTransforming the data (e.g. log)\\nRemove them only if you’re certain they’re anomalies not worth predicting\\n**Q404. What are some differences you would expect in a model that**\\n**[minimizes squared error, versus a model that minimizes absolute error? In](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n**which cases would each error metric be appropriate?**\\nMSE is more strict to having outliers. MAE is more robust in that sense, but is\\nharder to fit the model for because it cannot be numerically optimized. So when\\nthere are less variability in the model and the model is computationally easy to\\nfit, we should use MAE, and if that’s not the case, we should use MSE.\\nMSE: easier to compute the gradient, MAE: linear programming needed to\\ncompute the gradient\\nMAE more robust to outliers. If the consequences of large errors are great, use\\nMSE\\nMSE corresponds to maximizing likelihood of Gaussian random variables\\n**Q405. What error metric would you use to evaluate how good a binary**\\n**[classifier is? What if the classes are imbalanced? What if there are more](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n**than 2 groups?**\\nAccuracy: proportion of instances you predict correctly. Pros: intuitive, easy to\\nexplain, Cons: works poorly when the class labels are imbalanced and the signal\\nfrom the data is weak\\nAUROC: plot fpr on the x axis and tpr on the y axis for different threshold.\\nGiven a random positive instance and a random negative instance, the AUC is\\nthe probability that you can identify who’s who. Pros: Works well when testing\\nthe ability of distinguishing the two classes, Cons: can’t interpret predictions as\\nprobabilities (because AUC is determined by rankings), so can’t explain the\\nuncertainty of the model\\nlogloss/deviance: Pros: error metric based on probabilities, Cons: very sensitive\\nto false positives, negatives\\nWhen there are more than 2 groups, we can have k binary classifications and add\\nthem up for logloss. Some metrics like AUC is only applicable in the binary\\ncase.\\n**Q406. What are various ways to predict a binary response variable? Can**\\n**you compare two of them and tell me when one would be more appropriate?**\\n**What’s the difference between these? (SVM, Logistic Regression, Naive**\\n**Bayes, Decision Tree, etc.)**\\nThings to look at: N, P, linearly seperable?, features independent?, likely to\\n\\n\\n-----\\n\\noverfit?, speed, performance, memory usage\\nLogistic Regression:\\nfeatures roughly linear, problem roughly linearly separable\\nrobust to noise, use l1,l2 regularization for model selection, avoid overfitting\\nthe output come as probabilities\\nefficient and the computation can be distributed\\ncan be used as a baseline for other algorithms\\n(-) can hardly handle categorical features\\nSVM:\\nwith a nonlinear kernel, can deal with problems that are not linearly separable\\n(-) slow to train, for most industry scale applications, not really efficient\\nNaive Bayes:\\ncomputationally efficient when P is large by alleviating the curse of\\ndimensionality\\nworks surprisingly well for some cases even if the condition doesn’t hold\\nwith word frequencies as features, the independence assumption can be seen\\nreasonable. So the algorithm can be used in text categorization\\n(-) conditional independence of every other feature should be met\\nTree Ensembles:\\ngood for large N and large P, can deal with categorical features very well\\nnon parametric, so no need to worry about outliers\\nGBT’s work better but the parameters are harder to tune\\nRF works out of the box, but usually performs worse than GBT\\nDeep Learning:\\nworks well for some classification tasks (e.g. image)\\nused to squeeze something out of the problem\\n**Q407. What is regularization and where might it be helpful? What is an**\\n**example of using regularization in a model?**\\nRegularization is useful for reducing variance in the model, meaning avoiding\\noverfitting . For example, we can use L1 regularization in Lasso regression to\\npenalize large coefficients.\\n**[Q408. Why might it be preferable to include fewer predictors over many?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nWhen we add irrelevant features, it increases model’s tendency to overfit\\nbecause those features introduce more noise. When two variables are correlated,\\nthey might be harder to interpret in case of regression, etc.\\ncurse of dimensionality\\nadding random noise makes the model more complicated but useless\\ncomputational cost\\n\\n\\n-----\\n\\nAsk someone for more details.\\n**Q409. Given training data on tweets and their retweets, how would you**\\n**[predict the number ofretweets of a given tweet after 7 days after only](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n**observing 2 days worth of data?**\\nBuild a time series model with the training data with a seven day cycle and then\\nuse that for a new data with only 2 days data.\\nAsk someone for more details.\\nBuild a regression function to estimate the number of retweets as a function of\\ntime t\\nto determine if one regression function can be built, see if there are clusters in\\nterms of the trends in the number of retweets\\nif not, we have to add features to the regression function\\nfeatures + # of retweets on the first and the second day -> predict the seventh day\\nhttps://en.wikipedia.org/wiki/Dynamic_time_warping\\n**Q410. How could you collect and analyze data to use social media to predict**\\n**the weather?**\\nWe can collect social media data using twitter, Facebook, instagram API’s. Then,\\nfor example, for twitter, we can construct features from each tweet, e.g. the\\ntweeted date, number of favorites, retweets, and of course, the features created\\nfrom the tweeted content itself. Then use a multi variate time series model to\\npredict the weather.\\nAsk someone for more details. Get Data Science Training in Kalayan Nagar\\nBangalore.\\n**Q411. How would you construct a feed to show relevant content for a site**\\n**that involves userinteractions with items?**\\nWe can do so using building a recommendation engine. The easiest we can do is\\nto show contents that are popular other users, which is still a valid strategy if for\\nexample the contents are news articles. To be more accurate, we can build a\\ncontent based filtering or collaborative filtering. If there’s enough user usage\\ndata, we can try collaborative filtering and recommend contents other similar\\nusers have consumed. If there isn’t, we can recommend similar items based on\\nvectorization of items (content based filtering).\\n**Q412. How would you design the people you may know feature on LinkedIn**\\n**or Facebook?**\\nFind strong unconnected people in weighted connection graph\\nDefine similarity as how strong the two people are connected\\nGiven a certain feature, we can calculate the similarity based on\\nfriend connections (neighbors)\\n\\n\\n-----\\n\\nCheck-in’s people being at the same location all the time.\\nsame college, workplace\\nHave randomly dropped graphs test the performance of the algorithm\\nref. News Feed Optimization\\nAffinity score: how close the content creator and the users are\\nWeight: weight for the edge type (comment, like, tag, etc.). Emphasis on features\\nthe company wants to promote\\nTime decay: the older the less important\\n**Q413. How would you predict who someone may want to send a Snapchat**\\n**or Gmail to?**\\nfor each user, assign a score of how likely someone would send an email to\\nthe rest is feature engineering:\\nnumber of past emails, how many responses, the last time they exchanged an\\nemail, whether the last email ends with a question mark, features about the other\\nusers, etc.\\nAsk someone for more details.\\nPeople who someone sent emails the most in the past, conditioning on time\\ndecay.\\n**[Q414. How would you suggest to a franchise where to open a new store?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nbuild a master dataset with local demographic information available for each\\nlocation.\\nlocal income levels, proximity to traffic, weather, population density, proximity\\nto other businesses\\na reference dataset on local, regional, and national macroeconomic conditions\\n(e.g. unemployment, inflation, prime interest rate, etc.)\\nany data on the local franchise owner-operators, to the degree the manager\\nidentify a set of KPIs acceptable to the management that had requested the\\nanalysis concerning the most desirable factors surrounding a franchise\\nquarterly operating profit, ROI, EVA, pay-down rate, etc.\\nrun econometric models to understand the relative significance of each variable\\nrun machine learning algorithms to predict the performance of each location\\ncandidate\\n**Q415. In a search engine, given partial data on what the user has typed, how**\\n**would you predict the user’s eventual search query?**\\nBased on the past frequencies of words shown up given a sequence of words, we\\ncan construct conditional probabilities of the set of next sequences of words that\\ncan show up (n-gram). The sequences with highest conditional probabilities can\\nshow up as top candidates.\\n\\n\\n-----\\n\\nTo further improve this algorithm,\\nwe can put more weight on past sequences which showed up more recently and\\nnear your location to account for trends\\nshow your recent searches given partial data\\n**Q416. Given a database of all previous alumni donations to your university,**\\n**how would you predict which recent alumni are most likely to donate?**\\nBased on frequency and amount of donations, graduation year, major, etc,\\nconstruct a supervised regression (or binary classification) algorithm.\\n**Q417. You’re Uber and you want to design a heatmap to recommend to**\\n**drivers where to wait for a passenger. How would you approach this?**\\nBased on the past pickup location of passengers around the same time of the day,\\nday of the week (month, year), construct\\nAsk someone for more details.\\nBased on the number of past pickups\\naccount for periodicity (seasonal, monthly, weekly, daily, hourly)\\nspecial events (concerts, festivals, etc.) from tweets\\n**[Q418. How would you build a model to predict a March Madness bracket?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nOne vector each for team A and B. Take the difference of the two vectors and\\n[use that as an input to predict the probability that team A would win by training](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)\\nthe model. Train the models using past tournament data and make a prediction\\nfor the new tournament by running the trained model for each round of the\\ntournament\\nSome extensions:\\nExperiment with different ways of consolidating the 2 team vectors into one (e.g\\nconcantenating, averaging, etc)\\nConsider using a RNN type model that looks at time series data.\\n**Q419. You want to run a regression to predict the probability of a flight**\\n**[delay, but there are flights with delays of up to 12 hours that are really](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n**messing up your model. How can you address this?**\\nThis is equivalent to making the model more robust to outliers.\\n\\nProbability\\n\\n**Q421. Bobo the amoeba has a 25%, 25%, and 50% chance of producing 0,**\\n**[1, or 2 o spring, respectively. Each of Bobo’s descendants also have the same](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n**probabilities. What is the probability that Bobo’s lineage dies out?**\\np=1/4+1/4p+1/2p^2 => p=1/2\\n**Q422. In any 15-minute interval, there is a 20% probability that you will see**\\n**at least one shooting star. What is the proba- bility that you see at least one**\\n\\n\\n-----\\n\\n**shooting star in the period of an hour?**\\n1-(0.8)^4. Or, we can use Poisson processes\\n**Q424. How can you get a fair coin toss if someone hands you a coin that is**\\n**weighted to come up heads more often than tails?**\\nFlip twice and if HT then H, TH then T.\\n**Q425. You have an 50-50 mixture of two normal distributions with the same**\\n**[standard deviation. How far apart do the means need to be in order for this](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n**distribution to be bimodal?**\\nmore than two standard deviations\\n**Q426. Given draws from a normal distribution with known parameters,**\\n**how can you simulate draws from a uniform distribution?**\\nplug in the value to the CDF of the same random variable\\n**Q427. A certain couple tells you that they have two children, at least one of**\\n**which is a girl. What is the probability that they have two girls?**\\n1/3\\n**Q428. You have a group of couples that decide to have children until they**\\n**have their first girl, afterwhich they stop having children. What is the**\\n**expected gender ratio of the children that are born?What is the expected**\\n**number of children each couple will have?**\\ngender ratio is 1:1. Expected number of children is 2. let X be the number of\\nchildren until getting a female (happens with prob 1/2). this follows a geometric\\ndistribution with probability 1/2\\n**[Q429. How many ways can you split 12 people into 3 teams of 4?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nthe outcome follows a multinomial distribution with n=12 and k=3. but the\\nclasses are indistinguishable\\n**Q430. Your hash function assigns each object to a number between 1:10,**\\n**each with equal probability. With 10 objects, what is the probability of a**\\n**hash collision? What is the expected number of hash collisions? What is the**\\n**expected number of hashes that are unused?**\\nthe probability of a hash collision: 1-(10!/10^10)\\nthe expected number of hash collisions: 1-10*(9/10)^10\\nthe expected number of hashes that are unused: 10*(9/10)^10\\n**Q431. You call 2 UberX’s and 3 Lyfts. If the time that each takes to reach**\\n**[you is IID, what is theprobability that all the Lyfts arrive first? What is the](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n**probability that all the UberX’s arrive first?**\\nLyfts arrive first: 2!*3!/5!\\n\\n\\n-----\\n\\nUbers arrive first: same\\n**Q432. I write a program should print out all the numbers from 1 to 300, but**\\n**prints out Fizz instead if the number is divisible by 3, Buzz instead if the**\\n**[number is divisible by 5, and FizzBuzz if the number is divisible by 3 and 5.](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n**What is the total number of numbers that is either Fizzed, Buzzed, or**\\n**FizzBuzzed?**\\n100+60-20=140\\n**Q433. On a dating site, users can select 5 out of 24 adjectives to describe**\\n**themselves. A match isdeclared between two users if they match on at least 4**\\n**adjectives. If Alice and Bob randomly pick adjectives, what is the**\\n**probability that they form a match?**\\n24C5*(1+5(24-5))/24C5*24C5 = 4/1771\\n**Q434. A lazy high school senior types up application and envelopes to n**\\n**[different colleges, but puts the applications randomly into the envelopes.](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n**What is the expected number of applications that went to the right college?**\\n1\\n**Q435. Let’s say you have a very tall father. On average, what would you**\\n**[expect the height of his son to be? Taller, equal, or shorter? What if you had](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n**a very short father?**\\nShorter. Regression to the mean\\n**Q436. What’s the expected number of coin flips until you get two heads in a**\\n**row?**\\nthe expected number of coin flips until you get two tails in a row.\\n**Q437. Let’s say we play a game where I keep flipping a coin until I get**\\n**[heads. If the first time I get heads is on the nth coin, then I pay you 2n-1](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n**dollars. How much would you pay me to play this game?**\\nless than $3\\n**Q438. You have two coins, one of which is fair and comes up heads with a**\\n**probability 1/2, and the other which is biased and comes up heads with**\\n**probability 3/4. You randomly pick coin and flip it twice, and get heads both**\\n**times. What is the probability that you picked the fair coin?**\\n4/13\\nData Analysis\\n\\n**Q439. Let’s say you’re building the recommended music engine at Spotify to**\\n**[recommend peoplemusic based on past lis- tening history. How would you](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n**approach this problem?**\\n\\n\\n-----\\n\\ncollaborative filtering\\n**Q440. What is R2? What are some other metrics that could be better than**\\n**R2 and why?**\\ngoodness of fit measure. variance explained by the regression / total variance\\nthe more predictors you add the higher R^2 becomes.\\nhence use adjusted R^2 which adjusts for the degrees of freedom\\nor train error metrics\\n**[Q441. What is the curse of dimensionality?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nHigh dimensionality makes clustering hard, because having lots of dimensions\\nmeans that everything is “far away” from each other.\\nFor example, to cover a fraction of the volume of the data we need to capture a\\nvery wide range for each variable as the number of variables increases\\nAll samples are close to the edge of the sample. And this is a bad news because\\nprediction is much more difficult near the edges of the training sample.\\nThe sampling density decreases exponentially as p increases and hence the data\\nbecomes much more sparse without significantly more data.\\nWe should conduct PCA to reduce dimensionality\\n**[Q442. Is more data always better?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nStatistically,\\nIt depends on the quality of your data, for example, if your data is biased, just\\ngetting more data won’t help.\\nIt depends on your model. If your model suffers from high bias, getting more\\ndata won’t improve your test results beyond a point. You’d need to add more\\nfeatures, etc.\\nPractically,\\nAlso there’s a tradeoff between having more data and the additional storage,\\ncomputational power, memory it requires. Hence, always think about the cost of\\nhaving more data.\\n**Q443. What are advantages of plotting your data before performing**\\n**analysis?**\\nData sets have errors. You won’t find them all but you might find some. That\\n212 year old man. That 9 foot tall woman.\\nVariables can have skewness, outliers etc. Then the arithmetic mean might not be\\nuseful. Which means the standard deviation isn’t useful.\\nVariables can be multimodal! If a variable is multimodal then anything based on\\nits mean or median is going to be suspect.\\n**Q444. How can you make sure that you don’t analyze something that ends**\\n**up meaningless?**\\n\\n\\n-----\\n\\nProper exploratory data analysis.\\nIn every data analysis task, there’s the exploratory phase where you’re just\\ngraphing things, testing things on small sets of the data, summarizing simple\\nstatistics, and getting rough ideas of what hypotheses you might want to pursue\\nfurther.\\nThen there’s the exploitatory phase, where you look deeply into a set of\\nhypotheses.\\nThe exploratory phase will generate lots of possible hypotheses, and the\\nexploitatory phase will let you really understand a few of them. Balance the two\\nand you’ll prevent yourself from wasting time on many things that end up\\nmeaningless, although not all.\\n**Q445. What is the role of trial and error in data analysis? What is the role**\\n**of making a hypothesisbefore diving in?**\\ndata analysis is a repetition of setting up a new hypothesis and trying to refute\\nthe null hypothesis.\\nThe scientific method is eminently inductive: we elaborate a hypothesis, test it\\nand refute it or not. As a result, we come up with new hypotheses which are in\\nturn tested and so on. This is an iterative process, as science always is.\\n**Q446. How can you determine which features are the most important in**\\n**your model?**\\nrun the features though a Gradient Boosting Machine or Random Forest to\\ngenerate plots of relative importance and information gain for each feature in the\\nensembles.\\nLook at the variables added in forward variable selection\\n**[Q447. How do you deal with some of your predictors being missing?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nRemove rows with missing values – This works well if 1) the values are missing\\nrandomly (see Vinay Prabhu’s answer for more details on this) 2) if you don’t\\nlose too much of the dataset after doing so.\\nBuild another predictive model to predict the missing values – This could be a\\nwhole project in itself, so simple techniques are usually used here.\\nUse a model that can incorporate missing data – Like a random forest, or any\\ntree-based method.\\n**Q448. You have several variables that are positively correlated with your**\\n**response, and you thinkcombining all of the variables could give you a good**\\n**[prediction of your response. However, you see that in the multiple linear](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n**regression, one of the weights on the predictors is negative. What could be**\\n**the issue?**\\n\\n\\n-----\\n\\nMulticollinearity refers to a situation in which two or more explanatory variables\\nin a multiple regression model are highly linearly related.\\nLeave the model as is, despite multicollinearity. The presence of\\nmulticollinearity doesn’t affect the efficiency of extrapolating the fitted model to\\nnew data provided that the predictor variables follow the same pattern of\\nmulticollinearity in the new data as in the data on which the regression model is\\nbased.\\nprincipal component regression\\n**Q449. Let’s say you’re given an unfeasible amount of predictors in a**\\n**[predictive modeling task. What are some ways to make the prediction more](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n**feasible?**\\nPCA\\n**Q450. Now you have a feasible amount of predictors, but you’re fairly sure**\\n**[that you don’t need all of them. How would you perform feature selection](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n**on the dataset?**\\nridge / lasso / elastic net regression\\nUnivariate Feature Selection where a statistical test is applied to each feature\\nindividually. You retain only the best features according to the test outcome\\nscores\\n“Recursive Feature Elimination”:\\nFirst, train a model with all the feature and evaluate its performance on held out\\ndata.\\nThen drop let say the 10% weakest features (e.g. the feature with least absolute\\ncoefficients in a linear model) and retrain on the remaining features.\\nIterate until you observe a sharp drop in the predictive accuracy of the model.\\n**Q451. Your linear regression didn’t run and communicates that there are an**\\n**[infinite number of best estimates for the regression coefficients. What could](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n**be wrong?**\\np > n.\\nIf some of the explanatory variables are perfectly correlated (positively or\\nnegatively) then the coefficients would not be unique.\\n**Q452. You run your regression on different subsets of your data, and that in**\\n**[each subset, the betavalue for a certain variable varies wildly. What could](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n**be the issue here?**\\nThe dataset might be heterogeneous. In which case, it is recommended to cluster\\ndatasets into different subsets wisely, and then draw different models for\\ndifferent subsets. Or, use models like non parametric models (trees) which can\\ndeal with heterogeneity quite nicely.\\n\\n\\n-----\\n\\nWhat is the main idea behind ensemble learning? If I had many different models\\nthat predicted the same response variable, what might I want to do to incorporate\\nall of the models? Would you expect this to perform better than an individual\\nmodel or worse?\\nThe assumption is that a group of weak learners can be combined to form a\\nstrong learner.\\nHence the combined model is expected to perform better than an individual\\nmodel.\\nAssumptions:\\naverage out biases\\nreduce variance\\nBagging works because some underlying learning algorithms are unstable:\\nslightly different inputs leads to very different outputs. If you can take advantage\\nof this instability by running multiple instances, it can be shown that the reduced\\ninstability leads to lower error. If you want to understand why, the original\\nbagging paper( http://www.springerlink.com/cont…) has a section called “why\\nbagging works”\\nBoosting works because of the focus on better defining the “decision edge”. By\\nreweighting examples near the margin (the positive and negative examples) you\\nget a reduced error (see http://citeseerx.ist.psu.edu/vie…)\\nUse the outputs of your models as inputs to a meta-model.\\nFor example, if you’re doing binary classification, you can use all the probability\\noutputs of your individual models as inputs to a final logistic regression (or any\\nmodel, really) that can combine the probability estimates.\\nOne very important point is to make sure that the output of your models are outof-sample predictions. This means that the predicted value for any row in your\\ndataframe should NOT depend on the actual value for that row.\\n**Q453. Given that you have wi data in your o ce, how would you determine**\\n**which rooms and areasare underutilized and overutilized?**\\nIf the data is more used in one room, then that one is over utilized! Maybe\\naccount for the room capacity and normalize the data.\\n**[Q454. How would you quantify the influence of a Twitter user?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nlike page rank with each user corresponding to the web pages and linking to the\\npage equivalent to following.\\n**Q455. You have 100 mathletes and 100 math problems. Each mathlete gets**\\n**[to choose 10 problems to solve. Given data on who got what problem](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n**correct, how would you rank the problems in terms of difficulty?**\\nOne way you could do this is by storing a “skill level” for each user and a\\n\\n\\n-----\\n\\n“difficulty level” for each problem. We assume that the probability that a user\\nsolves a problem only depends on the skill of the user and the difficulty of the\\nproblem.* Then we maximize the likelihood of the data to find the hidden skill\\nand difficulty levels.\\nThe Rasch model for dichotomous data takes the form:\\n{\\\\displaystyle \\\\Pr\\\\{X_{ni}=1\\\\}={\\\\frac {\\\\exp({\\\\beta _{n}}-{\\\\delta _{i}})}\\n{1+\\\\exp({\\\\beta _{n}}-{\\\\delta _{i}})}},}\\nwhere is the ability of person and is the difficulty of item}.\\n**Q456. You have 5000 people that rank 10 sushis in terms of salt- iness. How**\\n**[would you aggregate this data to estimate the true saltiness rank in each](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n**sushi?**\\nSome people would take the mean rank of each sushi. If I wanted something\\nsimple, I would use the median, since ranks are (strictly speaking) ordinal and\\nnot interval, so adding them is a bit risque (but people do it all the time and you\\nprobably won’t be far wrong).\\n**Q457. Given data on congressional bills and which congressio- nal**\\n**representatives co-sponsored the bills, how would you determine which**\\n**[other representatives are most similar to yours in voting behavior? How](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n**would you evaluate who is the most liberal? Most republican? Most**\\n**bipartisan?**\\ncollaborative filtering. you have your votes and we can calculate the similarity\\nfor each representatives and select the most similar representative\\nfor liberal and republican parties, find the mean vector and find the\\nrepresentative closest to the center point\\n**Q458. How would you come up with an algorithm to detect plagiarism in**\\n**online content?**\\nreduce the text to a more compact form (e.g. fingerprinting,\\nbag of wor\\nds) then compare those with other texts by calculating the similarity\\n**Q459. You have data on all purchases of customers at a grocery store.**\\n**Describe to me how you would program an algorithm that would cluster the**\\n**customers into groups. How would you determine the appropriate number**\\n**of clusters include?**\\nKNN\\nchoose a small value of k that still has a low SSE (elbow method)\\nhttps://bl.ocks.org/rpgove/0060ff3b656618e9136b\\nStatistical Inference\\n\\n\\n-----\\n\\n**Q460. In an A/B test, how can you check if assignment to the various**\\n**buckets was truly random?**\\nPlot the distributions of multiple features for both A and B and make sure that\\nthey have the same shape. More rigorously, we can conduct a permutation test to\\nsee if the distributions are the same.\\nMANOVA to compare different means\\n**Q461. What might be the benefits of running an A/A test, where you have**\\n**two buckets who areexposed to the exact same product?**\\nVerify the sampling algorithm is random.\\n**Q462. What would be the hazards of letting users sneak a peek at the other**\\n**bucket in an A/B test?**\\nThe user might not act the same suppose had they not seen the other bucket. You\\nare essentially adding additional variables of whether the user peeked the other\\nbucket, which are not random across groups.\\n**Q463. What would be some issues if blogs decide to cover one of your**\\n**experimental groups?**\\nSame as the previous question. The above problem can happen in larger scale.\\n**[Q464. How would you conduct an A/B test on an opt-in feature?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nAsk someone for more details.\\n**[Q465. How would you run an A/B test for many variants, say 20 or more?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\none control, 20 treatment, if the sample size for each group is big enough.\\nWays to attempt to correct for this include changing your confidence level (e.g.\\nBonferroni Correction) or doing family-wide tests before you dive in to the\\nindividual metrics (e.g. Fisher’s Protected LSD).\\n**Q466. How would you run an A/B test if the observations are extremely**\\n**right-skewed?**\\nlower the variability by modifying the KPI\\ncap values\\npercentile metrics\\nlog transform\\nhttps://www.quora.com/How-would-you-run-an-A-B-test-if-the-observationsare-extremely-right-skewed\\n**Q467. I have two different experiments that both change the sign-up button**\\n**[to my website. I want to test them at the same time. What kinds of things](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n**should I keep in mind?**\\nexclusive -> ok\\n**Q468. What is a p-value? What is the difference between type-1 and type-2**\\n\\n\\n-----\\n\\n**error?**\\ntype-1 error: rejecting Ho when Ho is a true\\ntype-2 error: not rejecting Ho when Ha is true\\n\\n[toggle_content title=”Q49. You are AirBnB and you want to test the hypothesis\\nthat a greater number of photographs increases the chances that a buyer selects\\nthe listing. How would you test this hypothesis?\\nFor randomly selected listings with more than 1 pictures, hide 1 random picture\\nfor group A, and show all for group B. Compare the booking rate for the two\\ngroups.\\nAsk someone for more details.\\n**Q469. How would you design an experiment to determine the impact of**\\n**latency on userengagement?**\\nThe best way I know to quantify the impact of performance is to isolate just that\\nfactor using a slowdown experiment, i.e., add a delay in an A/B test.\\n**Q470. What is maximum likelihood estimation? Could there be any case**\\n**where it doesn’t exist?**\\nA method for parameter optimization (fitting a model). We choose parameters so\\nas to maximize the likelihood function (how likely the outcome would happen\\ngiven the current data and our model).\\nmaximum likelihood estimation (MLE) is a method of estimating the parameters\\nof a statistical model given observations, by finding the parameter values that\\nmaximize the likelihood of making the observations given the parameters. MLE\\ncan be seen as a special case of the maximum a posteriori estimation (MAP) that\\nassumes a uniform prior distribution of the parameters, or as a variant of the\\nMAP that ignores the prior and which therefore is unregularized.\\nfor Gaussian mixtures, non-parametric models, it doesn’t exist\\n**Q471. What’s the difference between a MAP, MOM, MLE estimator? In**\\n**which cases would you want to use each?**\\nMAP estimates the posterior distribution given the prior distribution and data\\nwhich maximizes the likelihood function. MLE is a special case of MAP where\\nthe prior is uninformative uniform distribution.\\nMOM sets moment values and solves for the parameters. MOM has not used\\nmuch anymore because maximum likelihood estimators have higher probability\\nof being close to the quantities to be estimated and are more often unbiased.\\n**[Q472. What is a confidence interval and how do you interpret it?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nFor example, 95% confidence interval is an interval that when constructed for a\\nset of samples each sampled in the same way, the constructed intervals include\\nthe true mean 95% of the time.\\n\\n\\n-----\\n\\nif confidence intervals are constructed using a given confidence level in an\\ninfinite number of independent experiments, the proportion of those intervals\\nthat contain the true value of the parameter will match the confidence level.\\n**Q473. What is unbiasedness as a property of an estimator? Is this always a**\\n**[desirable property when performing inference? What about in data analysis](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n**or predictive modeling?**\\nUnbiasedness means that the expectation of the estimator is equal to the\\npopulation value we are estimating. This is desirable in inference because the\\ngoal is to explain the dataset as accurately as possible. However, this is not\\nalways desirable for data analysis or predictive modeling as there is the bias\\nvariance tradeoff. We sometimes want to prioritize the generalizability and avoid\\noverfitting by reducing variance and thus increasing bias.\\nOTHER Important Data Science Interview Questions and Answers\\n**[Q474. What is the difference between population and sample in data?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nSample is the set of people who participated in your study whereas the\\npopulation is the set of people to whom you want to generalize the results. For\\nexample – If you want to study the obesity among the children in India and you\\nstudy 1000 children then those 1000 became sample whereas the all the children\\nin the country is the population.\\nSample is the subset of population.\\n**[Q475. What is the difference sample and sample frame?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nSample frame is the number of people who wanted to study whereas sample is\\nthe actual number of people who participated in your study. Ex – If you sent a\\nmarketing survey link to 300 people through email and only 100 participated in\\nthe survey then 300 is the sample survey and 100 is the sample.\\nSample is the subset of sample frame. Both Sample and Sample Frame are\\nsubset of population.\\n**Q476. What is the difference between univariate, bivariate and multivariate**\\n**analysis?**\\nUnivariate analysis is performed on one variable, bivariate on two variable and\\nmultivariate analysis on two or more variables\\n**[Q477. What is difference between interpolation and extrapolation?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nExtrapolation is the estimation of future values based on the observed trend on\\nthe past. Interpolation is the estimation of missing past values within two values\\nin a sequence of values\\n**[Q478. What is precision and recall?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nPrecision is the percentage of correct predictions you have made and recall is the\\n\\n\\n-----\\n\\npercentage of predictions that actually turned out to be true\\n**[Q479. What is confusion matrix?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n\\nConfusion matrix is a table which contains information about predicted\\nvalues and actual values in a classification model\\nIt has four parts namely true positive,true negative, false positive and false\\nnegative\\nIt can be used to calculate accuracy, precision and recall\\n\\n**[Q480. What is hypothesis testing?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nWhile performing the an experiment hypothesis testing to is used to analyze the\\nvarious factors that are assumed to have an impact on the outcome of experiment\\nAn hypothesis is some kind of assumption and hypothesis testing is used to\\ndetermine whether the stated hypothesis is true or not\\nInitial assumption is called null hypothesis and the opposite alternate hypothesis\\n**[Q481. What is a p-value in statistics?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nIn hypothesis testing, p value helps to arrive at a conclusion. When p -value is\\ntoo small then null hypothesis is rejected and alternate is accepted. When pvalue is large then null hypothesis is accepted.\\n**Q482. What is difference between Type-I error and Type-II error in**\\n**hypothesis testing?**\\nType-I error is we reject the null hypothesis which was supposed to be accepted.\\nIt represents false positive\\nType-II error represents we accept the null hypothesis which was supposed to be\\nrejected. It represents false negative.\\n**[Q483. QWhat are the different types of missing value treatment?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n\\nDeletion of values\\nGuess the value\\nAverage Substitution\\nRegression based substitution\\nMultiple Imputation\\n\\n**[Q484. What is gradient descent?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nWhen building a statistical model the objective is reduce the value of the cost\\nfunction that is associated with the model. Gradient descent is an iterative\\noptimization technique used to determine the minima of the cost function\\n**Q485. What is difference between supervised and unsupervised learning**\\n**algorithms?**\\nSupervised learning are the class of algorithms in which model is trained by\\nexplicitly labelling the outcome. Ex. Regression, Classification\\n\\n\\n-----\\n\\nUnsupervised learning no output is given and the algorithm is made to learn the\\noutcomes implicity Ex. Association, Clustering\\n**[Q486. What is the need for regularization in model building?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nRegularization is used to penalize the model when it overfits the model. It\\npredominantly helps in solving the overfitting problem.\\n**[Q487. Difference between bias and variance tradeoff?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nHigh Bias is an underlying error wrong assumption that makes the model to\\nunderfit. High Variance in a model means noise in data has been too taken\\nseriously by the model which will result in overfitting.\\nTypically we would like to have a model with low bias and low variance\\n**[Q488. How to solve overfitting?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\n\\nIntroduce Regularization\\nPerform Cross Validation\\nReduce the number of features\\nIncrease the number of entries\\nEnsembling\\n\\n**[Q489. How will you detect the presence of overfitting?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nWhen you build a model which has very high model accuracy on train data set\\nand very low prediction accuracy in test data set then it is a indicator of\\noverfitting\\n**[Q490. How do you determine the number of clusters in k-means clustering?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nElbow method ( Plotting the percentage of variance explained w.r.t to number of\\nclusters)\\nGap Statistic\\nSilhouette method\\n**[Q491. What is the difference between causality and correlation?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nCorrelation is the measure that helps us understand the relationship between two\\nor more variables\\nCausation represents that causal relationship between two events. It is also\\nknown to represent cause and effect\\nCausation means there is correlation but correlation doesn’t necessarily mean\\ncausation\\n**[Q492. Explain normal distribution?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nNormal distribution is a bell shaped curve that represents distribution of data\\naround its mean. Any normal process would follow the normal distribution.\\nMost of data points tend to concentrated around the mean. If a point is further\\naway from the mean then it is less likely to appear\\n\\n\\n-----\\n\\n**Q493. What are the different ways of performing aggregation in python**\\n**using pandas?**\\nGroup by function\\nPivot function\\nAggregate function\\n**[Q494. What are merge two list and get only unique values?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nList a = [1,2,3,4] List b= [1,2,5,6] A = list(set(a+b))\\n**[Q495. How to save and retrieve model objects in python?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nBy using a library called pickle you can train any model and store the object in a\\npickle file.\\nWhen needed in future you can retrieve the object and use the model for\\nprediction.\\n\\n[toggle_content title=”Q96. What is an anomaly and how is it different from\\noutliers?\\nAnomaly detection is identification of items or events that didn’t fit to the exact\\npattern or other items in a dataset. Outliers are valid data points that are outside\\nthe norm whereas anomaly are invalid data points that are created by process\\nthat is different from process that created the other data points\\n**[Q497. What is an ensemble learning?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nEnsemble learning is the art of combining more than one model to predict the\\nfinal outcome of an experiment. Commonly used ensemble techniques bagging,\\nboosting and stacking\\n**[Q498. Name few libraries that is used in python for data analysis?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nNumpy\\nScipy\\nPandas\\nScikit learn\\nMatplotlib\\\\ seaborn\\n**[Q499. What are the different types of data?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nData is broadly classified into two types 1) Numerical 2) Categorical\\nNumerical variables is further classified into discrete and continuous data\\nCategorical variables\\nSystematic Sampling\\nStratified Sampling\\nQuota Sampling are further classified into Binary, Nominal and Ordinal data\\n**[Q500. What is a lambda function in python?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\\nLambda function are used to create small, one-time anonymous function in\\n\\n\\n-----\\n\\npython. It enables the programmer to create functions without a name and almost\\ninstantly\\n\\n\\n-----\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pymupdf4llm\n",
    "\n",
    "\n",
    "pdf_name = \"500 Data Science Interview Questions.pdf\"\n",
    "md_text = pymupdf4llm.to_markdown(f\"../../data/books/{pdf_name}\", write_images=True,\n",
    "                                  image_path=os.path.join(\"..\", \"..\", \"data\", \"books\", pdf_name.split(\".\")[0]), \n",
    "                                  page_chunks=False, hdr_info=False)\n",
    "md_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and question extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair 1:\n",
      "Question: What is Python?\n",
      "Answer: Python is a programming language.\n",
      "\n",
      "Pair 2:\n",
      "Question: Why use Python?\n",
      "Answer: It is easy to use.\n",
      "\n",
      "Pair 3:\n",
      "Question: What are Python libraries?\n",
      "Answer: Libraries are collections of modules and functions.\n",
      "\n",
      "Pair 42:\n",
      "Question: What is K-means? How can you select K for K-means? 43) What is Collaborative  filtering?\n",
      "Answer: The process of filtering used by most of the recommender systems to find patterns or information by collaborating viewpoints, various data sources and multiple agents.\n",
      "\n",
      "Pair 123:\n",
      "Question: How does Python handle memory?\n",
      "Answer: Memory management is automatic.\n",
      "\n",
      "Pair 124:\n",
      "Question: What is PEP?\n",
      "Answer: PEP stands for Python Enhancement Proposal.\n",
      "\n",
      "Pair 455:\n",
      "Question: You have 100 mathletes and 100 math problems. Each mathlete gets [to choose 10 problems to solve. Given data on who got what problem](https://www.besanttechnologies.com/data-science-interview-questions-and-answers) correct, how would you rank the problems in terms of difficulty?\n",
      "Answer: One way you could do this is by storing a “skill level” for each user and a   -----  “difficulty level” for each problem. We assume that the probability that a user solves a problem only depends on the skill of the user and the difficulty of the problem.* Then we maximize the likelihood of the data to find the hidden skill and difficulty levels. The Rasch model for dichotomous data takes the form: {\\displaystyle \\Pr\\{X_{ni}=1\\}={\frac {\\exp(eta _{n}}-{\\delta _{i}})} {1+\\exp(eta _{n}}-{\\delta _{i}})}},} where is the ability of person and is the difficulty of item}.\n",
      "\n",
      "Pair 456:\n",
      "Question: You have 5000 people that rank 10 sushis in terms of salt- iness. How [would you aggregate this data to estimate the true saltiness rank in each](https://www.besanttechnologies.com/data-science-interview-questions-and-answers) sushi?\n",
      "Answer: Some people would take the mean rank of each sushi. If I wanted something simple, I would use the median, since ranks are (strictly speaking) ordinal and not interval, so adding them is a bit risque (but people do it all the time and you probably won’t be far wrong).\n",
      "\n",
      "Pair 9:\n",
      "Question: You are AirBnB and you want to test the hypothesis that a greater number of photographs increases the chances that a buyer selects the listing. How would you test this hypothesis?\n",
      "Answer: For randomly selected listings with more than 1 pictures, hide 1 random picture for group A, and show all for group B. Compare the booking rate for the two groups. Ask someone for more details.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:36: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:36: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\a.ramirez.lopez\\AppData\\Local\\Temp\\ipykernel_10968\\3084383011.py:36: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  markdown_text = \"\"\"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "question_patterns = r\"\\*\\*Question|\\*\\*[0-9]{1,3}\\)|\\*\\*\\d{1,3}\\.|\\*\\*\\[?Q\\d{1,3}\\.|\\[toggle_content|$\"\n",
    "preanswer_patterns = r\"\\*\\*\\s*\"\n",
    "answer_patterns = r\"\\n[^\\*][^\\*].*?\"\n",
    "\n",
    "# Pattern 1: **Question [0-9]{1,3}. Question text** **Answer:** Answer text\n",
    "pattern1 = re.compile(rf\"\\*\\*Question ([0-9]{{1,3}})\\.?\\s*(.+?){preanswer_patterns}\\*?\\*?Answer:\\*?\\*?(.*?)(?={question_patterns})\", re.DOTALL)\n",
    "\n",
    "# Pattern 2: **[0-9]{1,3}) Question text** Answer text\n",
    "pattern2 = re.compile(rf\"\\*\\*([0-9]{{1,3}})\\)(.+?){preanswer_patterns}({answer_patterns})(?={question_patterns})\", re.DOTALL)\n",
    "\n",
    "# Pattern 3: **Q?[0-9]{1,3}. Question text** Answer text\n",
    "pattern3 = re.compile(rf\"\\*\\*Q?(\\d{{1,3}})\\.\\s*(.+?){preanswer_patterns}({answer_patterns})(?={question_patterns})\", re.DOTALL)\n",
    "\n",
    "# Pattern 4: **[Q?[0-9]{1,3}. Question text](Answer image)**\n",
    "pattern4 = re.compile(rf\"\\*\\*\\[Q?(\\d{{1,3}})\\.\\s*(.+?)\\]\\s*\\(({answer_patterns})\\)\\*\\*(?={question_patterns})\", re.DOTALL)\n",
    "\n",
    "# Pattern 5: toggle_content ... Q.[0-9]{1,3} Question text? Answer text\n",
    "pattern5 = re.compile(rf\"toggle_content.*Q?(\\d{{1,3}})\\.\\s*(.+?\\?)\\s*({answer_patterns})(?={question_patterns})\", re.DOTALL)\n",
    "\n",
    "def extract_question_answer_pairs(markdown_text):\n",
    "    question_answer_pairs = []\n",
    "\n",
    "    for pattern in [pattern1, pattern2, pattern3, pattern4, pattern5]:\n",
    "        match = pattern.findall(markdown_text)\n",
    "        for id, question, answer in match:\n",
    "            question = question.replace(\"**\", \"\")\n",
    "            question = question.replace(\"\\n\", \" \")\n",
    "            answer = answer.replace(\"\\n\", \" \")\n",
    "            question_answer_pairs.append((id, question.strip(), answer.strip()))\n",
    "\n",
    "    return question_answer_pairs\n",
    "\n",
    "# Example usage\n",
    "markdown_text = \"\"\"\n",
    "**Question 1. What is Python?**\n",
    "**Answer:** Python is a programming language.\n",
    "**Question 2. Why use Python?**\n",
    "**Answer:** It is easy to use.\n",
    "**3)What are Python libraries?**\n",
    "Libraries are collections of modules and functions.\n",
    "**42) What is K-means? How can you select K**\n",
    "**for K-means?**\n",
    "**43) What is Collaborative**\n",
    "** filtering?**\n",
    "The process of filtering used by most of the recommender systems to find\n",
    "patterns or information by collaborating viewpoints, various data sources and\n",
    "multiple agents.\n",
    "**123. How does Python handle memory?**\n",
    "Memory management is automatic.\n",
    "**Q124. What is PEP?**\n",
    "PEP stands for Python Enhancement Proposal.\n",
    "**[Q494. What are merge two list and get only unique values?](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\n",
    "**Q455. You have 100 mathletes and 100 math problems. Each mathlete gets**\n",
    "**[to choose 10 problems to solve. Given data on who got what problem](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\n",
    "**correct, how would you rank the problems in terms of difficulty?**\n",
    "One way you could do this is by storing a “skill level” for each user and a\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "“difficulty level” for each problem. We assume that the probability that a user\n",
    "solves a problem only depends on the skill of the user and the difficulty of the\n",
    "problem.* Then we maximize the likelihood of the data to find the hidden skill\n",
    "and difficulty levels.\n",
    "The Rasch model for dichotomous data takes the form:\n",
    "{\\displaystyle \\Pr\\{X_{ni}=1\\}={\\frac {\\exp({\\beta _{n}}-{\\delta _{i}})}\n",
    "{1+\\exp({\\beta _{n}}-{\\delta _{i}})}},}\n",
    "where is the ability of person and is the difficulty of item}.\n",
    "**Q456. You have 5000 people that rank 10 sushis in terms of salt- iness. How**\n",
    "**[would you aggregate this data to estimate the true saltiness rank in each](https://www.besanttechnologies.com/data-science-interview-questions-and-answers)**\n",
    "**sushi?**\n",
    "Some people would take the mean rank of each sushi. If I wanted something\n",
    "simple, I would use the median, since ranks are (strictly speaking) ordinal and\n",
    "not interval, so adding them is a bit risque (but people do it all the time and you\n",
    "probably won’t be far wrong).\n",
    "\n",
    "[toggle_content title=”Q49. You are AirBnB and you want to test the hypothesis\n",
    "that a greater number of photographs increases the chances that a buyer selects\n",
    "the listing. How would you test this hypothesis?\n",
    "For randomly selected listings with more than 1 pictures, hide 1 random picture\n",
    "for group A, and show all for group B. Compare the booking rate for the two\n",
    "groups.\n",
    "Ask someone for more details.\n",
    "\"\"\"\n",
    "\n",
    "question_answer_pairs = extract_question_answer_pairs(markdown_text)\n",
    "\n",
    "# Display the extracted pairs\n",
    "for (i, question, answer) in question_answer_pairs:\n",
    "    print(f\"Pair {i}:\\nQuestion: {question}\\nAnswer: {answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "question_answer_pairs = extract_question_answer_pairs(md_text.replace(\"\\n\\n-----\\n\\n\", \"\"))\n",
    "\n",
    "book_qa = []\n",
    "for i, question, answer in question_answer_pairs:\n",
    "    book_qa.append({\"question_n\": i,\n",
    "     \"question\": question,\n",
    "     \"answer\": answer})\n",
    "\n",
    "with open(\"../../data/500 Data Science Interview Questions.json\", \"w\") as file:\n",
    "    json.dump(book_qa, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227398"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now work with the markdown text, e.g. store as a UTF8-encoded file\n",
    "import pathlib\n",
    "pathlib.Path(\"output.md\").write_bytes(md_text.replace(\"\\n\\n-----\\n\\n\", \"\").encode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guidelines to custom titles in the markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pymupdf\n",
    "\n",
    "doc = pymupdf.open(f\"../../data/books/{pdf_name}\")\n",
    "\n",
    "for page in doc[4:5]:\n",
    "    pass\n",
    "\n",
    "page.get_textpage().extractDICT()['blocks'][1]['lines']\n",
    "\n",
    "# to hdr_info you can pass a function that gets: page.get_textpage().extractDICT() and returns \"\", or \"#\" up to 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to save the markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now work with the markdown text, e.g. store as a UTF8-encoded file\n",
    "import pathlib\n",
    "pathlib.Path(\"output.md\").write_bytes(md_text.encode())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs-qa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
